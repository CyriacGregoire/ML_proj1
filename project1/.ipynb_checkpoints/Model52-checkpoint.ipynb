{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f61bf89-b881-4273-8cb1-34ede6005c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from Data_cleaning import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e781094a-f133-4e09-81d5-51f6ec310fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data\\dataset\\dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "53c76369-82fa-4069-87fa-97e23940a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2, keep_mask = remove_nan_features(x_train, 0.4)\n",
    "X_test = x_test[:, keep_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e1e79408-dcdf-47da-8093-4eb499c42115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 163) (109379, 321)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_2.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "290d0a62-7411-4845-bcba-cb56b4ea84a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_ = (y_train + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a25c609f-9f1d-4efc-8ca1-da4821d96484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_three_way_split(X, y, val_ratio=0.15, test_ratio=0.15, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    idx_pos = np.where(y == 1)[0]\n",
    "    idx_neg = np.where(y == 0)[0]\n",
    "    np.random.shuffle(idx_pos)\n",
    "    np.random.shuffle(idx_neg)\n",
    "\n",
    "    n_pos, n_neg = len(idx_pos), len(idx_neg)\n",
    "    n_val_pos = int(n_pos * val_ratio)\n",
    "    n_test_pos = int(n_pos * test_ratio)\n",
    "    n_val_neg = int(n_neg * val_ratio)\n",
    "    n_test_neg = int(n_neg * test_ratio)\n",
    "\n",
    "    val_idx = np.concatenate([idx_pos[:n_val_pos], idx_neg[:n_val_neg]])\n",
    "    test_idx = np.concatenate([\n",
    "        idx_pos[n_val_pos:n_val_pos + n_test_pos],\n",
    "        idx_neg[n_val_neg:n_val_neg + n_test_neg]\n",
    "    ])\n",
    "    train_idx = np.concatenate([\n",
    "        idx_pos[n_val_pos + n_test_pos:],\n",
    "        idx_neg[n_val_neg + n_test_neg:]\n",
    "    ])\n",
    "\n",
    "    np.random.shuffle(train_idx)\n",
    "    np.random.shuffle(val_idx)\n",
    "    np.random.shuffle(test_idx)\n",
    "\n",
    "    return (\n",
    "        X[train_idx], y[train_idx],\n",
    "        X[val_idx], y[val_idx],\n",
    "        X[test_idx], y[test_idx]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9df92cc8-f5b8-477f-b634-cb2f4e4f730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_va, y_va, x_te, y_te = stratified_three_way_split(x_train_2, y_tr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "33a749a3-52e7-4156-b09e-6046825ca5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 163) (229695, 163) (229695,) (49220, 163) (49220,) (49220, 163) (49220,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_2.shape, x_tr.shape, y_tr.shape, x_va.shape, y_va.shape, x_te.shape, y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9252d56b-c061-4de8-bf77-b4d185acfccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_split(y_tr, y_val, y_te, name_train=\"Train\", name_val=\"Validation\", name_test=\"Test\"):\n",
    "    \"\"\"\n",
    "    Checks that a stratified 3-way split preserved class proportions and sample counts.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Sanity Check: Stratified Split ===\")\n",
    "    n_total = len(y_tr) + len(y_val) + len(y_te)\n",
    "    print(f\"Total samples: {n_total:,}\")\n",
    "\n",
    "    def report(y, name):\n",
    "        frac1 = np.mean(y)\n",
    "        n = len(y)\n",
    "        print(f\"{name:<12}: n={n:<8} | positives={frac1:.4f} | negatives={1-frac1:.4f}\")\n",
    "\n",
    "    report(y_tr, name_train)\n",
    "    report(y_val, name_val)\n",
    "    report(y_te, name_test)\n",
    "\n",
    "    # quick checks\n",
    "    assert set(np.unique(y_tr)) <= {0, 1}, \"❌ Train labels not binary\"\n",
    "    assert set(np.unique(y_val)) <= {0, 1}, \"❌ Val labels not binary\"\n",
    "    assert set(np.unique(y_te)) <= {0, 1}, \"❌ Test labels not binary\"\n",
    "\n",
    "    fracs = np.array([np.mean(y_tr), np.mean(y_val), np.mean(y_te)])\n",
    "    diff = np.max(fracs) - np.min(fracs)\n",
    "    if diff < 0.01:\n",
    "        print(f\"✅ Class balance preserved (max diff = {diff:.4f})\")\n",
    "    else:\n",
    "        print(f\"⚠️ Class ratio differs across splits (max diff = {diff:.4f})\")\n",
    "\n",
    "    print(\"=====================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc7c1438-83ba-46e1-bda5-35f40c619de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sanity Check: Stratified Split ===\n",
      "Total samples: 328,135\n",
      "Train       : n=229695   | positives=0.0883 | negatives=0.9117\n",
      "Validation  : n=49220    | positives=0.0883 | negatives=0.9117\n",
      "Test        : n=49220    | positives=0.0883 | negatives=0.9117\n",
      "✅ Class balance preserved (max diff = 0.0000)\n",
      "=====================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanity_check_split(y_tr, y_va, y_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fb252154-ffa5-42e3-9044-e0e7b676f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_integer_and_categorical_features(X, unique_threshold=10, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Detects which features in X are integer-like and which are categorical\n",
    "    (integer-like with few unique values).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        2D array of shape (n_samples, n_features).\n",
    "    unique_threshold : int\n",
    "        Maximum number of unique values (excluding NaNs) to consider a feature categorical.\n",
    "    tol : float\n",
    "        Numerical tolerance for detecting integer-like values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int_count : int\n",
    "        Number of integer-like features.\n",
    "    cat_count : int\n",
    "        Number of categorical features (integer-like with ≤ unique_threshold values).\n",
    "    int_mask : np.ndarray (bool)\n",
    "        Mask for integer-like features (True = integer-like).\n",
    "    cat_mask : np.ndarray (bool)\n",
    "        Mask for categorical features (True = categorical).\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    int_mask = np.zeros(n_features, dtype=bool)\n",
    "    cat_mask = np.zeros(n_features, dtype=bool)\n",
    "\n",
    "    for j in range(n_features):\n",
    "        col = X[:, j]\n",
    "        col_nonan = col[~np.isnan(col)]\n",
    "        if len(col_nonan) == 0:\n",
    "            continue\n",
    "\n",
    "        # Check if column is integer-like\n",
    "        if np.all(np.abs(col_nonan - np.round(col_nonan)) < tol):\n",
    "            int_mask[j] = True\n",
    "            # If also low-cardinality, mark as categorical\n",
    "            unique_vals = np.unique(col_nonan)\n",
    "            if len(unique_vals) <= unique_threshold:\n",
    "                cat_mask[j] = True\n",
    "\n",
    "    int_count = np.sum(int_mask)\n",
    "    cat_count = np.sum(cat_mask)\n",
    "    return int_count, cat_count, int_mask, cat_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "216d0659-a6ee-4a03-8dd3-0a567f6b7d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 102\n"
     ]
    }
   ],
   "source": [
    "int_count, cat_count, int_mask, cat_mask = detect_integer_and_categorical_features(x_train_2, tol = 1e-12)\n",
    "print(int_count, cat_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "87efecf3-6c5e-4563-9206-45186e831343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(X, cat_mask, reference_stats=None, numeric_strategy=\"median\"):\n",
    "    \"\"\"\n",
    "    Impute NaNs:\n",
    "      - numerical (cat_mask=False): median or mean (choose via numeric_strategy)\n",
    "      - categorical (cat_mask=True): mode (most frequent)\n",
    "\n",
    "    If reference_stats is provided (from training), they are used directly.\n",
    "    Otherwise, stats are computed from X and returned for reuse.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (n_samples, n_features)\n",
    "    cat_mask : np.ndarray of bool, shape (n_features,)\n",
    "    reference_stats : list/np.ndarray or None\n",
    "        Per-column fill values computed on the training set.\n",
    "    numeric_strategy : {\"median\",\"mean\"}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_imp : np.ndarray\n",
    "    stats : list of length n_features (per-column fill values)\n",
    "    \"\"\"\n",
    "    X_imp = X.copy()\n",
    "    n_features = X_imp.shape[1]\n",
    "    if cat_mask.shape[0] != n_features:\n",
    "        raise ValueError(\"cat_mask length must match number of columns in X.\")\n",
    "\n",
    "    if reference_stats is None:\n",
    "        stats = [None] * n_features\n",
    "        for j in range(n_features):\n",
    "            col = X_imp[:, j]\n",
    "            missing = np.isnan(col)\n",
    "            if np.all(missing):\n",
    "                # Degenerate case: all missing. Choose a safe default.\n",
    "                # For categorical, use 0; for numeric, use 0.0\n",
    "                fill = 0.0 if not cat_mask[j] else 0.0\n",
    "            else:\n",
    "                if cat_mask[j]:\n",
    "                    # mode\n",
    "                    vals, counts = np.unique(col[~missing], return_counts=True)\n",
    "                    fill = vals[np.argmax(counts)]\n",
    "                else:\n",
    "                    if numeric_strategy == \"mean\":\n",
    "                        fill = np.nanmean(col)\n",
    "                    else:  # default median\n",
    "                        fill = np.nanmedian(col)\n",
    "            if np.any(missing):\n",
    "                X_imp[missing, j] = fill\n",
    "            stats[j] = float(fill)\n",
    "        return X_imp, stats\n",
    "    else:\n",
    "        # Use provided stats; must match n_features\n",
    "        if len(reference_stats) != n_features:\n",
    "            raise ValueError(\"reference_stats length does not match number of columns in X.\")\n",
    "        for j in range(n_features):\n",
    "            fill = reference_stats[j]\n",
    "            missing = np.isnan(X_imp[:, j])\n",
    "            if np.any(missing):\n",
    "                X_imp[missing, j] = fill\n",
    "        return X_imp, list(reference_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "824134a8-7210-4359-bf8a-7e21008cac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training\n",
    "X_tr_imp, impute_stats = impute_missing_values(x_tr, cat_mask, numeric_strategy=\"median\")\n",
    "\n",
    "# Apply to val/test with the same stats\n",
    "X_val_imp, _ = impute_missing_values(x_va, cat_mask, reference_stats=impute_stats)\n",
    "X_te_imp,  _ = impute_missing_values(x_te,  cat_mask, reference_stats=impute_stats)\n",
    "X_test_imp, _ = impute_missing_values(X_test, cat_mask, reference_stats=impute_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0a3e9783-bc06-438b-9204-53e8092c1256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Imputation Check ===\n",
      "Train shape: (229695, 163)\n",
      "Val shape:   (49220, 163)\n",
      "Test shape:  (49220, 163)\n",
      "NaNs remaining in train: 0\n",
      "NaNs remaining in val:   0\n",
      "NaNs remaining in test:  0\n",
      "\n",
      "Example fill values:\n",
      "  Feature 0: Numeric, fill value = 29.0\n",
      "  Feature 1: Numeric, fill value = 6.0\n",
      "  Feature 2: Numeric, fill value = 6242015.0\n",
      "  Feature 3: Numeric, fill value = 6.0\n",
      "  Feature 4: Numeric, fill value = 14.0\n",
      "\n",
      "✅ Imputation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ✅ Check that imputation worked correctly\n",
    "print(\"=== Imputation Check ===\")\n",
    "print(f\"Train shape: {X_tr_imp.shape}\")\n",
    "print(f\"Val shape:   {X_val_imp.shape}\")\n",
    "print(f\"Test shape:  {X_te_imp.shape}\")\n",
    "\n",
    "# Check for remaining NaNs\n",
    "print(f\"NaNs remaining in train: {np.isnan(X_tr_imp).sum()}\")\n",
    "print(f\"NaNs remaining in val:   {np.isnan(X_val_imp).sum()}\")\n",
    "print(f\"NaNs remaining in test:  {np.isnan(X_te_imp).sum()}\")\n",
    "\n",
    "# Check some sample statistics\n",
    "print(\"\\nExample fill values:\")\n",
    "for j in range(min(5, len(impute_stats))):\n",
    "    kind = \"Categorical\" if cat_mask[j] else \"Numeric\"\n",
    "    print(f\"  Feature {j}: {kind}, fill value = {impute_stats[j]}\")\n",
    "\n",
    "print(\"\\n✅ Imputation completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "53e3c4f2-2861-4303-a900-6389f59dbd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_low_variance_or_correlation(X, y, cat_mask, \n",
    "                                     min_var=1e-8, \n",
    "                                     min_corr=0.2, \n",
    "                                     min_cat_assoc=1e-1):\n",
    "    \"\"\"\n",
    "    Drops columns with low variance (for numerics) or weak association (for categoricals)\n",
    "    with the target y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features)\n",
    "    y : np.ndarray\n",
    "        Target vector (n_samples,)\n",
    "    cat_mask : np.ndarray of bool\n",
    "        Mask indicating which features are categorical\n",
    "    min_var : float\n",
    "        Minimum variance to keep a numeric feature\n",
    "    min_corr : float\n",
    "        Minimum |Pearson correlation| to keep a numeric feature\n",
    "    min_cat_assoc : float\n",
    "        Minimum normalized chi2-like association to keep a categorical feature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_filtered : np.ndarray\n",
    "        Matrix with uninformative features removed\n",
    "    keep_mask : np.ndarray of bool\n",
    "        Boolean mask of kept features\n",
    "    dropped_info : dict\n",
    "        Information summary about dropped features\n",
    "    cat_mask_new : np.ndarray of bool\n",
    "        Updated categorical mask\n",
    "    num_mask_new : np.ndarray of bool\n",
    "        Updated numerical mask\n",
    "    \"\"\"\n",
    "\n",
    "    X_copy = X.copy()\n",
    "    n, d = X_copy.shape\n",
    "\n",
    "    # 1️⃣ --- Variance filter (numeric only)\n",
    "    var = np.var(X_copy, axis=0)\n",
    "    var_mask = np.ones(d, dtype=bool)\n",
    "    var_mask[~cat_mask] = var[~cat_mask] > min_var  # numeric only\n",
    "\n",
    "    # 2️⃣ --- Correlation filter for numeric, association for categorical\n",
    "    scores = np.zeros(d)\n",
    "\n",
    "    for j in range(d):\n",
    "        col = X_copy[:, j]\n",
    "        if cat_mask[j]:\n",
    "            # categorical feature: compute normalized chi² association\n",
    "            vals, counts = np.unique(col, return_counts=True)\n",
    "            if len(vals) < 2:\n",
    "                scores[j] = 0.0\n",
    "                continue\n",
    "            p_y = y.mean()\n",
    "            chi2 = 0.0\n",
    "            for v, cnt in zip(vals, counts):\n",
    "                mask = (col == v)\n",
    "                n_v = mask.sum()\n",
    "                if n_v == 0:\n",
    "                    continue\n",
    "                p1 = y[mask].mean()\n",
    "                expected = n_v * p_y\n",
    "                observed = n_v * p1\n",
    "                chi2 += (observed - expected) ** 2 / (expected + 1e-12)\n",
    "            scores[j] = chi2 / n  # normalized association strength\n",
    "        else:\n",
    "            # numeric feature: Pearson correlation\n",
    "            if np.std(col) < 1e-12:\n",
    "                scores[j] = 0.0\n",
    "            else:\n",
    "                corr = np.corrcoef(col, y)[0, 1]\n",
    "                scores[j] = 0.0 if np.isnan(corr) else abs(corr)\n",
    "\n",
    "    # threshold differently by type\n",
    "    assoc_mask = np.ones(d, dtype=bool)\n",
    "    assoc_mask[cat_mask] = scores[cat_mask] > min_cat_assoc\n",
    "    assoc_mask[~cat_mask] = scores[~cat_mask] > min_corr\n",
    "\n",
    "    # 3️⃣ --- Combine filters\n",
    "    keep_mask = var_mask & assoc_mask\n",
    "    X_filtered = X_copy[:, keep_mask]\n",
    "\n",
    "    # 4️⃣ --- Update masks\n",
    "    cat_mask_new = cat_mask[keep_mask]\n",
    "    num_mask_new = ~cat_mask_new\n",
    "\n",
    "    # 5️⃣ --- Info summary\n",
    "    dropped_info = {\n",
    "        \"total_features\": d,\n",
    "        \"kept_features\": int(np.sum(keep_mask)),\n",
    "        \"dropped_low_variance\": int(np.sum(~var_mask)),\n",
    "        \"dropped_low_assoc_or_corr\": int(np.sum(~assoc_mask & var_mask))\n",
    "    }\n",
    "\n",
    "    return X_filtered, keep_mask, dropped_info, cat_mask_new, num_mask_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d4255d0e-c0bf-4111-bb15-a59f93e4ce98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Selection Summary ===\n",
      "Total features before:       163\n",
      "Kept features:               35\n",
      "Dropped (low variance):      0\n",
      "Dropped (low assoc/corr):    128\n"
     ]
    }
   ],
   "source": [
    "# 🧩 Feature filtering\n",
    "X_tr_filt, keep_mask, info, cat_mask, num_mask = drop_low_variance_or_correlation(\n",
    "    X_tr_imp, y_tr, cat_mask,\n",
    "    min_var=1e-2,       # numeric variance threshold\n",
    "    min_corr=0.2,      # numeric correlation threshold\n",
    "    min_cat_assoc=1e-2 # categorical association threshold\n",
    ")\n",
    "\n",
    "# ✅ Summary\n",
    "print(\"=== Feature Selection Summary ===\")\n",
    "print(f\"Total features before:       {info['total_features']}\")\n",
    "print(f\"Kept features:               {info['kept_features']}\")\n",
    "print(f\"Dropped (low variance):      {info['dropped_low_variance']}\")\n",
    "print(f\"Dropped (low assoc/corr):    {info['dropped_low_assoc_or_corr']}\")\n",
    "\n",
    "# ✅ Apply same mask to validation and test data\n",
    "X_val_filt  = X_val_imp[:, keep_mask]\n",
    "X_te_filt   = X_te_imp[:, keep_mask]\n",
    "X_test_filt = X_test_imp[:, keep_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5bcf36c3-2d61-4f6c-ae0f-c7dfb93a650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def standardize_numeric_features(X, num_mask, reference_stats=None):\n",
    "    \"\"\"\n",
    "    Standardize only numerical features (mean=0, std=1),\n",
    "    leaving categorical ones unchanged.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features)\n",
    "    num_mask : np.ndarray of bool\n",
    "        Mask of numeric features (True = numeric)\n",
    "    reference_stats : dict or None\n",
    "        If None, compute mean/std from X (training set).\n",
    "        If provided, apply them (validation/test sets).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_scaled : np.ndarray\n",
    "        Matrix with standardized numeric features.\n",
    "    stats : dict\n",
    "        {\"mean\": mean_vector, \"std\": std_vector} for reuse on val/test.\n",
    "    \"\"\"\n",
    "    X_scaled = X.astype(float).copy()\n",
    "\n",
    "    # Safety check\n",
    "    if num_mask.shape[0] != X.shape[1]:\n",
    "        raise ValueError(\"num_mask length must match number of columns in X.\")\n",
    "\n",
    "    # --- Compute or apply scaling ---\n",
    "    if reference_stats is None:\n",
    "        mean = np.mean(X[:, num_mask], axis=0)\n",
    "        std = np.std(X[:, num_mask], axis=0)\n",
    "        std[std == 0] = 1.0  # avoid division by zero\n",
    "        X_scaled[:, num_mask] = (X[:, num_mask] - mean) / std\n",
    "        stats = {\"mean\": mean, \"std\": std}\n",
    "    else:\n",
    "        mean = reference_stats[\"mean\"]\n",
    "        std = reference_stats[\"std\"]\n",
    "        X_scaled[:, num_mask] = (X[:, num_mask] - mean) / std\n",
    "        stats = reference_stats\n",
    "\n",
    "    return X_scaled, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "da6730b8-70f8-41d7-9d71-5b3a37d7ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Fit on training data\n",
    "X_tr_scaled, scale_stats = standardize_numeric_features(X_tr_filt, num_mask)\n",
    "\n",
    "# 2️⃣ Apply to validation/test sets (no leakage)\n",
    "X_va_scaled, _ = standardize_numeric_features(X_val_filt, num_mask, scale_stats)\n",
    "X_te_scaled, _  = standardize_numeric_features(X_te_filt,  num_mask, scale_stats)\n",
    "X_test_scaled, _ = standardize_numeric_features(X_test_filt,  num_mask, scale_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fd31b677-0802-4e31-87fb-ed833cd51e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of numeric features (train): [ 1.25469050e-16 -2.19756442e-16]\n",
      "Std  of numeric features (train): [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of numeric features (train):\", np.mean(X_tr_scaled[:, num_mask], axis=0)[:5])\n",
    "print(\"Std  of numeric features (train):\", np.std(X_tr_scaled[:, num_mask], axis=0)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2e068763-dbeb-4896-ba2d-d19f85e9b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numeric features: 2\n",
      "Number of categorical features: 33\n",
      "num_mask shape: (35,)\n",
      "X_tr_filt shape: (229695, 35)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of numeric features:\", np.sum(num_mask))\n",
    "print(\"Number of categorical features:\", np.sum(~num_mask))\n",
    "print(\"num_mask shape:\", num_mask.shape)\n",
    "print(\"X_tr_filt shape:\", X_tr_filt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "73dbbf4e-33eb-4d51-82b5-32d59175a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_all_categories(X, cat_mask, reference_uniques=None):\n",
    "    \"\"\"\n",
    "    One-hot encodes *all* categorical features (pure binary 0/1),\n",
    "    leaving numeric features untouched.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Data matrix.\n",
    "    cat_mask : np.ndarray of bool\n",
    "        Mask where True marks categorical features.\n",
    "    reference_uniques : list or None\n",
    "        If provided, use these unique values (from training set)\n",
    "        to ensure consistent encoding across val/test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_encoded : np.ndarray\n",
    "        Encoded matrix with only numeric + one-hot binary columns.\n",
    "    uniques_list : list\n",
    "        List of unique categories per categorical feature (for reuse).\n",
    "    \"\"\"\n",
    "\n",
    "    n, d = X.shape\n",
    "    X_parts = []\n",
    "    uniques_list = []\n",
    "\n",
    "    for j in range(d):\n",
    "        col = X[:, j]\n",
    "        if cat_mask[j]:\n",
    "            # Use provided unique values (for val/test) or compute from X\n",
    "            if reference_uniques is None:\n",
    "                uniques = np.unique(col[~np.isnan(col)])  # ignore NaNs\n",
    "            else:\n",
    "                uniques = reference_uniques[j]\n",
    "\n",
    "            uniques_list.append(uniques)\n",
    "\n",
    "            # One-hot encode (drop first category to avoid dummy trap)\n",
    "            one_hot = np.zeros((n, len(uniques) - 1))\n",
    "            for i, u in enumerate(uniques[1:]):\n",
    "                one_hot[:, i] = (col == u).astype(float)\n",
    "\n",
    "            X_parts.append(one_hot)\n",
    "\n",
    "        else:\n",
    "            # Numeric feature → keep as is\n",
    "            X_parts.append(col.reshape(-1, 1))\n",
    "            uniques_list.append(None)\n",
    "\n",
    "    X_encoded = np.concatenate(X_parts, axis=1)\n",
    "    return X_encoded, uniques_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e32050ac-f929-404d-a9e5-3920ca7c396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On training set\n",
    "X_tr_encoded, uniques_list = one_hot_encode_all_categories(X_tr_scaled, cat_mask)\n",
    "\n",
    "# On validation and test (same category mapping)\n",
    "X_va_encoded, _ = one_hot_encode_all_categories(X_va_scaled, cat_mask, uniques_list)\n",
    "X_te_encoded, _ = one_hot_encode_all_categories(X_te_scaled, cat_mask, uniques_list)\n",
    "X_test_encoded, _ = one_hot_encode_all_categories(X_test_scaled, cat_mask, uniques_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ff72bd25-a7e3-456a-a765-be54a67125ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== One-Hot Encoding Sanity Check ===\n",
      "Train shape: (229695, 124)\n",
      "Val shape:   (49220, 124)\n",
      "Test shape:  (49220, 124)\n",
      "NaNs in train: 0\n",
      "NaNs in val:   0\n",
      "NaNs in test:  0\n",
      "\n",
      "Features one-hot encoded: 23\n",
      "New columns added: 89\n",
      "\n",
      "Data type of X_tr_encoded: float64\n",
      "\n",
      "✅ One-hot encoding looks good!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== One-Hot Encoding Sanity Check ===\")\n",
    "\n",
    "# 1️⃣ Shapes\n",
    "print(f\"Train shape: {X_tr_encoded.shape}\")\n",
    "print(f\"Val shape:   {X_va_encoded.shape}\")\n",
    "print(f\"Test shape:  {X_te_encoded.shape}\")\n",
    "\n",
    "# 2️⃣ Column consistency\n",
    "assert X_tr_encoded.shape[1] == X_va_encoded.shape[1] == X_te_encoded.shape[1], \\\n",
    "    \"❌ Mismatch in feature counts between splits!\"\n",
    "\n",
    "# 3️⃣ Check for NaNs\n",
    "print(f\"NaNs in train: {np.isnan(X_tr_encoded).sum()}\")\n",
    "print(f\"NaNs in val:   {np.isnan(X_va_encoded).sum()}\")\n",
    "print(f\"NaNs in test:  {np.isnan(X_te_encoded).sum()}\")\n",
    "\n",
    "# 4️⃣ How many categorical features were one-hot encoded\n",
    "encoded_features = sum(\n",
    "    (uniques is not None and 3 <= len(uniques) <= 5)\n",
    "    for uniques in uniques_list if uniques is not None\n",
    ")\n",
    "total_added = X_tr_encoded.shape[1] - X_tr_scaled.shape[1]\n",
    "\n",
    "print(f\"\\nFeatures one-hot encoded: {encoded_features}\")\n",
    "print(f\"New columns added: {total_added}\")\n",
    "\n",
    "# 5️⃣ Quick data type check\n",
    "print(f\"\\nData type of X_tr_encoded: {X_tr_encoded.dtype}\")\n",
    "\n",
    "print(\"\\n✅ One-hot encoding looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c62473a4-01fd-447a-935c-e47bc434327b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded train shape: (229695, 124)\n",
      "Min/Max categorical: -2.1916497651573863 1.7761965566945117\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded train shape:\", X_tr_encoded.shape)\n",
    "print(\"Min/Max categorical:\", np.min(X_tr_encoded), np.max(X_tr_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9edb1e0c-4c5e-4766-999f-e63eef781d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_encoded_masks(num_mask, cat_mask, uniques_list):\n",
    "    \"\"\"\n",
    "    After one-hot encoding, rebuilds masks for the encoded dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_mask : np.ndarray of bool\n",
    "        Original numeric mask (before encoding)\n",
    "    cat_mask : np.ndarray of bool\n",
    "        Original categorical mask (before encoding)\n",
    "    uniques_list : list\n",
    "        List of unique categories per original feature\n",
    "        (from the one-hot encoder)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    num_mask_encoded, cat_mask_encoded : np.ndarray of bool\n",
    "        Boolean masks aligned with encoded data shape\n",
    "    \"\"\"\n",
    "\n",
    "    num_mask_encoded = []\n",
    "    cat_mask_encoded = []\n",
    "\n",
    "    for is_num, is_cat, uniques in zip(num_mask, cat_mask, uniques_list):\n",
    "        if is_num:\n",
    "            # numeric column -> stays numeric\n",
    "            num_mask_encoded.append(True)\n",
    "            cat_mask_encoded.append(False)\n",
    "\n",
    "        elif is_cat:\n",
    "            if uniques is None:\n",
    "                # categorical feature but no encoding? -> error check\n",
    "                raise ValueError(\"Categorical feature without uniques_list entry\")\n",
    "            else:\n",
    "                # one-hot created (len(uniques)-1) binary columns\n",
    "                n_new = len(uniques) - 1\n",
    "                num_mask_encoded.extend([False] * n_new)\n",
    "                cat_mask_encoded.extend([True] * n_new)\n",
    "\n",
    "    return np.array(num_mask_encoded), np.array(cat_mask_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f99aea9d-09f0-4281-a32d-7e3aaf23b1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded masks: (124,) (124,)\n",
      "Numeric mean: -4.6772485969188335e-17 std: 1.0\n",
      "Categorical min: 0.0 max: 1.0\n"
     ]
    }
   ],
   "source": [
    "num_mask_encoded, cat_mask_encoded = rebuild_encoded_masks(num_mask, cat_mask, uniques_list)\n",
    "print(\"Encoded masks:\", num_mask_encoded.shape, cat_mask_encoded.shape)\n",
    "num_vals = X_tr_encoded[:, num_mask_encoded]\n",
    "cat_vals = X_tr_encoded[:, cat_mask_encoded]\n",
    "\n",
    "print(\"Numeric mean:\", np.mean(num_vals), \"std:\", np.std(num_vals))\n",
    "print(\"Categorical min:\", np.min(cat_vals), \"max:\", np.max(cat_vals))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "20e89752-8086-4973-9e1c-06d70c7f086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18765136657376646 0.41056955982492044\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(X_tr_encoded), np.std(X_tr_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "46a2a11c-9c87-4a33-bea5-d90d20076cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Numerically stable sigmoid.\"\"\"\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def logistic_loss(y, tx, w):\n",
    "    \"\"\"Compute standard (unpenalized) logistic loss.\"\"\"\n",
    "    pred = sigmoid(tx @ w)\n",
    "    eps = 1e-15  # to avoid log(0)\n",
    "    loss = -np.mean(y * np.log(pred + eps) + (1 - y) * np.log(1 - pred + eps))\n",
    "    return float(loss)\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def weighted_logistic_loss(y, tx, w, lambda_=0.0, pos_weight=1.0, neg_weight=1.0):\n",
    "    \"\"\"\n",
    "    Weighted (and optionally penalized) logistic loss.\n",
    "    Class weights allow balancing for imbalanced datasets.\n",
    "    \"\"\"\n",
    "    p = sigmoid(tx @ w)\n",
    "    eps = 1e-15  # avoid log(0)\n",
    "\n",
    "    # weights per sample\n",
    "    sample_weights = np.where(y == 1, pos_weight, neg_weight)\n",
    "\n",
    "    # weighted average loss\n",
    "    loss = -np.sum(sample_weights * (y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))) / np.sum(sample_weights)\n",
    "    \n",
    "    # no regularization term added to returned loss (for monitoring only)\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "def weighted_gradient_logistic(y, tx, w, lambda_=0.0, pos_weight=1.0, neg_weight=1.0):\n",
    "    \"\"\"\n",
    "    Gradient of the weighted logistic loss with L2 penalty.\n",
    "    \"\"\"\n",
    "    p = sigmoid(tx @ w)\n",
    "    sample_weights = np.where(y == 1, pos_weight, neg_weight)\n",
    "    error = sample_weights * (p - y)\n",
    "    grad = (tx.T @ error) / np.sum(sample_weights)\n",
    "    grad[1:] += 2 * lambda_ * w[1:]  # don't regularize bias\n",
    "    return grad.ravel()\n",
    "\n",
    "\n",
    "\n",
    "def compute_gradient_logistic(y, tx, w, lambda_=0.0):\n",
    "    \"\"\"Compute penalized gradient of logistic loss.\"\"\"\n",
    "    pred = sigmoid(tx @ w)\n",
    "    error = pred - y\n",
    "    grad = (tx.T @ error) / len(y)\n",
    "    grad[1:] += 2 * lambda_ * w[1:]  # don't regularize bias\n",
    "    return grad.ravel()\n",
    "\n",
    "\n",
    "def logistic_regression_penalized(\n",
    "    y, x, lambda_=1e-3, gamma=0.05, max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Logistic regression with L2 penalization in the gradient step only.\n",
    "    Returns (loss, w), where:\n",
    "      - loss = final *unpenalized* logistic loss (float)\n",
    "      - w = final weights (1D np.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    # Add bias column\n",
    "    tx = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    w = np.zeros(tx.shape[1])  # 1D weights\n",
    "    losses = []\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Compute gradient and loss\n",
    "        grad = compute_gradient_logistic(y, tx, w, lambda_)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        if grad_norm > clip_grad:\n",
    "            grad *= clip_grad / grad_norm  # stability\n",
    "\n",
    "        loss = logistic_loss(y, tx, w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Update weights\n",
    "        w -= gamma * grad\n",
    "\n",
    "        # Convergence check\n",
    "        if it > 0 and abs(losses[-1] - losses[-2]) < tol:\n",
    "            if verbose:\n",
    "                print(f\"✅ Converged at iteration {it}\")\n",
    "            break\n",
    "\n",
    "        if verbose and it % 100 == 0:\n",
    "            print(f\"Iter {it:5d} | Loss = {loss:.6f} | GradNorm = {grad_norm:.4f}\")\n",
    "\n",
    "    # Return *last* unpenalized loss and final weights\n",
    "    return losses[-1], w\n",
    "\n",
    "def logistic_regression_weighted_gd(\n",
    "    y, x, lambda_=1e-3, gamma=0.05, pos_weight=1.0, neg_weight=1.0,\n",
    "    max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Logistic regression with class weights and L2 regularization.\n",
    "    Returns (loss, w).\n",
    "    \"\"\"\n",
    "    tx = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    losses = []\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        grad = weighted_gradient_logistic(y, tx, w, lambda_, pos_weight, neg_weight)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        if grad_norm > clip_grad:\n",
    "            grad *= clip_grad / grad_norm\n",
    "\n",
    "        loss = weighted_logistic_loss(y, tx, w, lambda_, pos_weight, neg_weight)\n",
    "        losses.append(loss)\n",
    "\n",
    "        w -= gamma * grad\n",
    "\n",
    "        if it > 0 and abs(losses[-1] - losses[-2]) < tol:\n",
    "            if verbose:\n",
    "                print(f\"✅ Converged at iteration {it}\")\n",
    "            break\n",
    "\n",
    "        if verbose and it % 100 == 0:\n",
    "            print(f\"Iter {it:5d} | Loss = {loss:.6f} | GradNorm = {grad_norm:.4f}\")\n",
    "\n",
    "    return losses[-1], w\n",
    "\n",
    "def accuracy_numpy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy using NumPy.\n",
    "    \"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "\n",
    "# --- Full evaluation wrapper ---\n",
    "def evaluate_model(y_true, X, w, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate trained logistic regression on a dataset.\n",
    "    Returns accuracy and F1 score.\n",
    "    \"\"\"\n",
    "    preds, probs = predict_with_threshold(X, w, threshold=threshold)\n",
    "    acc = accuracy_numpy(y_true, preds)\n",
    "    f1 = f1_score(y_true, preds)\n",
    "    print(f\"✅ Accuracy: {acc*100:.2f}%\")\n",
    "    print(f\"✅ F1 Score: {f1:.4f}\")\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "30453234-d767-4778-9b02-817bea7cd71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_threshold(x, w, threshold=0.5):\n",
    "    tx = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    probs = sigmoid(tx @ w)\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c588ac2b-aed2-4cbd-8ca7-6372bef6327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute F1 score using only NumPy.\n",
    "    Works for binary classification (0/1).\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-15)\n",
    "    recall = tp / (tp + fn + 1e-15)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-15)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5cb5bfd1-781a-4865-b9f9-3d1aefc94720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_grid_search(\n",
    "    y_train, X_train,\n",
    "    y_val, X_val,\n",
    "    pos_weights=[8, 9],\n",
    "    lambdas=[1e-7, 1e-3],\n",
    "    thresholds=np.linspace(0.6, 0.85, 40),\n",
    "    max_iter=10000,\n",
    "    gamma=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Safe grid search for weighted penalized logistic regression.\n",
    "    Returns: best_params, best_f1, results_list\n",
    "    \"\"\"\n",
    "    best_f1 = -1\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    total = len(pos_weights) * len(lambdas) * len(thresholds)\n",
    "    run = 0\n",
    "\n",
    "    for pw in pos_weights:\n",
    "        for lam in lambdas:\n",
    "            run += 1\n",
    "            print(f\"\\n=== Run {run}/{total//len(thresholds)} (pos_weight={pw}, lambda_={lam}) ===\")\n",
    "\n",
    "            try:\n",
    "                # Train model\n",
    "                loss, w = logistic_regression_weighted_gd(\n",
    "                    y_train, X_train,\n",
    "                    lambda_=lam,\n",
    "                    gamma=gamma,\n",
    "                    pos_weight=pw,\n",
    "                    neg_weight=1.0,\n",
    "                    max_iter=max_iter,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                # Skip invalid runs\n",
    "                if np.isnan(loss) or np.isinf(loss) or loss > 10:\n",
    "                    print(f\"⚠️  Invalid loss ({loss:.4f}), skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Evaluate all thresholds for this model\n",
    "                for th in thresholds:\n",
    "                    preds, _ = predict_with_threshold(X_val, w, threshold=th)\n",
    "                    f1 = f1_score(y_val, preds)\n",
    "                    results.append((pw, lam, th, f1))\n",
    "\n",
    "                    print(f\"   → threshold={th:.2f} | F1={f1:.4f}\")\n",
    "\n",
    "                    # Update best model\n",
    "                    if f1 > best_f1:\n",
    "                        best_f1 = f1\n",
    "                        best_params = (pw, lam, th)\n",
    "                        print(f\"   ✅ New best F1 = {best_f1:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error for pos_weight={pw}, lambda_={lam}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Sort results by F1 descending\n",
    "    results.sort(key=lambda t: t[3], reverse=True)\n",
    "\n",
    "    print(\"\\n=== 🏁 Grid Search Complete ===\")\n",
    "    if best_params:\n",
    "        print(f\"🏆 Best F1 = {best_f1:.4f} at pos_weight={best_params[0]}, λ={best_params[1]}, threshold={best_params[2]}\")\n",
    "    else:\n",
    "        print(\"⚠️ No valid runs completed.\")\n",
    "\n",
    "    return best_params, best_f1, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9114ce7a-bf3f-4c02-a203-dd35e31ae233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/9 (pos_weight=8, lambda_=1e-08) ===\n",
      "   → threshold=0.65 | F1=0.3792\n",
      "   ✅ New best F1 = 0.3792\n",
      "   → threshold=0.66 | F1=0.3827\n",
      "   ✅ New best F1 = 0.3827\n",
      "   → threshold=0.67 | F1=0.3861\n",
      "   ✅ New best F1 = 0.3861\n",
      "   → threshold=0.68 | F1=0.3900\n",
      "   ✅ New best F1 = 0.3900\n",
      "   → threshold=0.69 | F1=0.3929\n",
      "   ✅ New best F1 = 0.3929\n",
      "   → threshold=0.71 | F1=0.3958\n",
      "   ✅ New best F1 = 0.3958\n",
      "   → threshold=0.72 | F1=0.3982\n",
      "   ✅ New best F1 = 0.3982\n",
      "   → threshold=0.73 | F1=0.4003\n",
      "   ✅ New best F1 = 0.4003\n",
      "   → threshold=0.74 | F1=0.4039\n",
      "   ✅ New best F1 = 0.4039\n",
      "   → threshold=0.75 | F1=0.4058\n",
      "   ✅ New best F1 = 0.4058\n",
      "\n",
      "=== Run 2/9 (pos_weight=8, lambda_=1e-07) ===\n",
      "   → threshold=0.65 | F1=0.3792\n",
      "   → threshold=0.66 | F1=0.3827\n",
      "   → threshold=0.67 | F1=0.3861\n",
      "   → threshold=0.68 | F1=0.3900\n",
      "   → threshold=0.69 | F1=0.3929\n",
      "   → threshold=0.71 | F1=0.3958\n",
      "   → threshold=0.72 | F1=0.3982\n",
      "   → threshold=0.73 | F1=0.4003\n",
      "   → threshold=0.74 | F1=0.4039\n",
      "   → threshold=0.75 | F1=0.4058\n",
      "\n",
      "=== Run 3/9 (pos_weight=8, lambda_=0.0001) ===\n",
      "   → threshold=0.65 | F1=0.3770\n",
      "   → threshold=0.66 | F1=0.3804\n",
      "   → threshold=0.67 | F1=0.3842\n",
      "   → threshold=0.68 | F1=0.3874\n",
      "   → threshold=0.69 | F1=0.3916\n",
      "   → threshold=0.71 | F1=0.3946\n",
      "   → threshold=0.72 | F1=0.3976\n",
      "   → threshold=0.73 | F1=0.3993\n",
      "   → threshold=0.74 | F1=0.4020\n",
      "   → threshold=0.75 | F1=0.4051\n",
      "\n",
      "=== Run 4/9 (pos_weight=9, lambda_=1e-08) ===\n",
      "   → threshold=0.65 | F1=0.3672\n",
      "   → threshold=0.66 | F1=0.3704\n",
      "   → threshold=0.67 | F1=0.3739\n",
      "   → threshold=0.68 | F1=0.3783\n",
      "   → threshold=0.69 | F1=0.3823\n",
      "   → threshold=0.71 | F1=0.3860\n",
      "   → threshold=0.72 | F1=0.3903\n",
      "   → threshold=0.73 | F1=0.3930\n",
      "   → threshold=0.74 | F1=0.3960\n",
      "   → threshold=0.75 | F1=0.3989\n",
      "\n",
      "=== Run 5/9 (pos_weight=9, lambda_=1e-07) ===\n",
      "   → threshold=0.65 | F1=0.3671\n",
      "   → threshold=0.66 | F1=0.3704\n",
      "   → threshold=0.67 | F1=0.3739\n",
      "   → threshold=0.68 | F1=0.3783\n",
      "   → threshold=0.69 | F1=0.3823\n",
      "   → threshold=0.71 | F1=0.3860\n",
      "   → threshold=0.72 | F1=0.3903\n",
      "   → threshold=0.73 | F1=0.3930\n",
      "   → threshold=0.74 | F1=0.3960\n",
      "   → threshold=0.75 | F1=0.3989\n",
      "\n",
      "=== Run 6/9 (pos_weight=9, lambda_=0.0001) ===\n",
      "   → threshold=0.65 | F1=0.3646\n",
      "   → threshold=0.66 | F1=0.3679\n",
      "   → threshold=0.67 | F1=0.3717\n",
      "   → threshold=0.68 | F1=0.3762\n",
      "   → threshold=0.69 | F1=0.3800\n",
      "   → threshold=0.71 | F1=0.3843\n",
      "   → threshold=0.72 | F1=0.3876\n",
      "   → threshold=0.73 | F1=0.3913\n",
      "   → threshold=0.74 | F1=0.3947\n",
      "   → threshold=0.75 | F1=0.3977\n",
      "\n",
      "=== Run 7/9 (pos_weight=10, lambda_=1e-08) ===\n",
      "   → threshold=0.65 | F1=0.3569\n",
      "   → threshold=0.66 | F1=0.3609\n",
      "   → threshold=0.67 | F1=0.3656\n",
      "   → threshold=0.68 | F1=0.3690\n",
      "   → threshold=0.69 | F1=0.3730\n",
      "   → threshold=0.71 | F1=0.3777\n",
      "   → threshold=0.72 | F1=0.3816\n",
      "   → threshold=0.73 | F1=0.3854\n",
      "   → threshold=0.74 | F1=0.3896\n",
      "   → threshold=0.75 | F1=0.3933\n",
      "\n",
      "=== Run 8/9 (pos_weight=10, lambda_=1e-07) ===\n",
      "   → threshold=0.65 | F1=0.3569\n",
      "   → threshold=0.66 | F1=0.3609\n",
      "   → threshold=0.67 | F1=0.3656\n",
      "   → threshold=0.68 | F1=0.3690\n",
      "   → threshold=0.69 | F1=0.3730\n",
      "   → threshold=0.71 | F1=0.3777\n",
      "   → threshold=0.72 | F1=0.3816\n",
      "   → threshold=0.73 | F1=0.3853\n",
      "   → threshold=0.74 | F1=0.3896\n",
      "   → threshold=0.75 | F1=0.3933\n",
      "\n",
      "=== Run 9/9 (pos_weight=10, lambda_=0.0001) ===\n",
      "   → threshold=0.65 | F1=0.3538\n",
      "   → threshold=0.66 | F1=0.3583\n",
      "   → threshold=0.67 | F1=0.3626\n",
      "   → threshold=0.68 | F1=0.3668\n",
      "   → threshold=0.69 | F1=0.3707\n",
      "   → threshold=0.71 | F1=0.3753\n",
      "   → threshold=0.72 | F1=0.3790\n",
      "   → threshold=0.73 | F1=0.3831\n",
      "   → threshold=0.74 | F1=0.3873\n",
      "   → threshold=0.75 | F1=0.3910\n",
      "\n",
      "=== 🏁 Grid Search Complete ===\n",
      "🏆 Best F1 = 0.4058 at pos_weight=8, λ=1e-08, threshold=0.75\n"
     ]
    }
   ],
   "source": [
    "best_params, best_f1, results = safe_grid_search(\n",
    "    y_tr, X_tr_encoded,\n",
    "    y_va, X_va_encoded,\n",
    "    pos_weights=[8, 9, 10],\n",
    "    lambdas=[1e-8, 1e-7, 1e-4],\n",
    "    thresholds=np.linspace(0.65, 0.75, 10), \n",
    "    gamma = 0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba2023e0-0d08-4844-86a4-31feed2dca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/6 (pos_weight=9, lambda_=1e-08) ===\n",
      "   ✅ New best F1 = 0.4087\n",
      "   ✅ New best F1 = 0.4109\n",
      "   ✅ New best F1 = 0.4125\n",
      "   ✅ New best F1 = 0.4156\n",
      "\n",
      "=== Run 2/6 (pos_weight=9, lambda_=1e-05) ===\n",
      "\n",
      "=== Run 3/6 (pos_weight=9, lambda_=0.001) ===\n",
      "\n",
      "=== Run 4/6 (pos_weight=10, lambda_=1e-08) ===\n",
      "\n",
      "=== Run 5/6 (pos_weight=10, lambda_=1e-05) ===\n",
      "\n",
      "=== Run 6/6 (pos_weight=10, lambda_=0.001) ===\n",
      "\n",
      "=== 🏁 Grid Search Complete ===\n",
      "🏆 Best F1 = 0.4156 at pos_weight=9, λ=1e-08, threshold=0.6833333333333333\n"
     ]
    }
   ],
   "source": [
    "best_params, best_f1, results = safe_grid_search(\n",
    "    y_tr, X_tr_encoded,\n",
    "    y_va, X_va_encoded,\n",
    "    pos_weights=[9, 10],\n",
    "    lambdas=[1e-8, 1e-5, 1e-3],\n",
    "    thresholds=np.linspace(0.65, 0.75, 10), \n",
    "    gamma = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4d504678-ef56-482c-913a-dc4933265868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/2 (pos_weight=8, lambda_=1e-08) ===\n",
      "   ✅ New best F1 = 0.4064\n",
      "   ✅ New best F1 = 0.4070\n",
      "   ✅ New best F1 = 0.4093\n",
      "   ✅ New best F1 = 0.4105\n",
      "   ✅ New best F1 = 0.4124\n",
      "   ✅ New best F1 = 0.4146\n",
      "   ✅ New best F1 = 0.4153\n",
      "\n",
      "=== Run 2/2 (pos_weight=9, lambda_=1e-08) ===\n",
      "\n",
      "=== 🏁 Grid Search Complete ===\n",
      "🏆 Best F1 = 0.4153 at pos_weight=8, λ=1e-08, threshold=0.6603448275862068\n"
     ]
    }
   ],
   "source": [
    "best_params, best_f1, results = safe_grid_search(\n",
    "    y_tr, X_tr_encoded,\n",
    "    y_va, X_va_encoded,\n",
    "    pos_weights=[8, 9],\n",
    "    lambdas=[1e-8],\n",
    "    thresholds=np.linspace(0.6, 0.85, 30), \n",
    "    gamma = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4578001-d1e1-4ade-ac4b-94379804945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     0 | Loss = 0.693147 | GradNorm = 0.5733\n",
      "Iter   100 | Loss = 0.478305 | GradNorm = 0.0115\n",
      "Iter   200 | Loss = 0.474415 | GradNorm = 0.0067\n",
      "Iter   300 | Loss = 0.472858 | GradNorm = 0.0046\n",
      "Iter   400 | Loss = 0.472064 | GradNorm = 0.0034\n",
      "Iter   500 | Loss = 0.471606 | GradNorm = 0.0027\n",
      "Iter   600 | Loss = 0.471317 | GradNorm = 0.0022\n",
      "Iter   700 | Loss = 0.471121 | GradNorm = 0.0018\n",
      "Iter   800 | Loss = 0.470980 | GradNorm = 0.0016\n",
      "Iter   900 | Loss = 0.470871 | GradNorm = 0.0014\n",
      "Iter  1000 | Loss = 0.470785 | GradNorm = 0.0012\n",
      "Iter  1100 | Loss = 0.470714 | GradNorm = 0.0011\n",
      "Iter  1200 | Loss = 0.470654 | GradNorm = 0.0011\n",
      "Iter  1300 | Loss = 0.470601 | GradNorm = 0.0010\n",
      "Iter  1400 | Loss = 0.470556 | GradNorm = 0.0009\n",
      "Iter  1500 | Loss = 0.470515 | GradNorm = 0.0009\n",
      "Iter  1600 | Loss = 0.470479 | GradNorm = 0.0008\n",
      "Iter  1700 | Loss = 0.470446 | GradNorm = 0.0008\n",
      "Iter  1800 | Loss = 0.470417 | GradNorm = 0.0007\n",
      "Iter  1900 | Loss = 0.470390 | GradNorm = 0.0007\n",
      "Iter  2000 | Loss = 0.470366 | GradNorm = 0.0007\n",
      "Iter  2100 | Loss = 0.470343 | GradNorm = 0.0007\n",
      "Iter  2200 | Loss = 0.470323 | GradNorm = 0.0006\n",
      "Iter  2300 | Loss = 0.470304 | GradNorm = 0.0006\n",
      "Iter  2400 | Loss = 0.470286 | GradNorm = 0.0006\n",
      "Iter  2500 | Loss = 0.470270 | GradNorm = 0.0006\n",
      "Iter  2600 | Loss = 0.470255 | GradNorm = 0.0005\n",
      "Iter  2700 | Loss = 0.470241 | GradNorm = 0.0005\n",
      "Iter  2800 | Loss = 0.470228 | GradNorm = 0.0005\n",
      "Iter  2900 | Loss = 0.470216 | GradNorm = 0.0005\n",
      "Iter  3000 | Loss = 0.470205 | GradNorm = 0.0005\n",
      "Iter  3100 | Loss = 0.470194 | GradNorm = 0.0005\n",
      "Iter  3200 | Loss = 0.470184 | GradNorm = 0.0004\n",
      "Iter  3300 | Loss = 0.470175 | GradNorm = 0.0004\n",
      "Iter  3400 | Loss = 0.470166 | GradNorm = 0.0004\n",
      "Iter  3500 | Loss = 0.470158 | GradNorm = 0.0004\n",
      "Iter  3600 | Loss = 0.470150 | GradNorm = 0.0004\n",
      "Iter  3700 | Loss = 0.470143 | GradNorm = 0.0004\n",
      "Iter  3800 | Loss = 0.470136 | GradNorm = 0.0004\n",
      "Iter  3900 | Loss = 0.470129 | GradNorm = 0.0004\n",
      "Iter  4000 | Loss = 0.470122 | GradNorm = 0.0004\n",
      "Iter  4100 | Loss = 0.470116 | GradNorm = 0.0003\n",
      "Iter  4200 | Loss = 0.470111 | GradNorm = 0.0003\n",
      "Iter  4300 | Loss = 0.470105 | GradNorm = 0.0003\n",
      "Iter  4400 | Loss = 0.470100 | GradNorm = 0.0003\n",
      "Iter  4500 | Loss = 0.470095 | GradNorm = 0.0003\n",
      "Iter  4600 | Loss = 0.470090 | GradNorm = 0.0003\n",
      "Iter  4700 | Loss = 0.470085 | GradNorm = 0.0003\n",
      "Iter  4800 | Loss = 0.470081 | GradNorm = 0.0003\n",
      "Iter  4900 | Loss = 0.470076 | GradNorm = 0.0003\n",
      "Iter  5000 | Loss = 0.470072 | GradNorm = 0.0003\n",
      "Iter  5100 | Loss = 0.470068 | GradNorm = 0.0003\n",
      "Iter  5200 | Loss = 0.470064 | GradNorm = 0.0003\n",
      "Iter  5300 | Loss = 0.470061 | GradNorm = 0.0003\n",
      "Iter  5400 | Loss = 0.470057 | GradNorm = 0.0003\n",
      "Iter  5500 | Loss = 0.470054 | GradNorm = 0.0003\n",
      "Iter  5600 | Loss = 0.470050 | GradNorm = 0.0003\n",
      "Iter  5700 | Loss = 0.470047 | GradNorm = 0.0003\n",
      "Iter  5800 | Loss = 0.470044 | GradNorm = 0.0003\n",
      "Iter  5900 | Loss = 0.470040 | GradNorm = 0.0002\n",
      "Iter  6000 | Loss = 0.470037 | GradNorm = 0.0002\n",
      "Iter  6100 | Loss = 0.470034 | GradNorm = 0.0002\n",
      "Iter  6200 | Loss = 0.470032 | GradNorm = 0.0002\n",
      "Iter  6300 | Loss = 0.470029 | GradNorm = 0.0002\n",
      "Iter  6400 | Loss = 0.470026 | GradNorm = 0.0002\n",
      "Iter  6500 | Loss = 0.470023 | GradNorm = 0.0002\n",
      "Iter  6600 | Loss = 0.470021 | GradNorm = 0.0002\n",
      "Iter  6700 | Loss = 0.470018 | GradNorm = 0.0002\n",
      "Iter  6800 | Loss = 0.470016 | GradNorm = 0.0002\n",
      "Iter  6900 | Loss = 0.470013 | GradNorm = 0.0002\n",
      "Iter  7000 | Loss = 0.470011 | GradNorm = 0.0002\n",
      "Iter  7100 | Loss = 0.470009 | GradNorm = 0.0002\n",
      "Iter  7200 | Loss = 0.470006 | GradNorm = 0.0002\n",
      "Iter  7300 | Loss = 0.470004 | GradNorm = 0.0002\n",
      "Iter  7400 | Loss = 0.470002 | GradNorm = 0.0002\n",
      "Iter  7500 | Loss = 0.470000 | GradNorm = 0.0002\n",
      "Iter  7600 | Loss = 0.469998 | GradNorm = 0.0002\n",
      "Iter  7700 | Loss = 0.469995 | GradNorm = 0.0002\n",
      "Iter  7800 | Loss = 0.469993 | GradNorm = 0.0002\n",
      "Iter  7900 | Loss = 0.469991 | GradNorm = 0.0002\n",
      "Iter  8000 | Loss = 0.469989 | GradNorm = 0.0002\n",
      "Iter  8100 | Loss = 0.469988 | GradNorm = 0.0002\n",
      "Iter  8200 | Loss = 0.469986 | GradNorm = 0.0002\n",
      "Iter  8300 | Loss = 0.469984 | GradNorm = 0.0002\n",
      "Iter  8400 | Loss = 0.469982 | GradNorm = 0.0002\n",
      "Iter  8500 | Loss = 0.469980 | GradNorm = 0.0002\n",
      "Iter  8600 | Loss = 0.469978 | GradNorm = 0.0002\n",
      "Iter  8700 | Loss = 0.469977 | GradNorm = 0.0002\n",
      "Iter  8800 | Loss = 0.469975 | GradNorm = 0.0002\n",
      "Iter  8900 | Loss = 0.469973 | GradNorm = 0.0002\n",
      "Iter  9000 | Loss = 0.469972 | GradNorm = 0.0002\n",
      "Iter  9100 | Loss = 0.469970 | GradNorm = 0.0002\n",
      "Iter  9200 | Loss = 0.469968 | GradNorm = 0.0002\n",
      "Iter  9300 | Loss = 0.469967 | GradNorm = 0.0002\n",
      "Iter  9400 | Loss = 0.469965 | GradNorm = 0.0002\n",
      "Iter  9500 | Loss = 0.469964 | GradNorm = 0.0002\n",
      "Iter  9600 | Loss = 0.469962 | GradNorm = 0.0002\n",
      "Iter  9700 | Loss = 0.469961 | GradNorm = 0.0002\n",
      "Iter  9800 | Loss = 0.469959 | GradNorm = 0.0002\n",
      "Iter  9900 | Loss = 0.469958 | GradNorm = 0.0002\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation data\n",
    "x_tr_final = np.vstack((X_tr_encoded, X_va_encoded))\n",
    "y_tr_final = np.hstack((y_tr, y_va))\n",
    "loss, w = logistic_regression_weighted_gd(\n",
    "    y_tr_final, x_tr_final, lambda_=1e-8, gamma=0.5, pos_weight=8.0, neg_weight=1.0,\n",
    "    max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2748c60a-2a1f-4511-9a9c-df5ca0096663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     0 | Loss = 0.693147 | GradNorm = 0.5279\n",
      "Iter   100 | Loss = 0.481214 | GradNorm = 0.0117\n",
      "Iter   200 | Loss = 0.477209 | GradNorm = 0.0068\n",
      "Iter   300 | Loss = 0.475595 | GradNorm = 0.0047\n",
      "Iter   400 | Loss = 0.474771 | GradNorm = 0.0035\n",
      "Iter   500 | Loss = 0.474296 | GradNorm = 0.0027\n",
      "Iter   600 | Loss = 0.473997 | GradNorm = 0.0022\n",
      "Iter   700 | Loss = 0.473795 | GradNorm = 0.0018\n",
      "Iter   800 | Loss = 0.473648 | GradNorm = 0.0016\n",
      "Iter   900 | Loss = 0.473536 | GradNorm = 0.0014\n",
      "Iter  1000 | Loss = 0.473446 | GradNorm = 0.0013\n",
      "Iter  1100 | Loss = 0.473372 | GradNorm = 0.0012\n",
      "Iter  1200 | Loss = 0.473310 | GradNorm = 0.0011\n",
      "Iter  1300 | Loss = 0.473255 | GradNorm = 0.0010\n",
      "Iter  1400 | Loss = 0.473208 | GradNorm = 0.0009\n",
      "Iter  1500 | Loss = 0.473166 | GradNorm = 0.0009\n",
      "Iter  1600 | Loss = 0.473128 | GradNorm = 0.0008\n",
      "Iter  1700 | Loss = 0.473094 | GradNorm = 0.0008\n",
      "Iter  1800 | Loss = 0.473064 | GradNorm = 0.0008\n",
      "Iter  1900 | Loss = 0.473036 | GradNorm = 0.0007\n",
      "Iter  2000 | Loss = 0.473010 | GradNorm = 0.0007\n",
      "Iter  2100 | Loss = 0.472987 | GradNorm = 0.0007\n",
      "Iter  2200 | Loss = 0.472966 | GradNorm = 0.0006\n",
      "Iter  2300 | Loss = 0.472946 | GradNorm = 0.0006\n",
      "Iter  2400 | Loss = 0.472928 | GradNorm = 0.0006\n",
      "Iter  2500 | Loss = 0.472911 | GradNorm = 0.0006\n",
      "Iter  2600 | Loss = 0.472896 | GradNorm = 0.0005\n",
      "Iter  2700 | Loss = 0.472881 | GradNorm = 0.0005\n",
      "Iter  2800 | Loss = 0.472868 | GradNorm = 0.0005\n",
      "Iter  2900 | Loss = 0.472855 | GradNorm = 0.0005\n",
      "Iter  3000 | Loss = 0.472843 | GradNorm = 0.0005\n",
      "Iter  3100 | Loss = 0.472832 | GradNorm = 0.0005\n",
      "Iter  3200 | Loss = 0.472822 | GradNorm = 0.0004\n",
      "Iter  3300 | Loss = 0.472812 | GradNorm = 0.0004\n",
      "Iter  3400 | Loss = 0.472803 | GradNorm = 0.0004\n",
      "Iter  3500 | Loss = 0.472795 | GradNorm = 0.0004\n",
      "Iter  3600 | Loss = 0.472786 | GradNorm = 0.0004\n",
      "Iter  3700 | Loss = 0.472779 | GradNorm = 0.0004\n",
      "Iter  3800 | Loss = 0.472771 | GradNorm = 0.0004\n",
      "Iter  3900 | Loss = 0.472764 | GradNorm = 0.0004\n",
      "Iter  4000 | Loss = 0.472758 | GradNorm = 0.0004\n",
      "Iter  4100 | Loss = 0.472751 | GradNorm = 0.0004\n",
      "Iter  4200 | Loss = 0.472745 | GradNorm = 0.0003\n",
      "Iter  4300 | Loss = 0.472740 | GradNorm = 0.0003\n",
      "Iter  4400 | Loss = 0.472734 | GradNorm = 0.0003\n",
      "Iter  4500 | Loss = 0.472729 | GradNorm = 0.0003\n",
      "Iter  4600 | Loss = 0.472724 | GradNorm = 0.0003\n",
      "Iter  4700 | Loss = 0.472719 | GradNorm = 0.0003\n",
      "Iter  4800 | Loss = 0.472714 | GradNorm = 0.0003\n",
      "Iter  4900 | Loss = 0.472709 | GradNorm = 0.0003\n",
      "Iter  5000 | Loss = 0.472705 | GradNorm = 0.0003\n",
      "Iter  5100 | Loss = 0.472701 | GradNorm = 0.0003\n",
      "Iter  5200 | Loss = 0.472697 | GradNorm = 0.0003\n",
      "Iter  5300 | Loss = 0.472693 | GradNorm = 0.0003\n",
      "Iter  5400 | Loss = 0.472689 | GradNorm = 0.0003\n",
      "Iter  5500 | Loss = 0.472685 | GradNorm = 0.0003\n",
      "Iter  5600 | Loss = 0.472682 | GradNorm = 0.0003\n",
      "Iter  5700 | Loss = 0.472678 | GradNorm = 0.0003\n",
      "Iter  5800 | Loss = 0.472675 | GradNorm = 0.0003\n",
      "Iter  5900 | Loss = 0.472671 | GradNorm = 0.0003\n",
      "Iter  6000 | Loss = 0.472668 | GradNorm = 0.0003\n",
      "Iter  6100 | Loss = 0.472665 | GradNorm = 0.0002\n",
      "Iter  6200 | Loss = 0.472662 | GradNorm = 0.0002\n",
      "Iter  6300 | Loss = 0.472659 | GradNorm = 0.0002\n",
      "Iter  6400 | Loss = 0.472656 | GradNorm = 0.0002\n",
      "Iter  6500 | Loss = 0.472653 | GradNorm = 0.0002\n",
      "Iter  6600 | Loss = 0.472651 | GradNorm = 0.0002\n",
      "Iter  6700 | Loss = 0.472648 | GradNorm = 0.0002\n",
      "Iter  6800 | Loss = 0.472645 | GradNorm = 0.0002\n",
      "Iter  6900 | Loss = 0.472643 | GradNorm = 0.0002\n",
      "Iter  7000 | Loss = 0.472640 | GradNorm = 0.0002\n",
      "Iter  7100 | Loss = 0.472638 | GradNorm = 0.0002\n",
      "Iter  7200 | Loss = 0.472635 | GradNorm = 0.0002\n",
      "Iter  7300 | Loss = 0.472633 | GradNorm = 0.0002\n",
      "Iter  7400 | Loss = 0.472630 | GradNorm = 0.0002\n",
      "Iter  7500 | Loss = 0.472628 | GradNorm = 0.0002\n",
      "Iter  7600 | Loss = 0.472626 | GradNorm = 0.0002\n",
      "Iter  7700 | Loss = 0.472624 | GradNorm = 0.0002\n",
      "Iter  7800 | Loss = 0.472622 | GradNorm = 0.0002\n",
      "Iter  7900 | Loss = 0.472619 | GradNorm = 0.0002\n",
      "Iter  8000 | Loss = 0.472617 | GradNorm = 0.0002\n",
      "Iter  8100 | Loss = 0.472615 | GradNorm = 0.0002\n",
      "Iter  8200 | Loss = 0.472613 | GradNorm = 0.0002\n",
      "Iter  8300 | Loss = 0.472611 | GradNorm = 0.0002\n",
      "Iter  8400 | Loss = 0.472609 | GradNorm = 0.0002\n",
      "Iter  8500 | Loss = 0.472608 | GradNorm = 0.0002\n",
      "Iter  8600 | Loss = 0.472606 | GradNorm = 0.0002\n",
      "Iter  8700 | Loss = 0.472604 | GradNorm = 0.0002\n",
      "Iter  8800 | Loss = 0.472602 | GradNorm = 0.0002\n",
      "Iter  8900 | Loss = 0.472600 | GradNorm = 0.0002\n",
      "Iter  9000 | Loss = 0.472598 | GradNorm = 0.0002\n",
      "Iter  9100 | Loss = 0.472597 | GradNorm = 0.0002\n",
      "Iter  9200 | Loss = 0.472595 | GradNorm = 0.0002\n",
      "Iter  9300 | Loss = 0.472593 | GradNorm = 0.0002\n",
      "Iter  9400 | Loss = 0.472592 | GradNorm = 0.0002\n",
      "Iter  9500 | Loss = 0.472590 | GradNorm = 0.0002\n",
      "Iter  9600 | Loss = 0.472589 | GradNorm = 0.0002\n",
      "Iter  9700 | Loss = 0.472587 | GradNorm = 0.0002\n",
      "Iter  9800 | Loss = 0.472585 | GradNorm = 0.0002\n",
      "Iter  9900 | Loss = 0.472584 | GradNorm = 0.0002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss2, w2 = logistic_regression_weighted_gd(\n",
    "    y_tr_final, x_tr_final, lambda_=0, gamma=0.5, pos_weight=9.0, neg_weight=1.0,\n",
    "    max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50c23189-4cdc-45d1-b045-5aacf5d06025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     0 | Loss = 0.693147 | GradNorm = 0.5279\n",
      "Iter   100 | Loss = 0.481214 | GradNorm = 0.0117\n",
      "Iter   200 | Loss = 0.477209 | GradNorm = 0.0068\n",
      "Iter   300 | Loss = 0.475595 | GradNorm = 0.0047\n",
      "Iter   400 | Loss = 0.474771 | GradNorm = 0.0035\n",
      "Iter   500 | Loss = 0.474296 | GradNorm = 0.0027\n",
      "Iter   600 | Loss = 0.473997 | GradNorm = 0.0022\n",
      "Iter   700 | Loss = 0.473795 | GradNorm = 0.0018\n",
      "Iter   800 | Loss = 0.473648 | GradNorm = 0.0016\n",
      "Iter   900 | Loss = 0.473536 | GradNorm = 0.0014\n",
      "Iter  1000 | Loss = 0.473446 | GradNorm = 0.0013\n",
      "Iter  1100 | Loss = 0.473372 | GradNorm = 0.0012\n",
      "Iter  1200 | Loss = 0.473310 | GradNorm = 0.0011\n",
      "Iter  1300 | Loss = 0.473255 | GradNorm = 0.0010\n",
      "Iter  1400 | Loss = 0.473208 | GradNorm = 0.0009\n",
      "Iter  1500 | Loss = 0.473166 | GradNorm = 0.0009\n",
      "Iter  1600 | Loss = 0.473128 | GradNorm = 0.0008\n",
      "Iter  1700 | Loss = 0.473094 | GradNorm = 0.0008\n",
      "Iter  1800 | Loss = 0.473064 | GradNorm = 0.0008\n",
      "Iter  1900 | Loss = 0.473036 | GradNorm = 0.0007\n",
      "Iter  2000 | Loss = 0.473010 | GradNorm = 0.0007\n",
      "Iter  2100 | Loss = 0.472987 | GradNorm = 0.0007\n",
      "Iter  2200 | Loss = 0.472966 | GradNorm = 0.0006\n",
      "Iter  2300 | Loss = 0.472946 | GradNorm = 0.0006\n",
      "Iter  2400 | Loss = 0.472928 | GradNorm = 0.0006\n",
      "Iter  2500 | Loss = 0.472911 | GradNorm = 0.0006\n",
      "Iter  2600 | Loss = 0.472896 | GradNorm = 0.0005\n",
      "Iter  2700 | Loss = 0.472881 | GradNorm = 0.0005\n",
      "Iter  2800 | Loss = 0.472868 | GradNorm = 0.0005\n",
      "Iter  2900 | Loss = 0.472855 | GradNorm = 0.0005\n",
      "Iter  3000 | Loss = 0.472843 | GradNorm = 0.0005\n",
      "Iter  3100 | Loss = 0.472832 | GradNorm = 0.0005\n",
      "Iter  3200 | Loss = 0.472822 | GradNorm = 0.0004\n",
      "Iter  3300 | Loss = 0.472812 | GradNorm = 0.0004\n",
      "Iter  3400 | Loss = 0.472803 | GradNorm = 0.0004\n",
      "Iter  3500 | Loss = 0.472795 | GradNorm = 0.0004\n",
      "Iter  3600 | Loss = 0.472786 | GradNorm = 0.0004\n",
      "Iter  3700 | Loss = 0.472779 | GradNorm = 0.0004\n",
      "Iter  3800 | Loss = 0.472771 | GradNorm = 0.0004\n",
      "Iter  3900 | Loss = 0.472764 | GradNorm = 0.0004\n",
      "Iter  4000 | Loss = 0.472758 | GradNorm = 0.0004\n",
      "Iter  4100 | Loss = 0.472751 | GradNorm = 0.0004\n",
      "Iter  4200 | Loss = 0.472745 | GradNorm = 0.0003\n",
      "Iter  4300 | Loss = 0.472740 | GradNorm = 0.0003\n",
      "Iter  4400 | Loss = 0.472734 | GradNorm = 0.0003\n",
      "Iter  4500 | Loss = 0.472729 | GradNorm = 0.0003\n",
      "Iter  4600 | Loss = 0.472724 | GradNorm = 0.0003\n",
      "Iter  4700 | Loss = 0.472719 | GradNorm = 0.0003\n",
      "Iter  4800 | Loss = 0.472714 | GradNorm = 0.0003\n",
      "Iter  4900 | Loss = 0.472709 | GradNorm = 0.0003\n",
      "Iter  5000 | Loss = 0.472705 | GradNorm = 0.0003\n",
      "Iter  5100 | Loss = 0.472701 | GradNorm = 0.0003\n",
      "Iter  5200 | Loss = 0.472697 | GradNorm = 0.0003\n",
      "Iter  5300 | Loss = 0.472693 | GradNorm = 0.0003\n",
      "Iter  5400 | Loss = 0.472689 | GradNorm = 0.0003\n",
      "Iter  5500 | Loss = 0.472685 | GradNorm = 0.0003\n",
      "Iter  5600 | Loss = 0.472682 | GradNorm = 0.0003\n",
      "Iter  5700 | Loss = 0.472678 | GradNorm = 0.0003\n",
      "Iter  5800 | Loss = 0.472675 | GradNorm = 0.0003\n",
      "Iter  5900 | Loss = 0.472671 | GradNorm = 0.0003\n",
      "Iter  6000 | Loss = 0.472668 | GradNorm = 0.0003\n",
      "Iter  6100 | Loss = 0.472665 | GradNorm = 0.0002\n",
      "Iter  6200 | Loss = 0.472662 | GradNorm = 0.0002\n",
      "Iter  6300 | Loss = 0.472659 | GradNorm = 0.0002\n",
      "Iter  6400 | Loss = 0.472656 | GradNorm = 0.0002\n",
      "Iter  6500 | Loss = 0.472653 | GradNorm = 0.0002\n",
      "Iter  6600 | Loss = 0.472651 | GradNorm = 0.0002\n",
      "Iter  6700 | Loss = 0.472648 | GradNorm = 0.0002\n",
      "Iter  6800 | Loss = 0.472645 | GradNorm = 0.0002\n",
      "Iter  6900 | Loss = 0.472643 | GradNorm = 0.0002\n",
      "Iter  7000 | Loss = 0.472640 | GradNorm = 0.0002\n",
      "Iter  7100 | Loss = 0.472638 | GradNorm = 0.0002\n",
      "Iter  7200 | Loss = 0.472635 | GradNorm = 0.0002\n",
      "Iter  7300 | Loss = 0.472633 | GradNorm = 0.0002\n",
      "Iter  7400 | Loss = 0.472630 | GradNorm = 0.0002\n",
      "Iter  7500 | Loss = 0.472628 | GradNorm = 0.0002\n",
      "Iter  7600 | Loss = 0.472626 | GradNorm = 0.0002\n",
      "Iter  7700 | Loss = 0.472624 | GradNorm = 0.0002\n",
      "Iter  7800 | Loss = 0.472622 | GradNorm = 0.0002\n",
      "Iter  7900 | Loss = 0.472619 | GradNorm = 0.0002\n",
      "Iter  8000 | Loss = 0.472617 | GradNorm = 0.0002\n",
      "Iter  8100 | Loss = 0.472615 | GradNorm = 0.0002\n",
      "Iter  8200 | Loss = 0.472613 | GradNorm = 0.0002\n",
      "Iter  8300 | Loss = 0.472611 | GradNorm = 0.0002\n",
      "Iter  8400 | Loss = 0.472609 | GradNorm = 0.0002\n",
      "Iter  8500 | Loss = 0.472608 | GradNorm = 0.0002\n",
      "Iter  8600 | Loss = 0.472606 | GradNorm = 0.0002\n",
      "Iter  8700 | Loss = 0.472604 | GradNorm = 0.0002\n",
      "Iter  8800 | Loss = 0.472602 | GradNorm = 0.0002\n",
      "Iter  8900 | Loss = 0.472600 | GradNorm = 0.0002\n",
      "Iter  9000 | Loss = 0.472598 | GradNorm = 0.0002\n",
      "Iter  9100 | Loss = 0.472597 | GradNorm = 0.0002\n",
      "Iter  9200 | Loss = 0.472595 | GradNorm = 0.0002\n",
      "Iter  9300 | Loss = 0.472593 | GradNorm = 0.0002\n",
      "Iter  9400 | Loss = 0.472592 | GradNorm = 0.0002\n",
      "Iter  9500 | Loss = 0.472590 | GradNorm = 0.0002\n",
      "Iter  9600 | Loss = 0.472589 | GradNorm = 0.0002\n",
      "Iter  9700 | Loss = 0.472587 | GradNorm = 0.0002\n",
      "Iter  9800 | Loss = 0.472585 | GradNorm = 0.0002\n",
      "Iter  9900 | Loss = 0.472584 | GradNorm = 0.0002\n"
     ]
    }
   ],
   "source": [
    "loss3, w3 = logistic_regression_weighted_gd(\n",
    "    y_tr_final, x_tr_final, lambda_=1e-12, gamma=0.5, pos_weight=9.0, neg_weight=1.0,\n",
    "    max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d7cc7dc-f351-4fac-b015-c8d838e20736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 86.54%\n",
      "✅ F1 Score: 0.4153\n"
     ]
    }
   ],
   "source": [
    "# Suppose you've already trained your model\n",
    "# loss, w = logistic_regression_weighted_gd(...)\n",
    "\n",
    "# Evaluate on test or validation data\n",
    "acc, f1 = evaluate_model(y_te, X_te_encoded, w2, 0.69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc321a29-51bb-488d-9366-45dc54719fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
