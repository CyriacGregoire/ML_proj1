{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2167e8c7-57f2-477d-9e86-f5f9a707e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from Data_cleaning import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e717cb-604e-4af1-a14f-1a465fdaa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data\\dataset\\dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78337828-02dd-4dc7-ab8d-8b1c97c4fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mod = x_train[:1000]\n",
    "y_train_mod = y_train[:1000]\n",
    "\n",
    "x_train_mod = remove_nan_features(x_train_mod)\n",
    "x_train_mod = impute_missing_values(x_train_mod)\n",
    "x_train_mod = select_random_features(x_train_mod, 15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a00174-e764-4314-9a4d-9fabd055cc61",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "logistic_loss() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss, w \u001b[38;5;241m=\u001b[39m logistic_regression_penalized_gradient_descent_demo(y_train_mod, x_train_mod)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss, w)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML_proj1\\project1\\implementations.py:132\u001b[0m, in \u001b[0;36mlogistic_regression_penalized_gradient_descent_demo\u001b[1;34m(y, x, max_iter, gamma, lambda_, threshold)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# start the logistic regression\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# get loss and update w.\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m     loss, w \u001b[38;5;241m=\u001b[39m learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# log info\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML_proj1\\project1\\implementations.py:116\u001b[0m, in \u001b[0;36mlearning_by_penalized_gradient\u001b[1;34m(y, tx, w, gamma, lambda_)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearning_by_penalized_gradient\u001b[39m(y, tx, w, gamma, lambda_):\n\u001b[1;32m--> 116\u001b[0m     loss, grad \u001b[38;5;241m=\u001b[39m penalized_logistic_regression(y, tx, w, lambda_)\n\u001b[0;32m    117\u001b[0m     w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m-\u001b[39m gamma \u001b[38;5;241m*\u001b[39m grad\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, w\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML_proj1\\project1\\implementations.py:109\u001b[0m, in \u001b[0;36mpenalized_logistic_regression\u001b[1;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpenalized_logistic_regression\u001b[39m(y, tx, w, lambda_):\n\u001b[1;32m--> 109\u001b[0m     loss \u001b[38;5;241m=\u001b[39m logistic_loss(y, tx, w)\n\u001b[0;32m    110\u001b[0m     grad \u001b[38;5;241m=\u001b[39m compute_gradient_logistic(y, tx, w)\n\u001b[0;32m    111\u001b[0m     grad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m lambda_ \u001b[38;5;241m*\u001b[39m w  \n",
      "\u001b[1;31mTypeError\u001b[0m: logistic_loss() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "loss, w = logistic_regression_penalized_gradient_descent_demo(y_train_mod, x_train_mod)\n",
    "print(loss, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e7956-6b9b-4b28-b344-2a861b658085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f (x, w, limit = 0.5):\n",
    "    prob = sigmoid(x.T@w)\n",
    "    if prob > limit: return 1\n",
    "    else return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a571bd-5d65-46a9-b5c6-fdd98601ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_mod = x_train[2000:3000]\n",
    "y_test_mod = y_train[2000:3000]\n",
    "\n",
    "y_pred = f(x_test_mod, w)\n",
    "\n",
    "print(np.sum(abs(y_test_mod - y_pred)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
