{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2167e8c7-57f2-477d-9e86-f5f9a707e962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from Data_cleaning import *\n",
    "from helpers import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e717cb-604e-4af1-a14f-1a465fdaa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data\\dataset\\dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78337828-02dd-4dc7-ab8d-8b1c97c4fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Split first\n",
    "x_tr = x_train[:10000, :15]\n",
    "y_tr = y_train[:10000]\n",
    "x_te = x_train[10000:20000, :15]\n",
    "y_te = y_train[10000:20000]\n",
    "\n",
    "# 2️⃣ Remove NaN-heavy features on training data only\n",
    "x_tr, keep_mask = remove_nan_features(x_tr)\n",
    "x_te = x_te[:, keep_mask]  # apply same mask to test\n",
    "\n",
    "# 3️⃣ Impute missing values (same logic on both)\n",
    "x_tr = impute_missing_values(x_tr)\n",
    "x_te = impute_missing_values(x_te)\n",
    "\n",
    "# 4️⃣ Standardize based on training data\n",
    "x_tr, means, std = standardize_features(x_tr)\n",
    "x_te = (x_te - means) / std\n",
    "\n",
    "y_tr = (y_tr + 1) / 2\n",
    "y_te = (y_te + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d95ea905-d1bc-4b45-8e5c-9f1405b14e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (10000, 9)\n"
     ]
    }
   ],
   "source": [
    "print(y_tr.shape, x_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7a00174-e764-4314-9a4d-9fabd055cc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.313875\n",
      "Iteration   200, loss = 0.296801\n",
      "Iteration   300, loss = 0.294420\n",
      "Iteration   400, loss = 0.293941\n",
      "Iteration   500, loss = 0.293808\n",
      "Iteration   600, loss = 0.293749\n",
      "Iteration   700, loss = 0.293708\n",
      "Iteration   800, loss = 0.293672\n",
      "Iteration   900, loss = 0.293638\n",
      "Iteration  1000, loss = 0.293606\n",
      "Iteration  1100, loss = 0.293575\n",
      "Iteration  1200, loss = 0.293545\n",
      "Iteration  1300, loss = 0.293516\n",
      "Iteration  1400, loss = 0.293488\n",
      "Iteration  1500, loss = 0.293461\n",
      "Iteration  1600, loss = 0.293435\n",
      "Iteration  1700, loss = 0.293410\n",
      "Iteration  1800, loss = 0.293385\n",
      "Iteration  1900, loss = 0.293362\n",
      "Iteration  2000, loss = 0.293339\n",
      "Iteration  2100, loss = 0.293318\n",
      "Iteration  2200, loss = 0.293296\n",
      "Iteration  2300, loss = 0.293276\n",
      "Iteration  2400, loss = 0.293256\n",
      "Iteration  2500, loss = 0.293237\n",
      "Iteration  2600, loss = 0.293219\n",
      "Iteration  2700, loss = 0.293201\n",
      "Iteration  2800, loss = 0.293184\n",
      "Iteration  2900, loss = 0.293167\n",
      "Iteration  3000, loss = 0.293151\n",
      "Iteration  3100, loss = 0.293135\n",
      "Iteration  3200, loss = 0.293120\n",
      "Iteration  3300, loss = 0.293105\n",
      "Iteration  3400, loss = 0.293091\n",
      "Iteration  3500, loss = 0.293077\n",
      "Iteration  3600, loss = 0.293064\n",
      "Iteration  3700, loss = 0.293051\n",
      "Iteration  3800, loss = 0.293038\n",
      "Iteration  3900, loss = 0.293026\n",
      "Iteration  4000, loss = 0.293014\n",
      "Iteration  4100, loss = 0.293003\n",
      "Iteration  4200, loss = 0.292992\n",
      "Iteration  4300, loss = 0.292981\n",
      "Iteration  4400, loss = 0.292971\n",
      "Iteration  4500, loss = 0.292961\n",
      "Iteration  4600, loss = 0.292951\n",
      "Iteration  4700, loss = 0.292941\n",
      "Iteration  4800, loss = 0.292932\n",
      "Iteration  4900, loss = 0.292923\n",
      "Iteration  5000, loss = 0.292914\n",
      "Iteration  5100, loss = 0.292906\n",
      "Iteration  5200, loss = 0.292898\n",
      "Iteration  5300, loss = 0.292890\n",
      "Iteration  5400, loss = 0.292882\n",
      "Iteration  5500, loss = 0.292874\n",
      "Iteration  5600, loss = 0.292867\n",
      "Iteration  5700, loss = 0.292860\n",
      "Iteration  5800, loss = 0.292853\n",
      "Iteration  5900, loss = 0.292847\n",
      "Iteration  6000, loss = 0.292840\n",
      "Iteration  6100, loss = 0.292834\n",
      "Iteration  6200, loss = 0.292828\n",
      "Iteration  6300, loss = 0.292822\n",
      "Iteration  6400, loss = 0.292816\n",
      "Iteration  6500, loss = 0.292810\n",
      "Iteration  6600, loss = 0.292805\n",
      "Iteration  6700, loss = 0.292799\n",
      "Iteration  6800, loss = 0.292794\n",
      "Iteration  6900, loss = 0.292789\n",
      "Iteration  7000, loss = 0.292784\n",
      "Iteration  7100, loss = 0.292780\n",
      "Iteration  7200, loss = 0.292775\n",
      "Iteration  7300, loss = 0.292770\n",
      "Iteration  7400, loss = 0.292766\n",
      "Iteration  7500, loss = 0.292762\n",
      "Iteration  7600, loss = 0.292758\n",
      "Iteration  7700, loss = 0.292754\n",
      "Iteration  7800, loss = 0.292750\n",
      "Iteration  7900, loss = 0.292746\n",
      "Iteration  8000, loss = 0.292742\n",
      "Iteration  8100, loss = 0.292739\n",
      "Iteration  8200, loss = 0.292735\n",
      "Iteration  8300, loss = 0.292732\n",
      "Iteration  8400, loss = 0.292728\n",
      "Iteration  8500, loss = 0.292725\n",
      "Iteration  8600, loss = 0.292722\n",
      "Iteration  8700, loss = 0.292719\n",
      "Iteration  8800, loss = 0.292716\n",
      "Iteration  8900, loss = 0.292713\n",
      "Iteration  9000, loss = 0.292710\n",
      "Iteration  9100, loss = 0.292707\n",
      "Iteration  9200, loss = 0.292705\n",
      "Iteration  9300, loss = 0.292702\n",
      "Iteration  9400, loss = 0.292699\n",
      "Iteration  9500, loss = 0.292697\n",
      "Iteration  9600, loss = 0.292694\n",
      "Iteration  9700, loss = 0.292692\n",
      "Iteration  9800, loss = 0.292690\n",
      "Iteration  9900, loss = 0.292688\n",
      "0.29268536393128547 [-2.38068872  0.0057415   0.79723442 -0.39305138 -0.39002077 -0.10507682\n",
      " -0.37821735 -0.04087391 -0.09452648 -0.09452648]\n"
     ]
    }
   ],
   "source": [
    "loss, w = logistic_regression_penalized_gradient_descent_demo(y_tr, x_tr, 10000, 0.1, 0.000005)\n",
    "print(loss, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "df3bf6d7-13a2-47e8-b211-eb65b0252894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872.0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "184e7956-6b9b-4b28-b344-2a861b658085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels_logistic(x, w, threshold=0.4, return_original_labels=True):\n",
    "\n",
    "    # add bias term\n",
    "    tx = np.c_[np.ones(x.shape[0]), x]\n",
    "    \n",
    "    # compute probabilities\n",
    "    probs = sigmoid(tx @ w)\n",
    "    \n",
    "    # threshold\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    \n",
    "    # convert to {-1, 1} if desired\n",
    "    if return_original_labels:\n",
    "        preds = 2 * preds - 1\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c2a571bd-5d65-46a9-b5c6-fdd98601ac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 849.0\n",
      "Accuracy: 0.915\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = predict_labels_logistic(x_te, w, return_original_labels=False)\n",
    "n_errors = np.sum(np.abs(y_te - y_pred))\n",
    "accuracy = np.mean(y_te == y_pred)\n",
    "\n",
    "print(\"Misclassified samples:\", n_errors)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f36db5d4-c089-4962-bced-52e96460bb93",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472530a4-41d3-4616-b3a0-d0f7e481dfd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
