{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77861085-5158-4892-95ad-beb6ede3fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from Data_cleaning import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8f15e3-bbcf-4df3-a670-e1630669be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data\\dataset\\dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232104b1-5722-4495-a544-00e093fc4f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000,) (60000,) [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Turn y into 0s and 1s\n",
    "y_tr_ = (y_train + 1) / 2\n",
    "y_tr = y_tr_[:180000]\n",
    "y_va = y_tr_[180000:240000]\n",
    "y_te = y_tr_[240000:300000]\n",
    "print(y_tr.shape, y_te.shape, y_tr, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73bd42f1-675a-4d14-a1ac-720f16cd1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_categorical_features(X, threshold=10):\n",
    "    \"\"\"\n",
    "    Removes categorical (low-cardinality) features from a numeric dataset.\n",
    "    A feature is dropped if it has fewer than `threshold` unique values.\n",
    "    \"\"\"\n",
    "    X_np = np.asarray(X)\n",
    "    n_samples, n_features = X_np.shape\n",
    "    keep_mask = np.array([\n",
    "        np.unique(X_np[:, j]).size >= threshold for j in range(n_features)\n",
    "    ])\n",
    "    X_num = X_np[:, keep_mask]\n",
    "    return X_num, keep_mask\n",
    "\n",
    "def anova_f_test(X, y):\n",
    "    \"\"\"\n",
    "    Compute ANOVA F-statistic for each feature using NumPy only.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (n_samples, n_features)\n",
    "        Feature matrix.\n",
    "    y : np.ndarray, shape (n_samples,)\n",
    "        Class labels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    F_values : np.ndarray, shape (n_features,)\n",
    "        F-statistics for each feature.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    classes = np.unique(y)\n",
    "    k = len(classes)\n",
    "    \n",
    "    overall_means = np.nanmean(X, axis=0)  # handle NaNs safely\n",
    "    \n",
    "    # Initialize sums of squares\n",
    "    ssb = np.zeros(n_features)\n",
    "    ssw = np.zeros(n_features)\n",
    "    \n",
    "    for c in classes:\n",
    "        X_c = X[y == c]\n",
    "        n_c = X_c.shape[0]\n",
    "        mean_c = np.nanmean(X_c, axis=0)\n",
    "        \n",
    "        ssb += n_c * (mean_c - overall_means) ** 2\n",
    "        ssw += np.nansum((X_c - mean_c) ** 2, axis=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    ssw = np.where(ssw == 0, np.nan, ssw)\n",
    "    \n",
    "    F = (ssb / (k - 1)) / (ssw / (n_samples - k))\n",
    "    \n",
    "    return F\n",
    "def logistic_regression_penalized_gradient_descent_weighted(y, X, lambda_, gamma=0.5, max_iter=10000, threshold=1e-8):\n",
    "    \"\"\"\n",
    "    Logistic regression with L2 regularization and class imbalance weighting.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    tx = np.c_[np.ones((n, 1)), X]\n",
    "    w = np.zeros(d + 1)\n",
    "\n",
    "    # Compute class weights\n",
    "    n_pos = np.sum(y == 1)\n",
    "    n_neg = np.sum(y == 0)\n",
    "    w_pos = n / (2 * n_pos)\n",
    "    w_neg = n / (2 * n_neg)\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        y_pred = 1 / (1 + np.exp(-tx @ w))\n",
    "\n",
    "        # Weighted gradient\n",
    "        weights = np.where(y == 1, w_pos, w_neg)\n",
    "        error = y_pred - y\n",
    "        grad = (tx.T @ (weights * error)) / n + lambda_ * np.r_[0, w[1:]]  # no reg. on bias\n",
    "\n",
    "        # Update weights\n",
    "        w -= gamma * grad\n",
    "\n",
    "        # Convergence check\n",
    "        if np.linalg.norm(grad) < threshold:\n",
    "            break\n",
    "\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d6a7635-defe-43fb-905c-b29ce3f4303c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 109)\n",
      "(180000, 30)\n"
     ]
    }
   ],
   "source": [
    "# --- CLEANING ---\n",
    "# 0. Remove categorical (low-cardinality) features\n",
    "x_num, cat_mask = remove_categorical_features(x_train, threshold=10)\n",
    "print(x_num.shape)\n",
    "# 1. Remove features with too many NaNs\n",
    "x_clean, keep_mask = remove_nan_features(x_num)\n",
    "\n",
    "x_clean_tr = x_clean[:180000]\n",
    "x_clean_va = x_clean[180000:240000]\n",
    "x_clean_te = x_clean[240000:300000]\n",
    "\n",
    "# 2. Impute remaining missing values\n",
    "# Compute feature means from the training data (ignore NaNs)\n",
    "train_means = np.nanmean(x_clean_tr, axis=0)\n",
    "\n",
    "# Replace NaNs in training data with training means\n",
    "inds_tr = np.where(np.isnan(x_clean_tr))\n",
    "x_clean_tr[inds_tr] = np.take(train_means, inds_tr[1])\n",
    "\n",
    "# Replace NaNs in validation data with training means\n",
    "inds_va = np.where(np.isnan(x_clean_va))\n",
    "x_clean_va[inds_va] = np.take(train_means, inds_va[1])\n",
    "\n",
    "# Replace NaNs in test data with training means\n",
    "inds_te = np.where(np.isnan(x_clean_te))\n",
    "x_clean_te[inds_te] = np.take(train_means, inds_te[1])\n",
    "\n",
    "# Compute F-scores using only the training data\n",
    "F_scores = anova_f_test(x_clean_tr, y_tr)\n",
    "\n",
    "# Select top 30 features\n",
    "top_k = 30\n",
    "top_features_idx = np.argsort(F_scores)[-top_k:][::-1]\n",
    "\n",
    "# Apply the same feature selection to all sets\n",
    "x_anova_tr = x_clean_tr[:, top_features_idx]\n",
    "x_anova_va = x_clean_va[:, top_features_idx]\n",
    "x_anova_te = x_clean_te[:, top_features_idx]\n",
    "\n",
    "# --- STANDARDIZATION ---\n",
    "x_tr_st, means, stds = standardize_features(x_anova_tr)\n",
    "x_va_st = (x_anova_va - means) / stds\n",
    "x_te_st = (x_anova_te - means) / stds\n",
    "\n",
    "\n",
    "print(x_tr_st.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee303b1a-d641-41eb-889f-6250c7f848e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000, 30) (180000,)\n"
     ]
    }
   ],
   "source": [
    "#check shapes\n",
    "print(x_tr_st.shape, y_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "205a2dab-191a-456f-8c63-adafe4007a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CROSS-VALIDATION PIPELINE FOR LOGISTIC REGRESSION (RIDGE)\n",
    "# ============================================================\n",
    "# ------------------------------------------------------------\n",
    "# 1. Metric function\n",
    "# ------------------------------------------------------------\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Compute F1 score between true and predicted labels.\"\"\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    if tp + fp == 0 or tp + fn == 0:\n",
    "        return 0.0\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Stratified K-Fold Split\n",
    "# ------------------------------------------------------------\n",
    "def stratified_kfold_indices(y, k=5, seed=42):\n",
    "    \"\"\"Return list of (train_idx, val_idx) preserving class ratio.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    idx_pos = np.where(y == 1)[0]\n",
    "    idx_neg = np.where(y == 0)[0]\n",
    "    np.random.shuffle(idx_pos)\n",
    "    np.random.shuffle(idx_neg)\n",
    "    pos_folds = np.array_split(idx_pos, k)\n",
    "    neg_folds = np.array_split(idx_neg, k)\n",
    "\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        val_idx = np.concatenate([pos_folds[i], neg_folds[i]])\n",
    "        train_idx = np.setdiff1d(np.arange(len(y)), val_idx)\n",
    "        folds.append((train_idx, val_idx))\n",
    "    return folds\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Cross-validation for λ and α search\n",
    "# ------------------------------------------------------------\n",
    "def cross_validate_lambda_alpha(\n",
    "    X, y,\n",
    "    lambdas, alphas,\n",
    "    k=5,\n",
    "    gamma=0.5, max_iter=10000, threshold=1e-8,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation to find best (lambda, alpha)\n",
    "    for penalized logistic regression based on mean F1 score.\n",
    "\n",
    "    Uses your existing logistic_regression_penalized_gradient_descent_demo().\n",
    "    \"\"\"\n",
    "\n",
    "    best_lambda, best_alpha, best_mean_f1 = None, None, -1\n",
    "    folds = stratified_kfold_indices(y, k=k, seed=seed)\n",
    "\n",
    "    # Store all mean F1s (optional, for visualization)\n",
    "    f1_matrix = np.zeros((len(lambdas), len(alphas)))\n",
    "\n",
    "    for i_lam, lam in enumerate(lambdas):\n",
    "        for i_alpha, alpha in enumerate(alphas):\n",
    "            fold_f1s = []\n",
    "\n",
    "            # ----- Perform k-fold cross-validation -----\n",
    "            for i, (train_idx, val_idx) in enumerate(folds):\n",
    "                X_train, y_train = X[train_idx], y[train_idx]\n",
    "                X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "                # Train penalized logistic regression with current lambda\n",
    "                w = logistic_regression_penalized_gradient_descent_weighted(\n",
    "                y_train, X_train,\n",
    "                lambda_=lam,\n",
    "                gamma=gamma,\n",
    "                max_iter=max_iter,\n",
    "                threshold=threshold\n",
    ")\n",
    "\n",
    "\n",
    "                # Predict probabilities on validation data\n",
    "                tx_val = np.c_[np.ones((y_val.shape[0], 1)), X_val]\n",
    "                y_proba = sigmoid(tx_val @ w)\n",
    "\n",
    "                # Apply classification threshold α\n",
    "                y_pred = (y_proba >= alpha).astype(int)\n",
    "\n",
    "                # Compute F1 on this fold\n",
    "                fold_f1s.append(f1_score(y_val, y_pred))\n",
    "\n",
    "            # ----- Average F1 across folds -----\n",
    "            mean_f1 = np.mean(fold_f1s)\n",
    "            f1_matrix[i_lam, i_alpha] = mean_f1\n",
    "\n",
    "            # Track best parameters\n",
    "            if mean_f1 > best_mean_f1:\n",
    "                best_mean_f1 = mean_f1\n",
    "                best_lambda = lam\n",
    "                best_alpha = alpha\n",
    "\n",
    "    print(f\"Best λ = {best_lambda}, Best α = {best_alpha}, Mean F1 = {best_mean_f1:.4f}\")\n",
    "    return best_lambda, best_alpha, best_mean_f1, f1_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a188fd0d-fe63-4f9c-953a-3d3ebcbfc660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NaN in X? False\n",
      "Any NaN in y? False\n",
      "Unique values in y: [0. 1.]\n",
      "Shape X: (180000, 30)\n",
      "Shape y: (180000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Any NaN in X?\", np.isnan(x_tr_st).any())\n",
    "print(\"Any NaN in y?\", np.isnan(y_tr).any())\n",
    "print(\"Unique values in y:\", np.unique(y_tr))\n",
    "print(\"Shape X:\", x_tr_st.shape)\n",
    "print(\"Shape y:\", y_tr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03550206-e03c-4e3f-9189-2b5ab38aa8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244205\n",
      "Iteration   200, loss = 0.243175\n",
      "Iteration   300, loss = 0.243098\n",
      "Iteration   400, loss = 0.243077\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.243505\n",
      "Iteration   200, loss = 0.242437\n",
      "Iteration   300, loss = 0.242352\n",
      "Iteration   400, loss = 0.242327\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.243873\n",
      "Iteration   200, loss = 0.242817\n",
      "Iteration   300, loss = 0.242739\n",
      "Iteration   400, loss = 0.242719\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244591\n",
      "Iteration   200, loss = 0.243538\n",
      "Iteration   300, loss = 0.243454\n",
      "Iteration   400, loss = 0.243429\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244205\n",
      "Iteration   200, loss = 0.243175\n",
      "Iteration   300, loss = 0.243098\n",
      "Iteration   400, loss = 0.243077\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.243505\n",
      "Iteration   200, loss = 0.242437\n",
      "Iteration   300, loss = 0.242352\n",
      "Iteration   400, loss = 0.242327\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.243873\n",
      "Iteration   200, loss = 0.242817\n",
      "Iteration   300, loss = 0.242739\n",
      "Iteration   400, loss = 0.242719\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244591\n",
      "Iteration   200, loss = 0.243538\n",
      "Iteration   300, loss = 0.243454\n",
      "Iteration   400, loss = 0.243429\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244205\n",
      "Iteration   200, loss = 0.243175\n",
      "Iteration   300, loss = 0.243098\n",
      "Iteration   400, loss = 0.243077\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.243505\n",
      "Iteration   200, loss = 0.242437\n",
      "Iteration   300, loss = 0.242352\n",
      "Iteration   400, loss = 0.242327\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.243873\n",
      "Iteration   200, loss = 0.242817\n",
      "Iteration   300, loss = 0.242739\n",
      "Iteration   400, loss = 0.242719\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244591\n",
      "Iteration   200, loss = 0.243538\n",
      "Iteration   300, loss = 0.243454\n",
      "Iteration   400, loss = 0.243429\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244205\n",
      "Iteration   200, loss = 0.243175\n",
      "Iteration   300, loss = 0.243098\n",
      "Iteration   400, loss = 0.243077\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.243505\n",
      "Iteration   200, loss = 0.242437\n",
      "Iteration   300, loss = 0.242352\n",
      "Iteration   400, loss = 0.242327\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.243873\n",
      "Iteration   200, loss = 0.242817\n",
      "Iteration   300, loss = 0.242739\n",
      "Iteration   400, loss = 0.242719\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244591\n",
      "Iteration   200, loss = 0.243538\n",
      "Iteration   300, loss = 0.243454\n",
      "Iteration   400, loss = 0.243429\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244205\n",
      "Iteration   200, loss = 0.243175\n",
      "Iteration   300, loss = 0.243098\n",
      "Iteration   400, loss = 0.243077\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.243505\n",
      "Iteration   200, loss = 0.242437\n",
      "Iteration   300, loss = 0.242352\n",
      "Iteration   400, loss = 0.242327\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.243873\n",
      "Iteration   200, loss = 0.242817\n",
      "Iteration   300, loss = 0.242739\n",
      "Iteration   400, loss = 0.242719\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244591\n",
      "Iteration   200, loss = 0.243538\n",
      "Iteration   300, loss = 0.243454\n",
      "Iteration   400, loss = 0.243429\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.245067\n",
      "Iteration   200, loss = 0.243753\n",
      "Iteration   300, loss = 0.243608\n",
      "Iteration   400, loss = 0.243578\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244381\n",
      "Iteration   200, loss = 0.243027\n",
      "Iteration   300, loss = 0.242873\n",
      "Iteration   400, loss = 0.242838\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244745\n",
      "Iteration   200, loss = 0.243404\n",
      "Iteration   300, loss = 0.243256\n",
      "Iteration   400, loss = 0.243225\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.245460\n",
      "Iteration   200, loss = 0.244123\n",
      "Iteration   300, loss = 0.243971\n",
      "Iteration   400, loss = 0.243937\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.245067\n",
      "Iteration   200, loss = 0.243753\n",
      "Iteration   300, loss = 0.243608\n",
      "Iteration   400, loss = 0.243578\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244381\n",
      "Iteration   200, loss = 0.243027\n",
      "Iteration   300, loss = 0.242873\n",
      "Iteration   400, loss = 0.242838\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244745\n",
      "Iteration   200, loss = 0.243404\n",
      "Iteration   300, loss = 0.243256\n",
      "Iteration   400, loss = 0.243225\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.245460\n",
      "Iteration   200, loss = 0.244123\n",
      "Iteration   300, loss = 0.243971\n",
      "Iteration   400, loss = 0.243937\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.245067\n",
      "Iteration   200, loss = 0.243753\n",
      "Iteration   300, loss = 0.243608\n",
      "Iteration   400, loss = 0.243578\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244381\n",
      "Iteration   200, loss = 0.243027\n",
      "Iteration   300, loss = 0.242873\n",
      "Iteration   400, loss = 0.242838\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244745\n",
      "Iteration   200, loss = 0.243404\n",
      "Iteration   300, loss = 0.243256\n",
      "Iteration   400, loss = 0.243225\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.245460\n",
      "Iteration   200, loss = 0.244123\n",
      "Iteration   300, loss = 0.243971\n",
      "Iteration   400, loss = 0.243937\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.245067\n",
      "Iteration   200, loss = 0.243753\n",
      "Iteration   300, loss = 0.243608\n",
      "Iteration   400, loss = 0.243578\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244381\n",
      "Iteration   200, loss = 0.243027\n",
      "Iteration   300, loss = 0.242873\n",
      "Iteration   400, loss = 0.242838\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244745\n",
      "Iteration   200, loss = 0.243404\n",
      "Iteration   300, loss = 0.243256\n",
      "Iteration   400, loss = 0.243225\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.245460\n",
      "Iteration   200, loss = 0.244123\n",
      "Iteration   300, loss = 0.243971\n",
      "Iteration   400, loss = 0.243937\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.245067\n",
      "Iteration   200, loss = 0.243753\n",
      "Iteration   300, loss = 0.243608\n",
      "Iteration   400, loss = 0.243578\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244381\n",
      "Iteration   200, loss = 0.243027\n",
      "Iteration   300, loss = 0.242873\n",
      "Iteration   400, loss = 0.242838\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.244745\n",
      "Iteration   200, loss = 0.243404\n",
      "Iteration   300, loss = 0.243256\n",
      "Iteration   400, loss = 0.243225\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.245460\n",
      "Iteration   200, loss = 0.244123\n",
      "Iteration   300, loss = 0.243971\n",
      "Iteration   400, loss = 0.243937\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259956\n",
      "Iteration   200, loss = 0.259541\n",
      "Converged at iteration 299\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259380\n",
      "Iteration   200, loss = 0.258955\n",
      "Iteration   300, loss = 0.258949\n",
      "Converged at iteration 311\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259718\n",
      "Iteration   200, loss = 0.259297\n",
      "Converged at iteration 295\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.260384\n",
      "Iteration   200, loss = 0.259965\n",
      "Iteration   300, loss = 0.259959\n",
      "Converged at iteration 310\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259956\n",
      "Iteration   200, loss = 0.259541\n",
      "Converged at iteration 299\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259380\n",
      "Iteration   200, loss = 0.258955\n",
      "Iteration   300, loss = 0.258949\n",
      "Converged at iteration 311\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259718\n",
      "Iteration   200, loss = 0.259297\n",
      "Converged at iteration 295\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.260384\n",
      "Iteration   200, loss = 0.259965\n",
      "Iteration   300, loss = 0.259959\n",
      "Converged at iteration 310\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259956\n",
      "Iteration   200, loss = 0.259541\n",
      "Converged at iteration 299\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259380\n",
      "Iteration   200, loss = 0.258955\n",
      "Iteration   300, loss = 0.258949\n",
      "Converged at iteration 311\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259718\n",
      "Iteration   200, loss = 0.259297\n",
      "Converged at iteration 295\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.260384\n",
      "Iteration   200, loss = 0.259965\n",
      "Iteration   300, loss = 0.259959\n",
      "Converged at iteration 310\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259956\n",
      "Iteration   200, loss = 0.259541\n",
      "Converged at iteration 299\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259380\n",
      "Iteration   200, loss = 0.258955\n",
      "Iteration   300, loss = 0.258949\n",
      "Converged at iteration 311\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259718\n",
      "Iteration   200, loss = 0.259297\n",
      "Converged at iteration 295\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.260384\n",
      "Iteration   200, loss = 0.259965\n",
      "Iteration   300, loss = 0.259959\n",
      "Converged at iteration 310\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259956\n",
      "Iteration   200, loss = 0.259541\n",
      "Converged at iteration 299\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259380\n",
      "Iteration   200, loss = 0.258955\n",
      "Iteration   300, loss = 0.258949\n",
      "Converged at iteration 311\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.259718\n",
      "Iteration   200, loss = 0.259297\n",
      "Converged at iteration 295\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.260384\n",
      "Iteration   200, loss = 0.259965\n",
      "Iteration   300, loss = 0.259959\n",
      "Converged at iteration 310\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Best λ = 0.1, Best α = 0.3, Mean F1 = 0.3222\n",
      "Iteration     0, loss = 0.693147\n",
      "Converged at iteration 71\n",
      "Final weights: [-9.63174843e-01  4.45511062e-04  5.50661296e-03  2.24553109e-03\n",
      "  2.51203466e-03 -1.12578400e-02 -6.72510231e-03 -4.47158018e-03\n",
      " -1.49377491e-02 -1.49377491e-02  1.29291309e-01 -3.28767934e-02\n",
      "  7.99216020e-03 -1.00243201e-02 -8.19981917e-03  1.39456347e-04\n",
      " -1.42240186e-02 -9.45720349e-02 -1.97727645e-02 -2.59823237e-02\n",
      " -4.94738289e-02 -5.76178357e-02 -8.24990326e-03 -2.78999557e-02\n",
      " -2.22959790e-02 -3.77987438e-02 -4.61479532e-02 -7.68650016e-03\n",
      " -2.27179085e-02 -6.83095148e-02 -5.49564167e-02]\n"
     ]
    }
   ],
   "source": [
    "# Define your grids\n",
    "lambdas = [1e-3, 1e-2]\n",
    "alphas = np.linspace(0.3, 0.5, 3)\n",
    "\n",
    "# Run cross-validation\n",
    "best_lambda, best_alpha, best_f1, f1_matrix = cross_validate_lambda_alpha(\n",
    "    x_tr_st, y_tr,\n",
    "    lambdas=lambdas,\n",
    "    alphas=alphas,\n",
    "    k=5,\n",
    "    gamma=0.5,\n",
    "    max_iter=500\n",
    ")\n",
    "\n",
    "# Retrain final model on ALL data with best_lambda\n",
    "final_loss, final_w = logistic_regression_penalized_gradient_descent_demo(\n",
    "    y_tr, x_tr_st, max_iter=10000, gamma=0.5, lambda_=best_lambda\n",
    ")\n",
    "\n",
    "print(\"Final weights:\", final_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2abe561-d2ff-4336-b79c-998754308c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL MODEL PERFORMANCE ON TEST SET ---\n",
      "Best λ: 0.1\n",
      "Best α: 0.3\n",
      "Test Accuracy : 0.9099\n",
      "Test F1 Score : 0.0000\n",
      "Test Precision: 0.0000\n",
      "Test Recall   : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\AppData\\Local\\Temp\\ipykernel_3536\\2017611484.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FINAL MODEL EVALUATION ON TEST SET\n",
    "# ============================================================\n",
    "\n",
    "# 1. Build design matrix for test data (add intercept)\n",
    "tx_te = np.c_[np.ones((x_te.shape[0], 1)), x_te]\n",
    "\n",
    "# 2. Compute predicted probabilities on test data\n",
    "y_te_proba = sigmoid(tx_te @ final_w)\n",
    "\n",
    "# 3. Convert probabilities to class predictions using best_alpha\n",
    "y_te_pred = (y_te_proba >= best_alpha).astype(int)\n",
    "\n",
    "# 4. Evaluate metrics\n",
    "def f1_score(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    if tp + fp == 0 or tp + fn == 0:\n",
    "        return 0.0\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "test_f1 = f1_score(y_te, y_te_pred)\n",
    "test_accuracy = np.mean(y_te == y_te_pred)\n",
    "test_precision = np.sum((y_te_pred == 1) & (y_te == 1)) / max(np.sum(y_te_pred == 1), 1)\n",
    "test_recall = np.sum((y_te_pred == 1) & (y_te == 1)) / max(np.sum(y_te == 1), 1)\n",
    "\n",
    "# 5. Print performance summary\n",
    "print(\"\\n--- FINAL MODEL PERFORMANCE ON TEST SET ---\")\n",
    "print(f\"Best λ: {best_lambda}\")\n",
    "print(f\"Best α: {best_alpha}\")\n",
    "print(f\"Test Accuracy : {test_accuracy:.4f}\")\n",
    "print(f\"Test F1 Score : {test_f1:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall   : {test_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf061d4-c891-4e16-b35d-9abb30dc1fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0faa9316-e422-4e7f-9a7a-22790f9d309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 9011.0\n",
      "Accuracy: 0.910\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\AppData\\Local\\Temp\\ipykernel_3536\\2017611484.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "def predict_labels_logistic(x, w, threshold=0.5, return_original_labels=True):\n",
    "\n",
    "    # add bias term\n",
    "    tx = np.c_[np.ones(x.shape[0]), x]\n",
    "    \n",
    "    # compute probabilities\n",
    "    probs = sigmoid(tx @ w)\n",
    "    \n",
    "    # threshold\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    \n",
    "    # convert to {-1, 1} if desired\n",
    "    if return_original_labels:\n",
    "        preds = 2 * preds - 1\n",
    "    \n",
    "    return preds\n",
    "y_pred = predict_labels_logistic(x_te, final_w, return_original_labels=False)\n",
    "n_errors = np.sum(np.abs(y_te - y_pred))\n",
    "accuracy = np.mean(y_te == y_pred)\n",
    "\n",
    "print(\"Misclassified samples:\", n_errors)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(np.sum(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4649094-fbf2-4914-bcb3-6fc423cce6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
