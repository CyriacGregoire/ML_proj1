{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77861085-5158-4892-95ad-beb6ede3fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from Data_cleaning import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8f15e3-bbcf-4df3-a670-e1630669be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data\\dataset\\dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "232104b1-5722-4495-a544-00e093fc4f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000,) (60000,) [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Turn y into 0s and 1s\n",
    "y_tr_ = (y_train + 1) / 2\n",
    "y_tr = y_tr_[:180000]\n",
    "y_va = y_tr_[180000:240000]\n",
    "y_te = y_tr_[240000:300000]\n",
    "print(y_tr.shape, y_te.shape, y_tr, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73bd42f1-675a-4d14-a1ac-720f16cd1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_categorical_features(X, threshold=100):\n",
    "    \"\"\"\n",
    "    Removes categorical (low-cardinality) features from a numeric dataset.\n",
    "    A feature is dropped if it has fewer than `threshold` unique values.\n",
    "    \"\"\"\n",
    "    X_np = np.asarray(X)\n",
    "    n_samples, n_features = X_np.shape\n",
    "    keep_mask = np.array([\n",
    "        np.unique(X_np[:, j]).size >= threshold for j in range(n_features)\n",
    "    ])\n",
    "    X_num = X_np[:, keep_mask]\n",
    "    return X_num, keep_mask\n",
    "\n",
    "def anova_f_test(X, y):\n",
    "    \"\"\"\n",
    "    Compute ANOVA F-statistic for each feature using NumPy only.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (n_samples, n_features)\n",
    "        Feature matrix.\n",
    "    y : np.ndarray, shape (n_samples,)\n",
    "        Class labels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    F_values : np.ndarray, shape (n_features,)\n",
    "        F-statistics for each feature.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    classes = np.unique(y)\n",
    "    k = len(classes)\n",
    "    \n",
    "    overall_means = np.nanmean(X, axis=0)  # handle NaNs safely\n",
    "    \n",
    "    # Initialize sums of squares\n",
    "    ssb = np.zeros(n_features)\n",
    "    ssw = np.zeros(n_features)\n",
    "    \n",
    "    for c in classes:\n",
    "        X_c = X[y == c]\n",
    "        n_c = X_c.shape[0]\n",
    "        mean_c = np.nanmean(X_c, axis=0)\n",
    "        \n",
    "        ssb += n_c * (mean_c - overall_means) ** 2\n",
    "        ssw += np.nansum((X_c - mean_c) ** 2, axis=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    ssw = np.where(ssw == 0, np.nan, ssw)\n",
    "    \n",
    "    F = (ssb / (k - 1)) / (ssw / (n_samples - k))\n",
    "    \n",
    "    return F\n",
    "def logistic_regression_penalized_gradient_descent_weighted(y, X, lambda_, gamma=0.5, max_iter=10000, threshold=1e-8):\n",
    "    \"\"\"\n",
    "    Logistic regression with L2 regularization and class imbalance weighting.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    tx = np.c_[np.ones((n, 1)), X]\n",
    "    w = np.zeros(d + 1)\n",
    "\n",
    "    # Compute class weights\n",
    "    n_pos = np.sum(y == 1)\n",
    "    n_neg = np.sum(y == 0)\n",
    "    w_pos = n / (2 * n_pos)\n",
    "    w_neg = n / (2 * n_neg)\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        y_pred = 1 / (1 + np.exp(-tx @ w))\n",
    "\n",
    "        # Weighted gradient\n",
    "        weights = np.where(y == 1, w_pos, w_neg)\n",
    "        error = y_pred - y\n",
    "        grad = (tx.T @ (weights * error)) / n + lambda_ * np.r_[0, w[1:]]  # no reg. on bias\n",
    "\n",
    "        # Update weights\n",
    "        w -= gamma * grad\n",
    "\n",
    "        # Convergence check\n",
    "        if np.linalg.norm(grad) < threshold:\n",
    "            break\n",
    "\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a63ccc-81f1-433a-88e9-ace35b58fa44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d6a7635-defe-43fb-905c-b29ce3f4303c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 11)\n",
      "(180000, 8)\n"
     ]
    }
   ],
   "source": [
    "# --- CLEANING ---\n",
    "# 0. Remove categorical (low-cardinality) features\n",
    "x_num, cat_mask = remove_categorical_features(x_train, threshold=1000)\n",
    "print(x_num.shape)\n",
    "# 1. Remove features with too many NaNs\n",
    "x_clean, keep_mask = remove_nan_features(x_num)\n",
    "\n",
    "x_clean_tr = x_clean[:180000]\n",
    "x_clean_va = x_clean[180000:240000]\n",
    "x_clean_te = x_clean[240000:300000]\n",
    "\n",
    "# 2. Impute remaining missing values\n",
    "# Compute feature means from the training data (ignore NaNs)\n",
    "train_means = np.nanmean(x_clean_tr, axis=0)\n",
    "\n",
    "# Replace NaNs in training data with training means\n",
    "inds_tr = np.where(np.isnan(x_clean_tr))\n",
    "x_clean_tr[inds_tr] = np.take(train_means, inds_tr[1])\n",
    "\n",
    "# Replace NaNs in validation data with training means\n",
    "inds_va = np.where(np.isnan(x_clean_va))\n",
    "x_clean_va[inds_va] = np.take(train_means, inds_va[1])\n",
    "\n",
    "# Replace NaNs in test data with training means\n",
    "inds_te = np.where(np.isnan(x_clean_te))\n",
    "x_clean_te[inds_te] = np.take(train_means, inds_te[1])\n",
    "\n",
    "# Compute F-scores using only the training data\n",
    "F_scores = anova_f_test(x_clean_tr, y_tr)\n",
    "\n",
    "# Select top 30 features\n",
    "top_k = 30\n",
    "top_features_idx = np.argsort(F_scores)[-top_k:][::-1]\n",
    "\n",
    "# Apply the same feature selection to all sets\n",
    "x_anova_tr = x_clean_tr[:, top_features_idx]\n",
    "x_anova_va = x_clean_va[:, top_features_idx]\n",
    "x_anova_te = x_clean_te[:, top_features_idx]\n",
    "\n",
    "# --- STANDARDIZATION ---\n",
    "x_tr_st, means, stds = standardize_features(x_anova_tr)\n",
    "x_va_st = (x_anova_va - means) / stds\n",
    "x_te_st = (x_anova_te - means) / stds\n",
    "\n",
    "\n",
    "print(x_tr_st.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee303b1a-d641-41eb-889f-6250c7f848e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000, 27) (180000,)\n"
     ]
    }
   ],
   "source": [
    "#check shapes\n",
    "print(x_tr_st.shape, y_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "205a2dab-191a-456f-8c63-adafe4007a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CROSS-VALIDATION PIPELINE FOR LOGISTIC REGRESSION (RIDGE)\n",
    "# ============================================================\n",
    "# ------------------------------------------------------------\n",
    "# 1. Metric function\n",
    "# ------------------------------------------------------------\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Compute F1 score between true and predicted labels.\"\"\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    if tp + fp == 0 or tp + fn == 0:\n",
    "        return 0.0\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Stratified K-Fold Split\n",
    "# ------------------------------------------------------------\n",
    "def stratified_kfold_indices(y, k=5, seed=42):\n",
    "    \"\"\"Return list of (train_idx, val_idx) preserving class ratio.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    idx_pos = np.where(y == 1)[0]\n",
    "    idx_neg = np.where(y == 0)[0]\n",
    "    np.random.shuffle(idx_pos)\n",
    "    np.random.shuffle(idx_neg)\n",
    "    pos_folds = np.array_split(idx_pos, k)\n",
    "    neg_folds = np.array_split(idx_neg, k)\n",
    "\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        val_idx = np.concatenate([pos_folds[i], neg_folds[i]])\n",
    "        train_idx = np.setdiff1d(np.arange(len(y)), val_idx)\n",
    "        folds.append((train_idx, val_idx))\n",
    "    return folds\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Cross-validation for λ and α search\n",
    "# ------------------------------------------------------------\n",
    "def cross_validate_lambda_alpha(\n",
    "    X, y,\n",
    "    lambdas, alphas,\n",
    "    k=5,\n",
    "    gamma=0.5, max_iter=10000, threshold=1e-8,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation to find best (lambda, alpha)\n",
    "    for penalized logistic regression based on mean F1 score.\n",
    "\n",
    "    Uses your existing logistic_regression_penalized_gradient_descent_demo().\n",
    "    \"\"\"\n",
    "\n",
    "    best_lambda, best_alpha, best_mean_f1 = None, None, -1\n",
    "    folds = stratified_kfold_indices(y, k=k, seed=seed)\n",
    "\n",
    "    # Store all mean F1s (optional, for visualization)\n",
    "    f1_matrix = np.zeros((len(lambdas), len(alphas)))\n",
    "\n",
    "    for i_lam, lam in enumerate(lambdas):\n",
    "        for i_alpha, alpha in enumerate(alphas):\n",
    "            fold_f1s = []\n",
    "\n",
    "            # ----- Perform k-fold cross-validation -----\n",
    "            for i, (train_idx, val_idx) in enumerate(folds):\n",
    "                X_train, y_train = X[train_idx], y[train_idx]\n",
    "                X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "                # Train penalized logistic regression with current lambda\n",
    "                w = logistic_regression_penalized_gradient_descent_weighted(\n",
    "                y_train, X_train,\n",
    "                lambda_=lam,\n",
    "                gamma=gamma,\n",
    "                max_iter=max_iter,\n",
    "                threshold=threshold\n",
    ")\n",
    "\n",
    "\n",
    "                # Predict probabilities on validation data\n",
    "                tx_val = np.c_[np.ones((y_val.shape[0], 1)), X_val]\n",
    "                y_proba = sigmoid(tx_val @ w)\n",
    "\n",
    "                # Apply classification threshold α\n",
    "                y_pred = (y_proba >= alpha).astype(int)\n",
    "\n",
    "                # Compute F1 on this fold\n",
    "                fold_f1s.append(f1_score(y_val, y_pred))\n",
    "\n",
    "            # ----- Average F1 across folds -----\n",
    "            mean_f1 = np.mean(fold_f1s)\n",
    "            f1_matrix[i_lam, i_alpha] = mean_f1\n",
    "\n",
    "            # Track best parameters\n",
    "            if mean_f1 > best_mean_f1:\n",
    "                best_mean_f1 = mean_f1\n",
    "                best_lambda = lam\n",
    "                best_alpha = alpha\n",
    "\n",
    "    print(f\"Best λ = {best_lambda}, Best α = {best_alpha}, Mean F1 = {best_mean_f1:.4f}\")\n",
    "    return best_lambda, best_alpha, best_mean_f1, f1_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a188fd0d-fe63-4f9c-953a-3d3ebcbfc660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NaN in X? False\n",
      "Any NaN in y? False\n",
      "Unique values in y: [0. 1.]\n",
      "Shape X: (180000, 30)\n",
      "Shape y: (180000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Any NaN in X?\", np.isnan(x_tr_st).any())\n",
    "print(\"Any NaN in y?\", np.isnan(y_tr).any())\n",
    "print(\"Unique values in y:\", np.unique(y_tr))\n",
    "print(\"Shape X:\", x_tr_st.shape)\n",
    "print(\"Shape y:\", y_tr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc2d48b8-0ed3-493c-89c3-d527af89a35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best λ = 0.001, Best α = 0.5, Val F1 = 0.2933\n"
     ]
    }
   ],
   "source": [
    "lambdas = [1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "alphas  = np.linspace(0.3, 0.5, 3)\n",
    "\n",
    "best_lambda, best_alpha, best_f1 = None, None, -1\n",
    "\n",
    "for lam in lambdas:\n",
    "    # Train on TRAIN set\n",
    "    w = logistic_regression_penalized_gradient_descent_weighted(\n",
    "        y_tr, x_tr_st,\n",
    "        lambda_=lam,\n",
    "        gamma=0.5,\n",
    "        max_iter=500\n",
    "    )\n",
    "\n",
    "    # Predict probabilities on VALIDATION set\n",
    "    tx_va = np.c_[np.ones((x_va_st.shape[0], 1)), x_va_st]\n",
    "    y_val_proba = sigmoid(tx_va @ w)\n",
    "\n",
    "    # Search over classification thresholds\n",
    "    for alpha in alphas:\n",
    "        y_val_pred = (y_val_proba >= alpha).astype(int)\n",
    "        f1 = f1_score(y_va, y_val_pred)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_lambda = lam\n",
    "            best_alpha = alpha\n",
    "\n",
    "print(f\"Best λ = {best_lambda}, Best α = {best_alpha}, Val F1 = {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2abe561-d2ff-4336-b79c-998754308c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine TRAIN + VALIDATION data\n",
    "X_all = np.vstack([x_tr_st, x_va_st])\n",
    "y_all = np.concatenate([y_tr, y_va])\n",
    "\n",
    "# Retrain with best λ\n",
    "final_w = logistic_regression_penalized_gradient_descent_weighted(\n",
    "    y_all, X_all,\n",
    "    lambda_=best_lambda,\n",
    "    gamma=0.5,\n",
    "    max_iter=5000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaf061d4-c891-4e16-b35d-9abb30dc1fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 = 0.2925, Test Accuracy = 0.6696\n"
     ]
    }
   ],
   "source": [
    "tx_te = np.c_[np.ones((x_te_st.shape[0], 1)), x_te_st]\n",
    "y_te_proba = sigmoid(tx_te @ final_w)\n",
    "y_te_pred  = (y_te_proba >= best_alpha).astype(int)\n",
    "\n",
    "final_f1 = f1_score(y_te, y_te_pred)\n",
    "final_acc = np.mean(y_te_pred == y_te)\n",
    "\n",
    "print(f\"Test F1 = {final_f1:.4f}, Test Accuracy = {final_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4649094-fbf2-4914-bcb3-6fc423cce6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels_logistic(x, w, threshold=0.5, return_original_labels=True):\n",
    "\n",
    "    # add bias term\n",
    "    tx = np.c_[np.ones(x.shape[0]), x]\n",
    "    \n",
    "    # compute probabilities\n",
    "    probs = sigmoid(tx @ w)\n",
    "    \n",
    "    # threshold\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    \n",
    "    # convert to {-1, 1} if desired\n",
    "    if return_original_labels:\n",
    "        preds = 2 * preds - 1\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d801925-8946-4d85-a0d9-fbfc7e89086b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
