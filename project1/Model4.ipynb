{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5fa786cb-5c54-4a53-ab00-4db1f5f37dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from Data_cleaning import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec0cb7e1-ebee-49fd-9ffa-06d33217aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data\\dataset\\dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f8b1c298-a97e-44c8-8130-e2c07ce43cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240000,) (88135,) [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Turn y into 0s and 1s\n",
    "y_tr_ = (y_train + 1) / 2\n",
    "y_tr = y_tr_[:240000]\n",
    "y_te = y_tr_[240000:]\n",
    "print(y_tr.shape, y_te.shape, y_tr, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "216cd581-9a46-4243-98b3-4951df6412dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_corr_each_feature(X, y):\n",
    "    \"\"\"\n",
    "    Compute absolute Pearson correlation between each column of X and y,\n",
    "    ignoring NaNs. Returns |r_j| for each feature.\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    corrs = np.zeros(n_features)\n",
    "    \n",
    "    for j in range(n_features):\n",
    "        xj = X[:, j]\n",
    "        \n",
    "        # mask valid (non-NaN) pairs\n",
    "        mask = ~np.isnan(xj) & ~np.isnan(y)\n",
    "        if np.sum(mask) < 2:\n",
    "            corrs[j] = np.nan\n",
    "            continue\n",
    "        \n",
    "        x_valid = xj[mask]\n",
    "        y_valid = y[mask]\n",
    "        \n",
    "        # compute Pearson correlation\n",
    "        x_centered = x_valid - np.mean(x_valid)\n",
    "        y_centered = y_valid - np.mean(y_valid)\n",
    "        \n",
    "        num = np.sum(x_centered * y_centered)\n",
    "        den = np.sqrt(np.sum(x_centered**2) * np.sum(y_centered**2))\n",
    "        corrs[j] = abs(num / den) if den != 0 else np.nan  # <-- absolute value here\n",
    "\n",
    "    return corrs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "26d6f6db-6c09-4a09-9907-5d07eeab8c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240000, 144) (240000,)\n"
     ]
    }
   ],
   "source": [
    "# --- CLEANING ---\n",
    "\"\"\"\n",
    "# 0. Remove categorical (low-cardinality) features\n",
    "x_num, cat_mask = remove_categorical_features(x_train, threshold=10)\n",
    "print(x_num.shape)\n",
    "\"\"\"\n",
    "# 1. Remove features with too many NaNs\n",
    "x_clean, keep_mask = remove_nan_features(x_train)\n",
    "x_clean_tr = x_clean[:240000]\n",
    "x_clean_te = x_clean[240000:]\n",
    "\n",
    "# 2. Impute remaining missing values\n",
    "# Compute feature means from the training data (ignore NaNs)\n",
    "train_means = np.nanmean(x_clean_tr, axis=0)\n",
    "train_stds = np.nanstd(x_clean_tr, axis = 0)\n",
    "# Replace NaNs in training data with training means\n",
    "inds_tr = np.where(np.isnan(x_clean_tr))\n",
    "x_clean_tr[inds_tr] = np.take(train_means, inds_tr[1])\n",
    "\n",
    "# Replace NaNs in test data with training means\n",
    "inds_te = np.where(np.isnan(x_clean_te))\n",
    "x_clean_te[inds_te] = np.take(train_means, inds_te[1])\n",
    "\"\"\"\n",
    "# Compute F-scores using only the training data\n",
    "F_scores = anova_f_test(x_clean_tr, y_tr)\n",
    "\n",
    "\n",
    "# Select top 30 features\n",
    "top_k = \n",
    "top_features_idx = np.argsort(F_scores)[-top_k:][::-1]\n",
    "\n",
    "# Apply the same feature selection to all sets\n",
    "x_anova_tr = x_clean_tr[:, top_features_idx]\n",
    "x_anova_te = x_clean_te[:, top_features_idx]\n",
    "\"\"\"\n",
    "# --- STANDARDIZATION ---\n",
    "x_tr_st = (x_clean_tr - train_means) / train_stds\n",
    "x_te_st = (x_clean_te - train_means) / train_stds\n",
    "\n",
    "\n",
    "print(x_tr_st.shape, y_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bcb25fc6-0489-4b95-8918-a3db317df343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42262, 144) (42262,)\n"
     ]
    }
   ],
   "source": [
    "x_tr_ba, y_tr_ba = balance_data(x_tr_st, y_tr)\n",
    "print(x_tr_ba.shape, y_tr_ba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9466a697-84f7-437c-9f6d-d577439ab24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5081081431065447 [-1.70926626e-02  2.64747576e-02 -1.35694610e-02 -1.35815254e-02\n",
      "  1.05221982e-03 -1.71557880e-02  3.94386838e-03 -2.33641828e-03\n",
      " -2.33641828e-03  1.33759420e-01 -2.00081120e-02 -1.63375036e-03\n",
      " -7.18644285e-03  2.37593864e-03 -1.04251847e-02 -3.22424636e-03\n",
      " -5.85386445e-02 -4.83793257e-02 -5.42016602e-02 -6.62735293e-02\n",
      " -1.87262441e-02 -1.41425416e-02 -4.41069808e-03 -1.53855514e-03\n",
      " -1.11224887e-02  3.84590846e-03 -7.78985903e-03 -9.17470227e-03\n",
      " -2.63354832e-02 -1.01355748e-01 -9.94850164e-03  1.00494809e-03\n",
      "  7.44871490e-03 -4.05707312e-03  4.23609207e-02  3.34737633e-03\n",
      "  1.56486183e-02  1.31373406e-02 -1.00154625e-02  7.39979463e-03\n",
      " -1.31643008e-02 -6.43006046e-03 -7.64933507e-04 -1.50417960e-04\n",
      " -1.16622088e-02  4.67799011e-03  1.50542101e-04 -2.54615894e-02\n",
      "  5.72305020e-03  1.90485568e-02 -2.19375375e-04  1.84587776e-03\n",
      "  3.04779006e-03  8.84876487e-03 -2.25939932e-03 -4.00153166e-03\n",
      "  1.18253185e-02  7.22852850e-03  1.51440332e-02 -4.20824981e-03\n",
      " -2.42804301e-02 -4.35003554e-02  1.68835062e-02 -5.70154722e-03\n",
      "  2.16755742e-02 -1.07484624e-02  3.85057934e-03  1.49120632e-02\n",
      " -7.96849964e-03 -1.31363622e-03 -3.20183330e-03 -1.55685592e-02\n",
      "  5.57265547e-02  3.45066159e-02  4.54812750e-02  6.12692922e-02\n",
      "  1.22364038e-02  4.87582832e-04 -2.32585777e-03 -2.39382502e-02\n",
      " -1.32600972e-01  1.32649868e-01 -1.56712183e-03  1.40448563e-02\n",
      "  2.83798162e-03 -1.17553519e-02 -1.14809347e-02  1.29143183e-02\n",
      " -2.97509633e-02  7.43288931e-02 -7.86628824e-02 -1.44490371e-03\n",
      " -1.09921047e-02  8.10509639e-03 -1.31453936e-02  6.82099763e-03\n",
      " -1.13295740e-03 -3.71619871e-03 -2.63358467e-03 -2.23321861e-02\n",
      " -2.04309023e-02  3.41426354e-02  7.06249713e-03 -2.03728906e-02\n",
      " -1.62032444e-03  7.97008258e-02 -7.50798829e-02 -1.51338839e-02\n",
      " -3.81332434e-02  9.27992228e-03  9.71414670e-03  4.59536465e-03\n",
      "  1.06199055e-02  1.74883801e-02  7.55660076e-03  1.57976321e-02\n",
      "  7.00195528e-03  4.12977166e-02 -1.38975771e-02  1.63833646e-02\n",
      "  9.67502360e-03 -1.14528933e-03  4.51330040e-03 -1.56084131e-02\n",
      " -7.28802835e-03 -6.80116390e-02 -2.19005640e+00  2.18418562e+00\n",
      "  8.32073559e-03  7.15412714e-02  8.82853559e-03 -2.96786780e-02\n",
      "  1.61034527e-02  1.31202151e-02 -3.30677434e-02 -1.76595482e-02\n",
      " -7.03314345e-03  2.55339322e-02  2.09673580e-03  2.70340930e-03\n",
      " -4.64052718e-03  3.47372640e-02 -2.83169951e-02  4.25710671e-02] (144,)\n"
     ]
    }
   ],
   "source": [
    "loss, w = ridge_regression(\n",
    "    y_tr_ba, x_tr_ba,lambda_=1e-5)\n",
    "print(loss, w, w.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "38761bfb-2df4-41f1-b2c6-10e22dfaf649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88135, 144) (144,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 144 is different from 145)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_te_st\u001b[38;5;241m.\u001b[39mshape, w\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m predict_ridge(x_te_st, w, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m n_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mabs(y_te_clean \u001b[38;5;241m-\u001b[39m y_pred))\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\ML_proj1\\project1\\implementations.py:166\u001b[0m, in \u001b[0;36mpredict_ridge\u001b[1;34m(X, w, limit)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mkfold_logistic_ridge\u001b[39m(\n\u001b[0;32m    157\u001b[0m         X, y, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, lambda_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    158\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m    Performs K-Fold Cross Validation for logistic regression.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    X : np.ndarray, shape (n_samples, n_features) with Nan values and unbalanced data\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    all the features will be used in this model (otherwise delete befor calling the function)\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m    y : np.ndarray, shape (n_samples,) with binary labels (0/1)\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;124;03m    k : int\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m        Number of folds.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m    lam : float\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m        Regularization strength.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m    lr : float\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m        Learning rate.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m    epochs : int\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m        Number of gradient descent epochs.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    batch_size : int\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m        Batch size for training.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    random_state : int or None\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m        For reproducible shuffling.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    mean_accuracy : float\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m        Average validation accuracy across folds.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    mean_dispersions_pred : float\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m        Average predicted dispersion across folds.\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m    mean_dispersions_true : float\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m        Average true dispersion across folds.\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    w : np.ndarray, shape (n_features + 1,)\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m        Model parameters from the last fold (arbitrary fold).\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m         np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(random_state)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 144 is different from 145)"
     ]
    }
   ],
   "source": [
    "print(x_te_st.shape, w.shape)\n",
    "\n",
    "y_pred = predict_ridge(x_te_st, w, 0.5)\n",
    "print(\"xd\")\n",
    "n_errors = np.sum(np.abs(y_te_clean - y_pred))\n",
    "accuracy = np.mean(y_te_clean == y_pred)\n",
    "\n",
    "print(\"Misclassified samples:\", n_errors)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(np.sum(y_pred), np.sum(y_te_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "82260f60-f9b3-4b08-8599-13d7fe487d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Compute F1 score between true and predicted labels.\"\"\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    if tp + fp == 0 or tp + fn == 0:\n",
    "        return 0.0\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b8dad6ee-a308-469f-9d1e-15089466a98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4006347527109231\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_te_clean, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a98ac2-669d-4aed-8a7d-3e47eb3ee6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
