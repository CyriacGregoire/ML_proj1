{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaded5ba-d2e7-413f-95f0-fd2dda0cae84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nChanges from OHE2:\\n-Here we do not drop features according to correlation with the target\\n-We also try here lambda = 1e-7\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Changes from OHE2:\n",
    "-Here we do not drop features according to correlation with the target\n",
    "-We also try here lambda = 1e-7\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e573548-4263-465b-a59d-889d84e5828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from Data_cleaning import *\n",
    "from helpers import *\n",
    "from exploratory_data_analysis import *\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfcc5cb-2547-473e-9262-593fbee3a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data\\dataset\\dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f19f029-3fe3-48df-85db-dc014737c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tr = (y_train + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9ad8390-3cdf-470f-99d9-d78ac49bed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTINELS = (7, 9, 77, 88, 99, 777, 888, 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a64ba230-81a2-4fc1-a374-41b7f880ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sentinels_with_nan(X, sentinels=SENTINELS):\n",
    "    Xf = X.astype(np.float64, copy=True)\n",
    "    if len(sentinels) == 0:\n",
    "        return Xf\n",
    "    sentinels = np.array(sentinels, dtype=np.float64)\n",
    "    for s in sentinels:\n",
    "        Xf[Xf == s] = np.nan\n",
    "    return Xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03b022dd-c2a3-4434-8201-c6b9d76bfb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_sent = replace_sentinels_with_nan(x_train)\n",
    "x_te_sent = replace_sentinels_with_nan(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32cd612a-c252-4583-921e-fb84de1350e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 321) (109379, 321)\n"
     ]
    }
   ],
   "source": [
    "print(x_tr_sent.shape, x_te_sent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "add28ffe-6d2a-4476-ae00-fdb66be8d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_near_constant_columns(X, thresh=0.99):\n",
    "    \"\"\"\n",
    "    Returns a boolean mask of columns to KEEP.\n",
    "    A column is dropped (mask=False) if the most frequent finite value\n",
    "    appears in ≥ thresh proportion of the finite entries.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    keep = np.ones(d, dtype=bool)\n",
    "    for j in range(d):\n",
    "        col = X[:, j]\n",
    "        m = np.isfinite(col)\n",
    "        if not np.any(m):\n",
    "            # all missing → keep for now (the imputer will handle)\n",
    "            continue\n",
    "        vals = col[m]\n",
    "        u, c = np.unique(vals, return_counts=True)\n",
    "        top_prop = c.max() / c.sum()\n",
    "        if top_prop >= thresh:\n",
    "            keep[j] = False\n",
    "    return keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "630f77d2-b391-41a4-a0d2-a5e7eeb92f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 311 columns after removing near-constant ones.\n"
     ]
    }
   ],
   "source": [
    "keep_mask = detect_near_constant_columns(x_train, thresh=0.99)\n",
    "x_tr_noconst = x_tr_sent[:, keep_mask]\n",
    "x_test_noconst  = x_te_sent[:, keep_mask]\n",
    "print(f\"Kept {x_tr_noconst.shape[1]} columns after removing near-constant ones.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1d81eeb-df80-45af-9360-e5570a87d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, nan_mask = remove_nan_features(x_train, 0.9)\n",
    "X_test = x_test[:, nan_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9e79fa0-edcd-465e-a21a-cc87d92346da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 222) (109379, 222)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19a73a9e-c81c-4858-9898-6d36e4e4796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_id_like_columns(X, max_unique_ratio=0.9):\n",
    "    \"\"\"\n",
    "    Detects columns that behave like IDs (almost all values unique).\n",
    "    \n",
    "    A column is marked for removal if:\n",
    "        (# unique finite values / total rows) >= max_unique_ratio.\n",
    "    \n",
    "    Returns:\n",
    "        keep_mask (bool array): True for columns to keep, False for ID-like columns.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    keep = np.ones(d, dtype=bool)\n",
    "    for j in range(d):\n",
    "        col = X[:, j]\n",
    "        m = np.isfinite(col)\n",
    "        if not np.any(m):\n",
    "            # all NaN -> keep for now (handled elsewhere)\n",
    "            continue\n",
    "        u = np.unique(col[m]).size\n",
    "        unique_ratio = u / np.count_nonzero(m)\n",
    "        if unique_ratio >= max_unique_ratio:\n",
    "            keep[j] = False\n",
    "    return keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd3102a1-919d-46aa-85ac-92dd385b8fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 222 columns after removing ID-like ones.\n"
     ]
    }
   ],
   "source": [
    "un_mask = detect_id_like_columns(X_train, max_unique_ratio=0.9)\n",
    "X_train_unique = X_train[:, un_mask]\n",
    "X_test_unique  = X_test[:, un_mask]\n",
    "print(f\"Kept {X_train_unique.shape[1]} columns after removing ID-like ones.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87fa4409-2e9b-49a6-b09d-1b2ac946d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_mask, num_mask = detect_categorical_features(X_train, max_unique=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61d3bf7f-19b7-4271-9a14-46c0eb6e29e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: 157\n",
      "Numeric features: 65\n"
     ]
    }
   ],
   "source": [
    "print(\"Categorical features:\", np.sum(cat_mask))\n",
    "print(\"Numeric features:\", np.sum(num_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25e79e9d-7946-47ac-a3c2-39d5547fd7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_categorical_missing_code(X, cat_mask, missing_code=-1.0):\n",
    "    \"\"\"\n",
    "    Replace NaNs in *categorical* columns with a special code (default -1),\n",
    "    leaving numeric columns unchanged.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (n_samples, n_features)\n",
    "        Full feature matrix (numeric + categorical).\n",
    "    cat_mask : np.ndarray of bool, shape (n_features,)\n",
    "        True for categorical columns, False for numeric ones.\n",
    "    missing_code : float, default=-1.0\n",
    "        Code used to represent 'Missing' category.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_imputed : np.ndarray, same shape as X\n",
    "        Copy of X where NaNs in categorical columns are replaced by `missing_code`.\n",
    "    \"\"\"\n",
    "    Xf = np.array(X, dtype=np.float64, copy=True)\n",
    "    cat_idx = np.where(cat_mask)[0]\n",
    "\n",
    "    for j in cat_idx:\n",
    "        col = Xf[:, j]\n",
    "        nan_mask = np.isnan(col)\n",
    "        if np.any(nan_mask):\n",
    "            col[nan_mask] = missing_code\n",
    "            Xf[:, j] = col\n",
    "\n",
    "    return Xf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "622f1166-4177-469c-bfa3-ccfd43491faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat = impute_categorical_missing_code(X_train, cat_mask, missing_code=-1.0)\n",
    "X_test_cat  = impute_categorical_missing_code(X_test,  cat_mask, missing_code=-1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7abeb201-81b0-42c8-be04-d5e838728f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_va, y_va, x_te, y_te = stratified_three_way_split(X_train_cat, Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f52657ef-a06b-48b3-9604-553bdf3c2030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 222) (229695, 222) (229695,) (49220, 222) (49220,) (49220, 222) (49220,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, x_tr.shape, y_tr.shape, x_va.shape, y_va.shape, x_te.shape, y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "029090c6-776a-4d19-ba96-6706ce90ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical imputation (compute medians from training)\n",
    "x_tr_imp, medians = impute_numerical(x_tr, num_mask)\n",
    "# Apply same medians to val/test\n",
    "x_va_imp, _ = impute_numerical(x_va, num_mask, medians)\n",
    "x_te_imp, _ = impute_numerical(x_te, num_mask, medians)\n",
    "x_test_imp, _ = impute_numerical(X_test_cat, num_mask, medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "42074bd4-7a55-4e44-a3fd-1f2fd4185230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "# Check for any remaining NaN values\n",
    "print(np.isnan(x_tr_imp).sum(),\n",
    "      np.isnan(x_va_imp).sum(),\n",
    "      np.isnan(x_te_imp).sum(), np.isnan(x_test_imp).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b701e560-cdf7-4900-aae7-793bab8dce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_std, means, stds = standardize_features(x_tr_imp, num_mask)\n",
    "x_va_std, _, _ = standardize_features(x_va_imp, num_mask, means, stds)\n",
    "x_te_std, _, _ = standardize_features(x_te_imp, num_mask, means, stds)\n",
    "x_test_std, _, _ = standardize_features(x_test_imp, num_mask, means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23c0c8cb-0db2-4b3f-b4b4-8fd42f9294c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit one-hot encoder on training data\n",
    "x_tr_enc, categories = one_hot_encode(x_tr_std, cat_mask, drop_first=True)\n",
    "\n",
    "# Apply the same categories to validation and test data\n",
    "x_va_enc, _ = one_hot_encode(x_va_std, cat_mask, categories, drop_first=True)\n",
    "x_te_enc, _ = one_hot_encode(x_te_std, cat_mask, categories, drop_first=True)\n",
    "x_test_enc, _ = one_hot_encode(x_test_std, cat_mask, categories, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3852be6-87f7-4b43-a1cf-ff30b1fdbedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(229695, 852) (49220, 852) (49220, 852) (109379, 852)\n"
     ]
    }
   ],
   "source": [
    "print(x_tr_enc.shape, x_va_enc.shape, x_te_enc.shape, x_test_enc.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "829b21c5-0a47-4bb9-ad60-70a19aeddd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_feature_selection(X, y, cat_mask, top_k=None, percentile=None, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Performs chi-squared feature selection *only on categorical features*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (n_samples, n_features)\n",
    "        Full feature matrix (numeric + categorical, after encoding).\n",
    "    y : np.ndarray, shape (n_samples,)\n",
    "        Binary target labels (0/1).\n",
    "    cat_mask : np.ndarray of bool, shape (n_features,)\n",
    "        Boolean mask: True = categorical feature, False = numeric feature.\n",
    "    top_k : int or None\n",
    "        Keep top_k categorical features by chi² score.\n",
    "    percentile : float or None\n",
    "        Keep the top `percentile`% of categorical features (e.g. 40 -> keep top 40%).\n",
    "    eps : float\n",
    "        Small constant to avoid division by zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    chi2_scores : np.ndarray, shape (n_features,)\n",
    "        Chi-squared scores (0 for numeric columns).\n",
    "    keep_mask : np.ndarray of bool, shape (n_features,)\n",
    "        True for features to keep, False for features to drop.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    chi2_scores = np.zeros(d)\n",
    "    y = (y > 0).astype(int)\n",
    "\n",
    "    # Precompute label masks\n",
    "    y_pos = (y == 1)\n",
    "    y_neg = ~y_pos\n",
    "    n_pos = np.sum(y_pos)\n",
    "    n_neg = np.sum(y_neg)\n",
    "\n",
    "    cat_idx = np.where(cat_mask)[0]\n",
    "\n",
    "    # Compute chi² only for categorical columns\n",
    "    for j in cat_idx:\n",
    "        col = X[:, j]\n",
    "        # observed counts\n",
    "        o_pos = np.sum(col[y_pos])\n",
    "        o_neg = np.sum(col[y_neg])\n",
    "\n",
    "        # expected counts under independence\n",
    "        col_sum = np.sum(col)\n",
    "        if col_sum == 0:\n",
    "            chi2_scores[j] = 0\n",
    "            continue\n",
    "\n",
    "        e_pos = (col_sum * n_pos) / n\n",
    "        e_neg = (col_sum * n_neg) / n\n",
    "\n",
    "        chi2_scores[j] = ((o_pos - e_pos)**2 / (e_pos + eps)) + ((o_neg - e_neg)**2 / (e_neg + eps))\n",
    "\n",
    "    # Select categorical features\n",
    "    cat_scores = chi2_scores[cat_idx]\n",
    "    if percentile is not None:\n",
    "        k = int(np.ceil(len(cat_idx) * (percentile / 100)))\n",
    "    elif top_k is not None:\n",
    "        k = min(top_k, len(cat_idx))\n",
    "    else:\n",
    "        raise ValueError(\"Specify either top_k or percentile.\")\n",
    "\n",
    "    threshold = np.sort(cat_scores)[-k]\n",
    "    keep_mask_cat = chi2_scores >= threshold\n",
    "\n",
    "    # Merge: keep all numeric + selected categorical\n",
    "    keep_mask = (~cat_mask) | keep_mask_cat\n",
    "    return chi2_scores, keep_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41d0c382-66cc-452a-b424-85661036ee2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 380 / 852 features\n"
     ]
    }
   ],
   "source": [
    "new_cat_mask, new_num_mask = detect_categorical_features(x_tr_enc, max_unique=10)\n",
    "chi2_scores, chi_mask = chi2_feature_selection(x_tr_enc, y_tr, new_cat_mask, percentile=40)\n",
    "print(f\"Kept {chi_mask.sum()} / {len(chi_mask)} features\")\n",
    "\n",
    "# Apply to all sets\n",
    "x_tr_sel = x_tr_enc[:, chi_mask]\n",
    "x_va_sel = x_va_enc[:, chi_mask]\n",
    "x_te_sel = x_te_enc[:, chi_mask]\n",
    "x_test_sel = x_test_enc[:, chi_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05e24ffc-06f3-45a9-a5b4-9d98a65983f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_categorical_mask, new_numerical_mask = detect_categorical_features(x_tr_sel, max_unique=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e242d5bb-07dd-4712-b2c8-29c4e90ca3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315 65\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(new_categorical_mask), np.sum(new_numerical_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77787bea-f9ae-4147-9eda-86199fbbe28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(229695, 380) (49220, 380) (49220, 380) (109379, 380)\n"
     ]
    }
   ],
   "source": [
    "print(x_tr_sel.shape, x_va_sel.shape, x_te_sel.shape, x_test_sel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "49903d84-3ce8-40f0-b96e-5a8f40da8e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     0 | Loss = 0.693147 | GradNorm = 0.6153\n",
      "Iter   100 | Loss = 0.673456 | GradNorm = 1.1636\n",
      "Iter   200 | Loss = 0.653394 | GradNorm = 1.1245\n",
      "Iter   300 | Loss = 0.643178 | GradNorm = 1.1017\n",
      "Iter   400 | Loss = 0.636834 | GradNorm = 1.0861\n",
      "Iter   500 | Loss = 0.632589 | GradNorm = 1.0748\n",
      "Iter   600 | Loss = 0.629631 | GradNorm = 1.0666\n",
      "Iter   700 | Loss = 0.627509 | GradNorm = 1.0605\n",
      "Iter   800 | Loss = 0.625950 | GradNorm = 1.0560\n",
      "Iter   900 | Loss = 0.624780 | GradNorm = 1.0525\n",
      "Iter  1000 | Loss = 0.623885 | GradNorm = 1.0498\n",
      "Iter  1100 | Loss = 0.623186 | GradNorm = 1.0477\n",
      "Iter  1200 | Loss = 0.622632 | GradNorm = 1.0460\n",
      "Iter  1300 | Loss = 0.622186 | GradNorm = 1.0447\n",
      "Iter  1400 | Loss = 0.621821 | GradNorm = 1.0436\n",
      "Iter  1500 | Loss = 0.621517 | GradNorm = 1.0427\n",
      "Iter  1600 | Loss = 0.621262 | GradNorm = 1.0419\n",
      "Iter  1700 | Loss = 0.621045 | GradNorm = 1.0413\n",
      "Iter  1800 | Loss = 0.620858 | GradNorm = 1.0408\n",
      "Iter  1900 | Loss = 0.620696 | GradNorm = 1.0403\n",
      "Iter  2000 | Loss = 0.620554 | GradNorm = 1.0399\n",
      "Iter  2100 | Loss = 0.620427 | GradNorm = 1.0395\n",
      "Iter  2200 | Loss = 0.620315 | GradNorm = 1.0392\n",
      "Iter  2300 | Loss = 0.620214 | GradNorm = 1.0389\n",
      "Iter  2400 | Loss = 0.620123 | GradNorm = 1.0387\n",
      "Iter  2500 | Loss = 0.620040 | GradNorm = 1.0384\n",
      "Iter  2600 | Loss = 0.619964 | GradNorm = 1.0382\n",
      "Iter  2700 | Loss = 0.619894 | GradNorm = 1.0380\n",
      "Iter  2800 | Loss = 0.619830 | GradNorm = 1.0379\n",
      "Iter  2900 | Loss = 0.619770 | GradNorm = 1.0377\n",
      "Iter  3000 | Loss = 0.619715 | GradNorm = 1.0375\n",
      "Iter  3100 | Loss = 0.619663 | GradNorm = 1.0374\n",
      "Iter  3200 | Loss = 0.619614 | GradNorm = 1.0373\n",
      "Iter  3300 | Loss = 0.619569 | GradNorm = 1.0372\n",
      "Iter  3400 | Loss = 0.619526 | GradNorm = 1.0370\n",
      "Iter  3500 | Loss = 0.619486 | GradNorm = 1.0369\n",
      "Iter  3600 | Loss = 0.619448 | GradNorm = 1.0368\n",
      "Iter  3700 | Loss = 0.619411 | GradNorm = 1.0368\n",
      "Iter  3800 | Loss = 0.619377 | GradNorm = 1.0367\n",
      "Iter  3900 | Loss = 0.619344 | GradNorm = 1.0366\n",
      "Iter  4000 | Loss = 0.619313 | GradNorm = 1.0365\n",
      "Iter  4100 | Loss = 0.619283 | GradNorm = 1.0364\n",
      "Iter  4200 | Loss = 0.619255 | GradNorm = 1.0363\n",
      "Iter  4300 | Loss = 0.619227 | GradNorm = 1.0363\n",
      "Iter  4400 | Loss = 0.619201 | GradNorm = 1.0362\n",
      "Iter  4500 | Loss = 0.619176 | GradNorm = 1.0362\n",
      "Iter  4600 | Loss = 0.619152 | GradNorm = 1.0361\n",
      "Iter  4700 | Loss = 0.619128 | GradNorm = 1.0360\n",
      "Iter  4800 | Loss = 0.619106 | GradNorm = 1.0360\n",
      "Iter  4900 | Loss = 0.619084 | GradNorm = 1.0359\n",
      "Iter  5000 | Loss = 0.619063 | GradNorm = 1.0359\n",
      "Iter  5100 | Loss = 0.619043 | GradNorm = 1.0358\n",
      "Iter  5200 | Loss = 0.619023 | GradNorm = 1.0358\n",
      "Iter  5300 | Loss = 0.619004 | GradNorm = 1.0357\n",
      "Iter  5400 | Loss = 0.618986 | GradNorm = 1.0357\n",
      "Iter  5500 | Loss = 0.618968 | GradNorm = 1.0356\n",
      "Iter  5600 | Loss = 0.618951 | GradNorm = 1.0356\n",
      "Iter  5700 | Loss = 0.618934 | GradNorm = 1.0356\n",
      "Iter  5800 | Loss = 0.618917 | GradNorm = 1.0355\n",
      "Iter  5900 | Loss = 0.618901 | GradNorm = 1.0355\n",
      "Iter  6000 | Loss = 0.618886 | GradNorm = 1.0354\n",
      "Iter  6100 | Loss = 0.618871 | GradNorm = 1.0354\n",
      "Iter  6200 | Loss = 0.618856 | GradNorm = 1.0354\n",
      "Iter  6300 | Loss = 0.618842 | GradNorm = 1.0353\n",
      "Iter  6400 | Loss = 0.618828 | GradNorm = 1.0353\n",
      "Iter  6500 | Loss = 0.618814 | GradNorm = 1.0353\n",
      "Iter  6600 | Loss = 0.618801 | GradNorm = 1.0352\n",
      "Iter  6700 | Loss = 0.618788 | GradNorm = 1.0352\n",
      "Iter  6800 | Loss = 0.618775 | GradNorm = 1.0352\n",
      "Iter  6900 | Loss = 0.618763 | GradNorm = 1.0351\n",
      "Iter  7000 | Loss = 0.618751 | GradNorm = 1.0351\n",
      "Iter  7100 | Loss = 0.618739 | GradNorm = 1.0351\n",
      "Iter  7200 | Loss = 0.618727 | GradNorm = 1.0350\n",
      "Iter  7300 | Loss = 0.618716 | GradNorm = 1.0350\n",
      "Iter  7400 | Loss = 0.618705 | GradNorm = 1.0350\n",
      "Iter  7500 | Loss = 0.618694 | GradNorm = 1.0350\n",
      "Iter  7600 | Loss = 0.618683 | GradNorm = 1.0349\n",
      "Iter  7700 | Loss = 0.618673 | GradNorm = 1.0349\n",
      "Iter  7800 | Loss = 0.618663 | GradNorm = 1.0349\n",
      "Iter  7900 | Loss = 0.618653 | GradNorm = 1.0349\n",
      "Iter  8000 | Loss = 0.618643 | GradNorm = 1.0348\n",
      "Iter  8100 | Loss = 0.618633 | GradNorm = 1.0348\n",
      "Iter  8200 | Loss = 0.618624 | GradNorm = 1.0348\n",
      "Iter  8300 | Loss = 0.618615 | GradNorm = 1.0348\n",
      "Iter  8400 | Loss = 0.618605 | GradNorm = 1.0347\n",
      "Iter  8500 | Loss = 0.618596 | GradNorm = 1.0347\n",
      "Iter  8600 | Loss = 0.618588 | GradNorm = 1.0347\n",
      "Iter  8700 | Loss = 0.618579 | GradNorm = 1.0347\n",
      "Iter  8800 | Loss = 0.618571 | GradNorm = 1.0347\n",
      "Iter  8900 | Loss = 0.618562 | GradNorm = 1.0346\n",
      "Iter  9000 | Loss = 0.618554 | GradNorm = 1.0346\n",
      "Iter  9100 | Loss = 0.618546 | GradNorm = 1.0346\n",
      "Iter  9200 | Loss = 0.618538 | GradNorm = 1.0346\n",
      "Iter  9300 | Loss = 0.618530 | GradNorm = 1.0346\n",
      "Iter  9400 | Loss = 0.618523 | GradNorm = 1.0345\n",
      "Iter  9500 | Loss = 0.618515 | GradNorm = 1.0345\n",
      "Iter  9600 | Loss = 0.618508 | GradNorm = 1.0345\n",
      "Iter  9700 | Loss = 0.618500 | GradNorm = 1.0345\n",
      "Iter  9800 | Loss = 0.618493 | GradNorm = 1.0345\n",
      "Iter  9900 | Loss = 0.618486 | GradNorm = 1.0345\n",
      "Final training loss: 0.6012048327939511\n"
     ]
    }
   ],
   "source": [
    "best_lambda = 1e-8\n",
    "best_pos_weight = 9.12\n",
    "best_neg_weight = 1.0\n",
    "\n",
    "final_loss, w_final = logistic_regression_weighted_gd(\n",
    "    y_tr, x_tr_sel,\n",
    "    lambda_=best_lambda,\n",
    "    gamma=0.5,\n",
    "    pos_weight=best_pos_weight,\n",
    "    neg_weight=best_neg_weight,\n",
    "    max_iter=10000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Final training loss:\", final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b1cfab16-d751-4db6-bbe2-9fe913b46cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.680318325461723\n"
     ]
    }
   ],
   "source": [
    "print(w_final @ w_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af9b1635-a31c-4eab-af25-234ab8f12481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 86.73%\n",
      " F1 Score: 0.4209\n"
     ]
    }
   ],
   "source": [
    "acc, f1 = evaluate_model(y_va, x_va_sel, w_final, 0.94)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7b366ee-628d-48ab-a899-49743e3d1b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/6 (pos_weight=9, lambda_=1e-08) ===\n",
      "New best F1 = 0.4170\n",
      "New best F1 = 0.4173\n",
      "New best F1 = 0.4175\n",
      "New best F1 = 0.4177\n",
      "New best F1 = 0.4182\n",
      "New best F1 = 0.4186\n",
      "New best F1 = 0.4187\n",
      "New best F1 = 0.4194\n",
      "New best F1 = 0.4201\n",
      "New best F1 = 0.4203\n",
      "New best F1 = 0.4205\n",
      "New best F1 = 0.4205\n",
      "New best F1 = 0.4206\n",
      "New best F1 = 0.4212\n",
      "New best F1 = 0.4216\n",
      "New best F1 = 0.4221\n",
      "New best F1 = 0.4222\n",
      "\n",
      "=== Run 2/6 (pos_weight=9, lambda_=1e-07) ===\n",
      "\n",
      "=== Run 3/6 (pos_weight=9.5, lambda_=1e-08) ===\n",
      "New best F1 = 0.4224\n",
      "\n",
      "=== Run 4/6 (pos_weight=9.5, lambda_=1e-07) ===\n",
      "\n",
      "=== Run 5/6 (pos_weight=10, lambda_=1e-08) ===\n",
      "\n",
      "=== Run 6/6 (pos_weight=10, lambda_=1e-07) ===\n",
      "\n",
      "=== Grid Search Complete ===\n",
      "Best F1 = 0.4224 at pos_weight=9.5, λ=1e-08, threshold=0.7203517587939698\n"
     ]
    }
   ],
   "source": [
    "best_params, best_f1, results = grid_search(\n",
    "    y_tr, x_tr_final,\n",
    "    y_va, x_va_final,\n",
    "    pos_weights=[9, 9.5, 10],\n",
    "    lambdas=[1e-8, 1e-7],\n",
    "    thresholds=np.linspace(0.65, 0.85, 200),\n",
    "    max_iter=10000,\n",
    "    gamma=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "708f058b-afb4-4686-9795-eba2efd85d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = np.vstack([x_tr_final, x_va_final])\n",
    "y_final = np.concatenate([y_tr, y_va])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0adc122e-7a93-430f-9231-2fbd06f0e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     0 | Loss = 0.693147 | GradNorm = 0.4448\n",
      "Iter   100 | Loss = 0.472072 | GradNorm = 0.0239\n",
      "Iter   200 | Loss = 0.467679 | GradNorm = 0.0071\n",
      "Iter   300 | Loss = 0.465887 | GradNorm = 0.0050\n",
      "Iter   400 | Loss = 0.464938 | GradNorm = 0.0038\n",
      "Iter   500 | Loss = 0.464366 | GradNorm = 0.0030\n",
      "Iter   600 | Loss = 0.463989 | GradNorm = 0.0025\n",
      "Iter   700 | Loss = 0.463723 | GradNorm = 0.0021\n",
      "Iter   800 | Loss = 0.463527 | GradNorm = 0.0018\n",
      "Iter   900 | Loss = 0.463375 | GradNorm = 0.0016\n",
      "Iter  1000 | Loss = 0.463254 | GradNorm = 0.0015\n",
      "Iter  1100 | Loss = 0.463154 | GradNorm = 0.0013\n",
      "Iter  1200 | Loss = 0.463071 | GradNorm = 0.0012\n",
      "Iter  1300 | Loss = 0.462999 | GradNorm = 0.0012\n",
      "Iter  1400 | Loss = 0.462937 | GradNorm = 0.0011\n",
      "Iter  1500 | Loss = 0.462882 | GradNorm = 0.0010\n",
      "Iter  1600 | Loss = 0.462833 | GradNorm = 0.0010\n",
      "Iter  1700 | Loss = 0.462790 | GradNorm = 0.0009\n",
      "Iter  1800 | Loss = 0.462750 | GradNorm = 0.0009\n",
      "Iter  1900 | Loss = 0.462714 | GradNorm = 0.0008\n",
      "Iter  2000 | Loss = 0.462681 | GradNorm = 0.0008\n",
      "Iter  2100 | Loss = 0.462651 | GradNorm = 0.0008\n",
      "Iter  2200 | Loss = 0.462623 | GradNorm = 0.0007\n",
      "Iter  2300 | Loss = 0.462597 | GradNorm = 0.0007\n",
      "Iter  2400 | Loss = 0.462573 | GradNorm = 0.0007\n",
      "Iter  2500 | Loss = 0.462551 | GradNorm = 0.0007\n",
      "Iter  2600 | Loss = 0.462530 | GradNorm = 0.0006\n",
      "Iter  2700 | Loss = 0.462510 | GradNorm = 0.0006\n",
      "Iter  2800 | Loss = 0.462491 | GradNorm = 0.0006\n",
      "Iter  2900 | Loss = 0.462474 | GradNorm = 0.0006\n",
      "Iter  3000 | Loss = 0.462457 | GradNorm = 0.0006\n",
      "Iter  3100 | Loss = 0.462442 | GradNorm = 0.0006\n",
      "Iter  3200 | Loss = 0.462427 | GradNorm = 0.0005\n",
      "Iter  3300 | Loss = 0.462413 | GradNorm = 0.0005\n",
      "Iter  3400 | Loss = 0.462400 | GradNorm = 0.0005\n",
      "Iter  3500 | Loss = 0.462387 | GradNorm = 0.0005\n",
      "Iter  3600 | Loss = 0.462375 | GradNorm = 0.0005\n",
      "Iter  3700 | Loss = 0.462363 | GradNorm = 0.0005\n",
      "Iter  3800 | Loss = 0.462352 | GradNorm = 0.0005\n",
      "Iter  3900 | Loss = 0.462341 | GradNorm = 0.0005\n",
      "Iter  4000 | Loss = 0.462331 | GradNorm = 0.0004\n",
      "Iter  4100 | Loss = 0.462321 | GradNorm = 0.0004\n",
      "Iter  4200 | Loss = 0.462312 | GradNorm = 0.0004\n",
      "Iter  4300 | Loss = 0.462303 | GradNorm = 0.0004\n",
      "Iter  4400 | Loss = 0.462294 | GradNorm = 0.0004\n",
      "Iter  4500 | Loss = 0.462286 | GradNorm = 0.0004\n",
      "Iter  4600 | Loss = 0.462277 | GradNorm = 0.0004\n",
      "Iter  4700 | Loss = 0.462270 | GradNorm = 0.0004\n",
      "Iter  4800 | Loss = 0.462262 | GradNorm = 0.0004\n",
      "Iter  4900 | Loss = 0.462255 | GradNorm = 0.0004\n",
      "Iter  5000 | Loss = 0.462248 | GradNorm = 0.0004\n",
      "Iter  5100 | Loss = 0.462241 | GradNorm = 0.0004\n",
      "Iter  5200 | Loss = 0.462234 | GradNorm = 0.0004\n",
      "Iter  5300 | Loss = 0.462228 | GradNorm = 0.0004\n",
      "Iter  5400 | Loss = 0.462222 | GradNorm = 0.0003\n",
      "Iter  5500 | Loss = 0.462216 | GradNorm = 0.0003\n",
      "Iter  5600 | Loss = 0.462210 | GradNorm = 0.0003\n",
      "Iter  5700 | Loss = 0.462204 | GradNorm = 0.0003\n",
      "Iter  5800 | Loss = 0.462199 | GradNorm = 0.0003\n",
      "Iter  5900 | Loss = 0.462193 | GradNorm = 0.0003\n",
      "Iter  6000 | Loss = 0.462188 | GradNorm = 0.0003\n",
      "Iter  6100 | Loss = 0.462183 | GradNorm = 0.0003\n",
      "Iter  6200 | Loss = 0.462178 | GradNorm = 0.0003\n",
      "Iter  6300 | Loss = 0.462173 | GradNorm = 0.0003\n",
      "Iter  6400 | Loss = 0.462169 | GradNorm = 0.0003\n",
      "Iter  6500 | Loss = 0.462164 | GradNorm = 0.0003\n",
      "Iter  6600 | Loss = 0.462160 | GradNorm = 0.0003\n",
      "Iter  6700 | Loss = 0.462155 | GradNorm = 0.0003\n",
      "Iter  6800 | Loss = 0.462151 | GradNorm = 0.0003\n",
      "Iter  6900 | Loss = 0.462147 | GradNorm = 0.0003\n",
      "Iter  7000 | Loss = 0.462143 | GradNorm = 0.0003\n",
      "Iter  7100 | Loss = 0.462139 | GradNorm = 0.0003\n",
      "Iter  7200 | Loss = 0.462135 | GradNorm = 0.0003\n",
      "Iter  7300 | Loss = 0.462131 | GradNorm = 0.0003\n",
      "Iter  7400 | Loss = 0.462127 | GradNorm = 0.0003\n",
      "Iter  7500 | Loss = 0.462124 | GradNorm = 0.0003\n",
      "Iter  7600 | Loss = 0.462120 | GradNorm = 0.0003\n",
      "Iter  7700 | Loss = 0.462117 | GradNorm = 0.0003\n",
      "Iter  7800 | Loss = 0.462113 | GradNorm = 0.0003\n",
      "Iter  7900 | Loss = 0.462110 | GradNorm = 0.0003\n",
      "Iter  8000 | Loss = 0.462107 | GradNorm = 0.0003\n",
      "Iter  8100 | Loss = 0.462104 | GradNorm = 0.0003\n",
      "Iter  8200 | Loss = 0.462101 | GradNorm = 0.0002\n",
      "Iter  8300 | Loss = 0.462098 | GradNorm = 0.0002\n",
      "Iter  8400 | Loss = 0.462095 | GradNorm = 0.0002\n",
      "Iter  8500 | Loss = 0.462092 | GradNorm = 0.0002\n",
      "Iter  8600 | Loss = 0.462089 | GradNorm = 0.0002\n",
      "Iter  8700 | Loss = 0.462086 | GradNorm = 0.0002\n",
      "Iter  8800 | Loss = 0.462083 | GradNorm = 0.0002\n",
      "Iter  8900 | Loss = 0.462080 | GradNorm = 0.0002\n",
      "Iter  9000 | Loss = 0.462078 | GradNorm = 0.0002\n",
      "Iter  9100 | Loss = 0.462075 | GradNorm = 0.0002\n",
      "Iter  9200 | Loss = 0.462073 | GradNorm = 0.0002\n",
      "Iter  9300 | Loss = 0.462070 | GradNorm = 0.0002\n",
      "Iter  9400 | Loss = 0.462068 | GradNorm = 0.0002\n",
      "Iter  9500 | Loss = 0.462065 | GradNorm = 0.0002\n",
      "Iter  9600 | Loss = 0.462063 | GradNorm = 0.0002\n",
      "Iter  9700 | Loss = 0.462060 | GradNorm = 0.0002\n",
      "Iter  9800 | Loss = 0.462058 | GradNorm = 0.0002\n",
      "Iter  9900 | Loss = 0.462056 | GradNorm = 0.0002\n",
      "Final training loss: 0.4620536700275664\n"
     ]
    }
   ],
   "source": [
    "best_lambda = 1e-8\n",
    "best_pos_weight = 9.5\n",
    "best_neg_weight = 1.0\n",
    "best_threshold = 0.7203517587939698\n",
    "\n",
    "final_loss, w_final = logistic_regression_weighted_gd(\n",
    "    y_final, X_final,\n",
    "    lambda_=best_lambda,\n",
    "    gamma=0.5,\n",
    "    pos_weight=best_pos_weight,\n",
    "    neg_weight=best_neg_weight,\n",
    "    max_iter=10000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Final training loss:\", final_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f3d675b-4919-4047-8171-11cc226e3ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 87.47%\n",
      " F1 Score: 0.4270\n"
     ]
    }
   ],
   "source": [
    "acc, f1 = evaluate_model(y_te, x_te_final, w_final, best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "993ab2db-fe31-421f-80b0-177f3bc1b0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[40754  4120]\n",
      " [ 2048  2298]]\n",
      "TN=40754, FP=4120, FN=2048, TP=2298\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds, _ = predict_with_threshold(x_te_final, w_final, threshold=best_threshold)\n",
    "cm = confusion_matrix_numpy(y_te, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1c0b086-adb4-4729-8ceb-e1292cb08410",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final, _ = predict_with_threshold(x_test_final, w_final, best_threshold)\n",
    "y_pred_final = 2 * y_pred_final - 1   # converts 0→-1, 1→1\n",
    "\n",
    "create_csv_submission(test_ids, y_pred_final, \"ModelOHE3_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f303c970-65f7-45a4-aafa-3ba6fc7143f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
