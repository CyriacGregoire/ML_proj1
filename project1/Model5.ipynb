{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b595fcc-e721-491b-b57f-06ade4c6a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from Data_cleaning import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c2e90b-c8a9-46a4-b700-8d98cd92d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data\\dataset\\dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9eefd61-d894-40fb-a65e-689f8923fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2, keep_mask = remove_nan_features(x_train, 0.4)\n",
    "X_test = x_test[:, keep_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8654b7-421b-4c3f-a9a2-c78ee5a033cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 163) (109379, 321)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_2.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f069a3bb-001f-4d64-8860-f9c26575925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_ = (y_train + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d228da3a-90f1-4f4d-98fc-57023d32426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_three_way_split(X, y, val_ratio=0.15, test_ratio=0.15, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    idx_pos = np.where(y == 1)[0]\n",
    "    idx_neg = np.where(y == 0)[0]\n",
    "    np.random.shuffle(idx_pos)\n",
    "    np.random.shuffle(idx_neg)\n",
    "\n",
    "    n_pos, n_neg = len(idx_pos), len(idx_neg)\n",
    "    n_val_pos = int(n_pos * val_ratio)\n",
    "    n_test_pos = int(n_pos * test_ratio)\n",
    "    n_val_neg = int(n_neg * val_ratio)\n",
    "    n_test_neg = int(n_neg * test_ratio)\n",
    "\n",
    "    val_idx = np.concatenate([idx_pos[:n_val_pos], idx_neg[:n_val_neg]])\n",
    "    test_idx = np.concatenate([\n",
    "        idx_pos[n_val_pos:n_val_pos + n_test_pos],\n",
    "        idx_neg[n_val_neg:n_val_neg + n_test_neg]\n",
    "    ])\n",
    "    train_idx = np.concatenate([\n",
    "        idx_pos[n_val_pos + n_test_pos:],\n",
    "        idx_neg[n_val_neg + n_test_neg:]\n",
    "    ])\n",
    "\n",
    "    np.random.shuffle(train_idx)\n",
    "    np.random.shuffle(val_idx)\n",
    "    np.random.shuffle(test_idx)\n",
    "\n",
    "    return (\n",
    "        X[train_idx], y[train_idx],\n",
    "        X[val_idx], y[val_idx],\n",
    "        X[test_idx], y[test_idx]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adb3faf0-fae2-4cab-a317-cf5ddb4a3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_va, y_va, x_te, y_te = stratified_three_way_split(x_train_2, y_tr_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aefacb86-2b04-47ee-8ed1-e08ae7b7372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 163) (229695, 163) (229695,) (49220, 163) (49220,) (49220, 163) (49220,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_2.shape, x_tr.shape, y_tr.shape, x_va.shape, y_va.shape, x_te.shape, y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d292db8-c256-4547-9b52-0281d38c30e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_split(y_tr, y_val, y_te, name_train=\"Train\", name_val=\"Validation\", name_test=\"Test\"):\n",
    "    \"\"\"\n",
    "    Checks that a stratified 3-way split preserved class proportions and sample counts.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Sanity Check: Stratified Split ===\")\n",
    "    n_total = len(y_tr) + len(y_val) + len(y_te)\n",
    "    print(f\"Total samples: {n_total:,}\")\n",
    "\n",
    "    def report(y, name):\n",
    "        frac1 = np.mean(y)\n",
    "        n = len(y)\n",
    "        print(f\"{name:<12}: n={n:<8} | positives={frac1:.4f} | negatives={1-frac1:.4f}\")\n",
    "\n",
    "    report(y_tr, name_train)\n",
    "    report(y_val, name_val)\n",
    "    report(y_te, name_test)\n",
    "\n",
    "    # quick checks\n",
    "    assert set(np.unique(y_tr)) <= {0, 1}, \"❌ Train labels not binary\"\n",
    "    assert set(np.unique(y_val)) <= {0, 1}, \"❌ Val labels not binary\"\n",
    "    assert set(np.unique(y_te)) <= {0, 1}, \"❌ Test labels not binary\"\n",
    "\n",
    "    fracs = np.array([np.mean(y_tr), np.mean(y_val), np.mean(y_te)])\n",
    "    diff = np.max(fracs) - np.min(fracs)\n",
    "    if diff < 0.01:\n",
    "        print(f\"✅ Class balance preserved (max diff = {diff:.4f})\")\n",
    "    else:\n",
    "        print(f\"⚠️ Class ratio differs across splits (max diff = {diff:.4f})\")\n",
    "\n",
    "    print(\"=====================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43b7cc11-9722-421b-b286-52805208b91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sanity Check: Stratified Split ===\n",
      "Total samples: 328,135\n",
      "Train       : n=229695   | positives=0.0883 | negatives=0.9117\n",
      "Validation  : n=49220    | positives=0.0883 | negatives=0.9117\n",
      "Test        : n=49220    | positives=0.0883 | negatives=0.9117\n",
      "✅ Class balance preserved (max diff = 0.0000)\n",
      "=====================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanity_check_split(y_tr, y_va, y_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d184a51e-bceb-45c6-b3a5-2e6b3c11333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_integer_and_categorical_features(X, unique_threshold=10, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Detects which features in X are integer-like and which are categorical\n",
    "    (integer-like with few unique values).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        2D array of shape (n_samples, n_features).\n",
    "    unique_threshold : int\n",
    "        Maximum number of unique values (excluding NaNs) to consider a feature categorical.\n",
    "    tol : float\n",
    "        Numerical tolerance for detecting integer-like values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int_count : int\n",
    "        Number of integer-like features.\n",
    "    cat_count : int\n",
    "        Number of categorical features (integer-like with ≤ unique_threshold values).\n",
    "    int_mask : np.ndarray (bool)\n",
    "        Mask for integer-like features (True = integer-like).\n",
    "    cat_mask : np.ndarray (bool)\n",
    "        Mask for categorical features (True = categorical).\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    int_mask = np.zeros(n_features, dtype=bool)\n",
    "    cat_mask = np.zeros(n_features, dtype=bool)\n",
    "\n",
    "    for j in range(n_features):\n",
    "        col = X[:, j]\n",
    "        col_nonan = col[~np.isnan(col)]\n",
    "        if len(col_nonan) == 0:\n",
    "            continue\n",
    "\n",
    "        # Check if column is integer-like\n",
    "        if np.all(np.abs(col_nonan - np.round(col_nonan)) < tol):\n",
    "            int_mask[j] = True\n",
    "            # If also low-cardinality, mark as categorical\n",
    "            unique_vals = np.unique(col_nonan)\n",
    "            if len(unique_vals) <= unique_threshold:\n",
    "                cat_mask[j] = True\n",
    "\n",
    "    int_count = np.sum(int_mask)\n",
    "    cat_count = np.sum(cat_mask)\n",
    "    return int_count, cat_count, int_mask, cat_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d44209-4c77-488e-8a46-9bd2a8806332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 102\n"
     ]
    }
   ],
   "source": [
    "int_count, cat_count, int_mask, cat_mask = detect_integer_and_categorical_features(x_train_2, tol = 1e-12)\n",
    "print(int_count, cat_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96879132-3b27-40af-b1fe-cbf2f8734343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(X, cat_mask, reference_stats=None, numeric_strategy=\"median\"):\n",
    "    \"\"\"\n",
    "    Impute NaNs:\n",
    "      - numerical (cat_mask=False): median or mean (choose via numeric_strategy)\n",
    "      - categorical (cat_mask=True): mode (most frequent)\n",
    "\n",
    "    If reference_stats is provided (from training), they are used directly.\n",
    "    Otherwise, stats are computed from X and returned for reuse.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (n_samples, n_features)\n",
    "    cat_mask : np.ndarray of bool, shape (n_features,)\n",
    "    reference_stats : list/np.ndarray or None\n",
    "        Per-column fill values computed on the training set.\n",
    "    numeric_strategy : {\"median\",\"mean\"}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_imp : np.ndarray\n",
    "    stats : list of length n_features (per-column fill values)\n",
    "    \"\"\"\n",
    "    X_imp = X.copy()\n",
    "    n_features = X_imp.shape[1]\n",
    "    if cat_mask.shape[0] != n_features:\n",
    "        raise ValueError(\"cat_mask length must match number of columns in X.\")\n",
    "\n",
    "    if reference_stats is None:\n",
    "        stats = [None] * n_features\n",
    "        for j in range(n_features):\n",
    "            col = X_imp[:, j]\n",
    "            missing = np.isnan(col)\n",
    "            if np.all(missing):\n",
    "                # Degenerate case: all missing. Choose a safe default.\n",
    "                # For categorical, use 0; for numeric, use 0.0\n",
    "                fill = 0.0 if not cat_mask[j] else 0.0\n",
    "            else:\n",
    "                if cat_mask[j]:\n",
    "                    # mode\n",
    "                    vals, counts = np.unique(col[~missing], return_counts=True)\n",
    "                    fill = vals[np.argmax(counts)]\n",
    "                else:\n",
    "                    if numeric_strategy == \"mean\":\n",
    "                        fill = np.nanmean(col)\n",
    "                    else:  # default median\n",
    "                        fill = np.nanmedian(col)\n",
    "            if np.any(missing):\n",
    "                X_imp[missing, j] = fill\n",
    "            stats[j] = float(fill)\n",
    "        return X_imp, stats\n",
    "    else:\n",
    "        # Use provided stats; must match n_features\n",
    "        if len(reference_stats) != n_features:\n",
    "            raise ValueError(\"reference_stats length does not match number of columns in X.\")\n",
    "        for j in range(n_features):\n",
    "            fill = reference_stats[j]\n",
    "            missing = np.isnan(X_imp[:, j])\n",
    "            if np.any(missing):\n",
    "                X_imp[missing, j] = fill\n",
    "        return X_imp, list(reference_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebe626e8-fbc1-4666-a200-1f8d25f30c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training\n",
    "X_tr_imp, impute_stats = impute_missing_values(x_tr, cat_mask, numeric_strategy=\"median\")\n",
    "\n",
    "# Apply to val/test with the same stats\n",
    "X_val_imp, _ = impute_missing_values(x_va, cat_mask, reference_stats=impute_stats)\n",
    "X_te_imp,  _ = impute_missing_values(x_te,  cat_mask, reference_stats=impute_stats)\n",
    "X_test_imp, _ = impute_missing_values(X_test, cat_mask, reference_stats=impute_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4489ed74-5e2b-439a-bae6-46382c715e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Imputation Check ===\n",
      "Train shape: (229695, 163)\n",
      "Val shape:   (49220, 163)\n",
      "Test shape:  (49220, 163)\n",
      "NaNs remaining in train: 0\n",
      "NaNs remaining in val:   0\n",
      "NaNs remaining in test:  0\n",
      "\n",
      "Example fill values:\n",
      "  Feature 0: Numeric, fill value = 29.0\n",
      "  Feature 1: Numeric, fill value = 6.0\n",
      "  Feature 2: Numeric, fill value = 6242015.0\n",
      "  Feature 3: Numeric, fill value = 6.0\n",
      "  Feature 4: Numeric, fill value = 14.0\n",
      "\n",
      "✅ Imputation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ✅ Check that imputation worked correctly\n",
    "print(\"=== Imputation Check ===\")\n",
    "print(f\"Train shape: {X_tr_imp.shape}\")\n",
    "print(f\"Val shape:   {X_val_imp.shape}\")\n",
    "print(f\"Test shape:  {X_te_imp.shape}\")\n",
    "\n",
    "# Check for remaining NaNs\n",
    "print(f\"NaNs remaining in train: {np.isnan(X_tr_imp).sum()}\")\n",
    "print(f\"NaNs remaining in val:   {np.isnan(X_val_imp).sum()}\")\n",
    "print(f\"NaNs remaining in test:  {np.isnan(X_te_imp).sum()}\")\n",
    "\n",
    "# Check some sample statistics\n",
    "print(\"\\nExample fill values:\")\n",
    "for j in range(min(5, len(impute_stats))):\n",
    "    kind = \"Categorical\" if cat_mask[j] else \"Numeric\"\n",
    "    print(f\"  Feature {j}: {kind}, fill value = {impute_stats[j]}\")\n",
    "\n",
    "print(\"\\n✅ Imputation completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5126cc3-3b63-45bd-bc4c-b515184da4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_low_variance_or_correlation(X, y, cat_mask, min_var=1e-8, min_corr=0.01):\n",
    "    \"\"\"\n",
    "    Drops columns with low variance or low correlation with the target.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features)\n",
    "    y : np.ndarray\n",
    "        Target vector (n_samples,)\n",
    "    cat_mask : np.ndarray of bool\n",
    "        Mask indicating which features are categorical\n",
    "    min_var : float\n",
    "        Minimum variance to keep a feature\n",
    "    min_corr : float\n",
    "        Minimum absolute correlation with y to keep a feature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_filtered : np.ndarray\n",
    "        Matrix with uninformative features removed\n",
    "    keep_mask : np.ndarray of bool\n",
    "        Boolean mask of kept features\n",
    "    dropped_info : dict\n",
    "        Information about how many features were dropped\n",
    "    cat_mask_new : np.ndarray of bool\n",
    "        Updated categorical mask after filtering\n",
    "    num_mask_new : np.ndarray of bool\n",
    "        Updated numerical mask after filtering\n",
    "    \"\"\"\n",
    "    X_copy = X.copy()\n",
    "\n",
    "    # 1️⃣ Variance filter\n",
    "    var = np.var(X_copy, axis=0)\n",
    "    var_mask = var > min_var\n",
    "\n",
    "    # 2️⃣ Correlation filter\n",
    "    corrs = np.zeros(X_copy.shape[1])\n",
    "    for j in range(X_copy.shape[1]):\n",
    "        col = X_copy[:, j]\n",
    "        if np.std(col) < 1e-12:\n",
    "            corrs[j] = 0\n",
    "        else:\n",
    "            corr = np.corrcoef(col, y)[0, 1]\n",
    "            corrs[j] = 0 if np.isnan(corr) else abs(corr)\n",
    "    corr_mask = corrs > min_corr\n",
    "\n",
    "    # 3️⃣ Combine filters\n",
    "    keep_mask = var_mask & corr_mask\n",
    "    X_filtered = X_copy[:, keep_mask]\n",
    "\n",
    "    # 4️⃣ Update masks\n",
    "    cat_mask_new = cat_mask[keep_mask]\n",
    "    num_mask_new = ~cat_mask_new\n",
    "\n",
    "    # 5️⃣ Info summary\n",
    "    dropped_info = {\n",
    "        \"total_features\": X.shape[1],\n",
    "        \"kept_features\": int(np.sum(keep_mask)),\n",
    "        \"dropped_low_variance\": int(np.sum(~var_mask)),\n",
    "        \"dropped_low_correlation\": int(np.sum(~corr_mask & var_mask))\n",
    "    }\n",
    "\n",
    "    return X_filtered, keep_mask, dropped_info, cat_mask_new, num_mask_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcb0fbb2-c403-4726-a221-17269511b481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Selection Summary ===\n",
      "Total features before: 163\n",
      "Kept features:         122\n",
      "Dropped (low variance): 0\n",
      "Dropped (low corr):     41\n"
     ]
    }
   ],
   "source": [
    "X_tr_filt, keep_mask, info, cat_mask, num_mask = drop_low_variance_or_correlation(\n",
    "    X_tr_imp, y_tr, cat_mask\n",
    ")\n",
    "\n",
    "print(\"=== Feature Selection Summary ===\")\n",
    "print(f\"Total features before: {info['total_features']}\")\n",
    "print(f\"Kept features:         {info['kept_features']}\")\n",
    "print(f\"Dropped (low variance): {info['dropped_low_variance']}\")\n",
    "print(f\"Dropped (low corr):     {info['dropped_low_correlation']}\")\n",
    "\n",
    "X_val_filt = X_val_imp[:, keep_mask]\n",
    "X_te_filt  = X_te_imp[:, keep_mask]\n",
    "X_test_filt = X_test_imp[:,keep_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5949e936-24b2-4573-80fb-76e89ef28728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def standardize_numeric_features(X, num_mask, reference_stats=None):\n",
    "    \"\"\"\n",
    "    Standardize only numerical features (mean=0, std=1),\n",
    "    leaving categorical ones unchanged.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features)\n",
    "    num_mask : np.ndarray of bool\n",
    "        Mask of numeric features (True = numeric)\n",
    "    reference_stats : dict or None\n",
    "        If None, compute mean/std from X (training set).\n",
    "        If provided, apply them (validation/test sets).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_scaled : np.ndarray\n",
    "        Matrix with standardized numeric features.\n",
    "    stats : dict\n",
    "        {\"mean\": mean_vector, \"std\": std_vector} for reuse on val/test.\n",
    "    \"\"\"\n",
    "    X_scaled = X.astype(float).copy()\n",
    "\n",
    "    # Safety check\n",
    "    if num_mask.shape[0] != X.shape[1]:\n",
    "        raise ValueError(\"num_mask length must match number of columns in X.\")\n",
    "\n",
    "    # --- Compute or apply scaling ---\n",
    "    if reference_stats is None:\n",
    "        mean = np.mean(X[:, num_mask], axis=0)\n",
    "        std = np.std(X[:, num_mask], axis=0)\n",
    "        std[std == 0] = 1.0  # avoid division by zero\n",
    "        X_scaled[:, num_mask] = (X[:, num_mask] - mean) / std\n",
    "        stats = {\"mean\": mean, \"std\": std}\n",
    "    else:\n",
    "        mean = reference_stats[\"mean\"]\n",
    "        std = reference_stats[\"std\"]\n",
    "        X_scaled[:, num_mask] = (X[:, num_mask] - mean) / std\n",
    "        stats = reference_stats\n",
    "\n",
    "    return X_scaled, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c26369b3-e883-4c1b-918e-570129809656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Fit on training data\n",
    "X_tr_scaled, scale_stats = standardize_numeric_features(X_tr_filt, num_mask)\n",
    "\n",
    "# 2️⃣ Apply to validation/test sets (no leakage)\n",
    "X_va_scaled, _ = standardize_numeric_features(X_val_filt, num_mask, scale_stats)\n",
    "X_te_scaled, _  = standardize_numeric_features(X_te_filt,  num_mask, scale_stats)\n",
    "X_test_scaled, _ = standardize_numeric_features(X_test_filt,  num_mask, scale_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70cf9ba5-2965-474e-acfd-acc7716b7592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of numeric features (train): [ 1.42668456e-16  1.50707534e-11  1.50707534e-11 -2.21488756e-17\n",
      "  8.27798760e-17]\n",
      "Std  of numeric features (train): [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of numeric features (train):\", np.mean(X_tr_scaled[:, num_mask], axis=0)[:5])\n",
    "print(\"Std  of numeric features (train):\", np.std(X_tr_scaled[:, num_mask], axis=0)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "098b2fda-a8ab-47be-8666-36c24564a2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numeric features: 46\n",
      "Number of categorical features: 76\n",
      "num_mask shape: (122,)\n",
      "X_tr_filt shape: (229695, 122)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of numeric features:\", np.sum(num_mask))\n",
    "print(\"Number of categorical features:\", np.sum(~num_mask))\n",
    "print(\"num_mask shape:\", num_mask.shape)\n",
    "print(\"X_tr_filt shape:\", X_tr_filt.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "165ad697-b119-4b30-b880-8aab7a632048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_all_categories(X, cat_mask, reference_uniques=None):\n",
    "    \"\"\"\n",
    "    One-hot encodes *all* categorical features (pure binary 0/1),\n",
    "    leaving numeric features untouched.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Data matrix.\n",
    "    cat_mask : np.ndarray of bool\n",
    "        Mask where True marks categorical features.\n",
    "    reference_uniques : list or None\n",
    "        If provided, use these unique values (from training set)\n",
    "        to ensure consistent encoding across val/test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_encoded : np.ndarray\n",
    "        Encoded matrix with only numeric + one-hot binary columns.\n",
    "    uniques_list : list\n",
    "        List of unique categories per categorical feature (for reuse).\n",
    "    \"\"\"\n",
    "\n",
    "    n, d = X.shape\n",
    "    X_parts = []\n",
    "    uniques_list = []\n",
    "\n",
    "    for j in range(d):\n",
    "        col = X[:, j]\n",
    "        if cat_mask[j]:\n",
    "            # Use provided unique values (for val/test) or compute from X\n",
    "            if reference_uniques is None:\n",
    "                uniques = np.unique(col[~np.isnan(col)])  # ignore NaNs\n",
    "            else:\n",
    "                uniques = reference_uniques[j]\n",
    "\n",
    "            uniques_list.append(uniques)\n",
    "\n",
    "            # One-hot encode (drop first category to avoid dummy trap)\n",
    "            one_hot = np.zeros((n, len(uniques) - 1))\n",
    "            for i, u in enumerate(uniques[1:]):\n",
    "                one_hot[:, i] = (col == u).astype(float)\n",
    "\n",
    "            X_parts.append(one_hot)\n",
    "\n",
    "        else:\n",
    "            # Numeric feature → keep as is\n",
    "            X_parts.append(col.reshape(-1, 1))\n",
    "            uniques_list.append(None)\n",
    "\n",
    "    X_encoded = np.concatenate(X_parts, axis=1)\n",
    "    return X_encoded, uniques_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99e472eb-b3cd-4729-a0b3-27338f706e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On training set\n",
    "X_tr_encoded, uniques_list = one_hot_encode_all_categories(X_tr_scaled, cat_mask)\n",
    "\n",
    "# On validation and test (same category mapping)\n",
    "X_va_encoded, _ = one_hot_encode_all_categories(X_va_scaled, cat_mask, uniques_list)\n",
    "X_te_encoded, _ = one_hot_encode_all_categories(X_te_scaled, cat_mask, uniques_list)\n",
    "X_test_encoded, _ = one_hot_encode_all_categories(X_test_scaled, cat_mask, uniques_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aaef55e-9d19-4167-acfd-773c63591c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== One-Hot Encoding Sanity Check ===\n",
      "Train shape: (229695, 303)\n",
      "Val shape:   (49220, 303)\n",
      "Test shape:  (49220, 303)\n",
      "NaNs in train: 0\n",
      "NaNs in val:   0\n",
      "NaNs in test:  0\n",
      "\n",
      "Features one-hot encoded: 57\n",
      "New columns added: 181\n",
      "\n",
      "Data type of X_tr_encoded: float64\n",
      "\n",
      "✅ One-hot encoding looks good!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== One-Hot Encoding Sanity Check ===\")\n",
    "\n",
    "# 1️⃣ Shapes\n",
    "print(f\"Train shape: {X_tr_encoded.shape}\")\n",
    "print(f\"Val shape:   {X_va_encoded.shape}\")\n",
    "print(f\"Test shape:  {X_te_encoded.shape}\")\n",
    "\n",
    "# 2️⃣ Column consistency\n",
    "assert X_tr_encoded.shape[1] == X_va_encoded.shape[1] == X_te_encoded.shape[1], \\\n",
    "    \"❌ Mismatch in feature counts between splits!\"\n",
    "\n",
    "# 3️⃣ Check for NaNs\n",
    "print(f\"NaNs in train: {np.isnan(X_tr_encoded).sum()}\")\n",
    "print(f\"NaNs in val:   {np.isnan(X_va_encoded).sum()}\")\n",
    "print(f\"NaNs in test:  {np.isnan(X_te_encoded).sum()}\")\n",
    "\n",
    "# 4️⃣ How many categorical features were one-hot encoded\n",
    "encoded_features = sum(\n",
    "    (uniques is not None and 3 <= len(uniques) <= 5)\n",
    "    for uniques in uniques_list if uniques is not None\n",
    ")\n",
    "total_added = X_tr_encoded.shape[1] - X_tr_scaled.shape[1]\n",
    "\n",
    "print(f\"\\nFeatures one-hot encoded: {encoded_features}\")\n",
    "print(f\"New columns added: {total_added}\")\n",
    "\n",
    "# 5️⃣ Quick data type check\n",
    "print(f\"\\nData type of X_tr_encoded: {X_tr_encoded.dtype}\")\n",
    "\n",
    "print(\"\\n✅ One-hot encoding looks good!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd717d7d-2fec-46a1-90b2-c7209786d9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded train shape: (229695, 303)\n",
      "Min/Max categorical: -7.575977237441043 203.53083771562615\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded train shape:\", X_tr_encoded.shape)\n",
    "print(\"Min/Max categorical:\", np.min(X_tr_encoded), np.max(X_tr_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "933bfb76-af51-4bd2-973f-6fb24dfbaf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_encoded_masks(num_mask, cat_mask, uniques_list):\n",
    "    \"\"\"\n",
    "    After one-hot encoding, rebuilds masks for the encoded dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_mask : np.ndarray of bool\n",
    "        Original numeric mask (before encoding)\n",
    "    cat_mask : np.ndarray of bool\n",
    "        Original categorical mask (before encoding)\n",
    "    uniques_list : list\n",
    "        List of unique categories per original feature\n",
    "        (from the one-hot encoder)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    num_mask_encoded, cat_mask_encoded : np.ndarray of bool\n",
    "        Boolean masks aligned with encoded data shape\n",
    "    \"\"\"\n",
    "\n",
    "    num_mask_encoded = []\n",
    "    cat_mask_encoded = []\n",
    "\n",
    "    for is_num, is_cat, uniques in zip(num_mask, cat_mask, uniques_list):\n",
    "        if is_num:\n",
    "            # numeric column -> stays numeric\n",
    "            num_mask_encoded.append(True)\n",
    "            cat_mask_encoded.append(False)\n",
    "\n",
    "        elif is_cat:\n",
    "            if uniques is None:\n",
    "                # categorical feature but no encoding? -> error check\n",
    "                raise ValueError(\"Categorical feature without uniques_list entry\")\n",
    "            else:\n",
    "                # one-hot created (len(uniques)-1) binary columns\n",
    "                n_new = len(uniques) - 1\n",
    "                num_mask_encoded.extend([False] * n_new)\n",
    "                cat_mask_encoded.extend([True] * n_new)\n",
    "\n",
    "    return np.array(num_mask_encoded), np.array(cat_mask_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2239fc7-3d39-476f-be46-26a7a8d0350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded masks: (303,) (303,)\n",
      "Numeric mean: 6.55285915048977e-13 std: 1.0\n",
      "Categorical min: 0.0 max: 1.0\n"
     ]
    }
   ],
   "source": [
    "num_mask_encoded, cat_mask_encoded = rebuild_encoded_masks(num_mask, cat_mask, uniques_list)\n",
    "print(\"Encoded masks:\", num_mask_encoded.shape, cat_mask_encoded.shape)\n",
    "num_vals = X_tr_encoded[:, num_mask_encoded]\n",
    "cat_vals = X_tr_encoded[:, cat_mask_encoded]\n",
    "\n",
    "print(\"Numeric mean:\", np.mean(num_vals), \"std:\", np.std(num_vals))\n",
    "print(\"Categorical min:\", np.min(cat_vals), \"max:\", np.max(cat_vals))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "315df0ba-a571-4d40-810d-04b87fcf6ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15121922693160866 0.5293081841950563\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(X_tr_encoded), np.std(X_tr_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d06aeea-4935-4fb8-8cb6-0a97b489f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Numerically stable sigmoid.\"\"\"\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def logistic_loss(y, tx, w):\n",
    "    \"\"\"Compute standard (unpenalized) logistic loss.\"\"\"\n",
    "    pred = sigmoid(tx @ w)\n",
    "    eps = 1e-15  # to avoid log(0)\n",
    "    loss = -np.mean(y * np.log(pred + eps) + (1 - y) * np.log(1 - pred + eps))\n",
    "    return float(loss)\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def weighted_logistic_loss(y, tx, w, lambda_=0.0, pos_weight=1.0, neg_weight=1.0):\n",
    "    \"\"\"\n",
    "    Weighted (and optionally penalized) logistic loss.\n",
    "    Class weights allow balancing for imbalanced datasets.\n",
    "    \"\"\"\n",
    "    p = sigmoid(tx @ w)\n",
    "    eps = 1e-15  # avoid log(0)\n",
    "\n",
    "    # weights per sample\n",
    "    sample_weights = np.where(y == 1, pos_weight, neg_weight)\n",
    "\n",
    "    # weighted average loss\n",
    "    loss = -np.sum(sample_weights * (y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))) / np.sum(sample_weights)\n",
    "    \n",
    "    # no regularization term added to returned loss (for monitoring only)\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "def weighted_gradient_logistic(y, tx, w, lambda_=0.0, pos_weight=1.0, neg_weight=1.0):\n",
    "    \"\"\"\n",
    "    Gradient of the weighted logistic loss with L2 penalty.\n",
    "    \"\"\"\n",
    "    p = sigmoid(tx @ w)\n",
    "    sample_weights = np.where(y == 1, pos_weight, neg_weight)\n",
    "    error = sample_weights * (p - y)\n",
    "    grad = (tx.T @ error) / np.sum(sample_weights)\n",
    "    grad[1:] += 2 * lambda_ * w[1:]  # don't regularize bias\n",
    "    return grad.ravel()\n",
    "\n",
    "\n",
    "\n",
    "def compute_gradient_logistic(y, tx, w, lambda_=0.0):\n",
    "    \"\"\"Compute penalized gradient of logistic loss.\"\"\"\n",
    "    pred = sigmoid(tx @ w)\n",
    "    error = pred - y\n",
    "    grad = (tx.T @ error) / len(y)\n",
    "    grad[1:] += 2 * lambda_ * w[1:]  # don't regularize bias\n",
    "    return grad.ravel()\n",
    "\n",
    "\n",
    "def logistic_regression_penalized(\n",
    "    y, x, lambda_=1e-3, gamma=0.05, max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Logistic regression with L2 penalization in the gradient step only.\n",
    "    Returns (loss, w), where:\n",
    "      - loss = final *unpenalized* logistic loss (float)\n",
    "      - w = final weights (1D np.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    # Add bias column\n",
    "    tx = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    w = np.zeros(tx.shape[1])  # 1D weights\n",
    "    losses = []\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Compute gradient and loss\n",
    "        grad = compute_gradient_logistic(y, tx, w, lambda_)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        if grad_norm > clip_grad:\n",
    "            grad *= clip_grad / grad_norm  # stability\n",
    "\n",
    "        loss = logistic_loss(y, tx, w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Update weights\n",
    "        w -= gamma * grad\n",
    "\n",
    "        # Convergence check\n",
    "        if it > 0 and abs(losses[-1] - losses[-2]) < tol:\n",
    "            if verbose:\n",
    "                print(f\"✅ Converged at iteration {it}\")\n",
    "            break\n",
    "\n",
    "        if verbose and it % 100 == 0:\n",
    "            print(f\"Iter {it:5d} | Loss = {loss:.6f} | GradNorm = {grad_norm:.4f}\")\n",
    "\n",
    "    # Return *last* unpenalized loss and final weights\n",
    "    return losses[-1], w\n",
    "\n",
    "def logistic_regression_weighted_gd(\n",
    "    y, x, lambda_=1e-3, gamma=0.05, pos_weight=1.0, neg_weight=1.0,\n",
    "    max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Logistic regression with class weights and L2 regularization.\n",
    "    Returns (loss, w).\n",
    "    \"\"\"\n",
    "    tx = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    losses = []\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        grad = weighted_gradient_logistic(y, tx, w, lambda_, pos_weight, neg_weight)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        if grad_norm > clip_grad:\n",
    "            grad *= clip_grad / grad_norm\n",
    "\n",
    "        loss = weighted_logistic_loss(y, tx, w, lambda_, pos_weight, neg_weight)\n",
    "        losses.append(loss)\n",
    "\n",
    "        w -= gamma * grad\n",
    "\n",
    "        if it > 0 and abs(losses[-1] - losses[-2]) < tol:\n",
    "            if verbose:\n",
    "                print(f\"✅ Converged at iteration {it}\")\n",
    "            break\n",
    "\n",
    "        if verbose and it % 100 == 0:\n",
    "            print(f\"Iter {it:5d} | Loss = {loss:.6f} | GradNorm = {grad_norm:.4f}\")\n",
    "\n",
    "    return losses[-1], w\n",
    "\n",
    "def accuracy_numpy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy using NumPy.\n",
    "    \"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "\n",
    "# --- Full evaluation wrapper ---\n",
    "def evaluate_model(y_true, X, w, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate trained logistic regression on a dataset.\n",
    "    Returns accuracy and F1 score.\n",
    "    \"\"\"\n",
    "    preds, probs = predict_with_threshold(X, w, threshold=threshold)\n",
    "    acc = accuracy_numpy(y_true, preds)\n",
    "    f1 = f1_score(y_true, preds)\n",
    "    print(f\"✅ Accuracy: {acc*100:.2f}%\")\n",
    "    print(f\"✅ F1 Score: {f1:.4f}\")\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03808747-c2eb-4db3-9b4e-a816f6d72fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_threshold(x, w, threshold=0.5):\n",
    "    tx = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    probs = sigmoid(tx @ w)\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    return preds, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3752962-cd84-4f7a-adab-e857ea203dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute F1 score using only NumPy.\n",
    "    Works for binary classification (0/1).\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-15)\n",
    "    recall = tp / (tp + fn + 1e-15)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-15)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc215321-270b-4349-870f-979863f74571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_grid_search(\n",
    "    y_train, X_train,\n",
    "    y_val, X_val,\n",
    "    pos_weights=[1, 3, 5, 9],\n",
    "    lambdas=[1e-5, 1e-3, 1e-2, 1e-1],\n",
    "    thresholds=[0.3, 0.5, 0.7],\n",
    "    max_iter=10000,\n",
    "    gamma=0.05\n",
    "):\n",
    "    \"\"\"\n",
    "    Safe grid search for weighted penalized logistic regression.\n",
    "    Returns: best_params, best_f1, results_list\n",
    "    \"\"\"\n",
    "    best_f1 = -1\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    total = len(pos_weights) * len(lambdas) * len(thresholds)\n",
    "    run = 0\n",
    "\n",
    "    for pw in pos_weights:\n",
    "        for lam in lambdas:\n",
    "            run += 1\n",
    "            print(f\"\\n=== Run {run}/{total//len(thresholds)} (pos_weight={pw}, lambda_={lam}) ===\")\n",
    "\n",
    "            try:\n",
    "                # Train model\n",
    "                loss, w = logistic_regression_weighted_gd(\n",
    "                    y_train, X_train,\n",
    "                    lambda_=lam,\n",
    "                    gamma=gamma,\n",
    "                    pos_weight=pw,\n",
    "                    neg_weight=1.0,\n",
    "                    max_iter=max_iter,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                # Skip invalid runs\n",
    "                if np.isnan(loss) or np.isinf(loss) or loss > 10:\n",
    "                    print(f\"⚠️  Invalid loss ({loss:.4f}), skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Evaluate all thresholds for this model\n",
    "                for th in thresholds:\n",
    "                    preds, _ = predict_with_threshold(X_val, w, threshold=th)\n",
    "                    f1 = f1_score(y_val, preds)\n",
    "                    results.append((pw, lam, th, f1))\n",
    "\n",
    "                    #print(f\"   → threshold={th:.2f} | F1={f1:.4f}\")\n",
    "\n",
    "                    # Update best model\n",
    "                    if f1 > best_f1:\n",
    "                        best_f1 = f1\n",
    "                        best_params = (pw, lam, th)\n",
    "                        print(f\"   ✅ New best F1 = {best_f1:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error for pos_weight={pw}, lambda_={lam}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Sort results by F1 descending\n",
    "    results.sort(key=lambda t: t[3], reverse=True)\n",
    "\n",
    "    print(\"\\n=== 🏁 Grid Search Complete ===\")\n",
    "    if best_params:\n",
    "        print(f\"🏆 Best F1 = {best_f1:.4f} at pos_weight={best_params[0]}, λ={best_params[1]}, threshold={best_params[2]}\")\n",
    "    else:\n",
    "        print(\"⚠️ No valid runs completed.\")\n",
    "\n",
    "    return best_params, best_f1, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e1962d4-c091-4964-9296-e968371bc144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/12 (pos_weight=1, lambda_=0.001) ===\n",
      "   → threshold=0.65 | F1=0.0778\n",
      "   ✅ New best F1 = 0.0778\n",
      "   → threshold=0.70 | F1=0.0472\n",
      "   → threshold=0.75 | F1=0.0231\n",
      "   → threshold=0.80 | F1=0.0091\n",
      "\n",
      "=== Run 2/12 (pos_weight=1, lambda_=0.01) ===\n",
      "   → threshold=0.65 | F1=0.0087\n",
      "   → threshold=0.70 | F1=0.0037\n",
      "   → threshold=0.75 | F1=0.0009\n",
      "   → threshold=0.80 | F1=0.0000\n",
      "\n",
      "=== Run 3/12 (pos_weight=1, lambda_=0.1) ===\n",
      "   → threshold=0.65 | F1=0.0000\n",
      "   → threshold=0.70 | F1=0.0000\n",
      "   → threshold=0.75 | F1=0.0000\n",
      "   → threshold=0.80 | F1=0.0000\n",
      "\n",
      "=== Run 4/12 (pos_weight=3, lambda_=0.001) ===\n",
      "   → threshold=0.65 | F1=0.3395\n",
      "   ✅ New best F1 = 0.3395\n",
      "   → threshold=0.70 | F1=0.2879\n",
      "   → threshold=0.75 | F1=0.2220\n",
      "   → threshold=0.80 | F1=0.1521\n",
      "\n",
      "=== Run 5/12 (pos_weight=3, lambda_=0.01) ===\n",
      "   → threshold=0.65 | F1=0.2689\n",
      "   → threshold=0.70 | F1=0.1983\n",
      "   → threshold=0.75 | F1=0.1274\n",
      "   → threshold=0.80 | F1=0.0631\n",
      "\n",
      "=== Run 6/12 (pos_weight=3, lambda_=0.1) ===\n",
      "   → threshold=0.65 | F1=0.0159\n",
      "   → threshold=0.70 | F1=0.0018\n",
      "   → threshold=0.75 | F1=0.0005\n",
      "   → threshold=0.80 | F1=0.0000\n",
      "\n",
      "=== Run 7/12 (pos_weight=5, lambda_=0.001) ===\n",
      "   → threshold=0.65 | F1=0.4082\n",
      "   ✅ New best F1 = 0.4082\n",
      "   → threshold=0.70 | F1=0.3865\n",
      "   → threshold=0.75 | F1=0.3525\n",
      "   → threshold=0.80 | F1=0.2862\n",
      "\n",
      "=== Run 8/12 (pos_weight=5, lambda_=0.01) ===\n",
      "   → threshold=0.65 | F1=0.3885\n",
      "   → threshold=0.70 | F1=0.3503\n",
      "   → threshold=0.75 | F1=0.2843\n",
      "   → threshold=0.80 | F1=0.2019\n",
      "\n",
      "=== Run 9/12 (pos_weight=5, lambda_=0.1) ===\n",
      "   → threshold=0.65 | F1=0.2077\n",
      "   → threshold=0.70 | F1=0.1132\n",
      "   → threshold=0.75 | F1=0.0340\n",
      "   → threshold=0.80 | F1=0.0032\n",
      "\n",
      "=== Run 10/12 (pos_weight=9, lambda_=0.001) ===\n",
      "   → threshold=0.65 | F1=0.4140\n",
      "   ✅ New best F1 = 0.4140\n",
      "   → threshold=0.70 | F1=0.4156\n",
      "   ✅ New best F1 = 0.4156\n",
      "   → threshold=0.75 | F1=0.4133\n",
      "   → threshold=0.80 | F1=0.3956\n",
      "\n",
      "=== Run 11/12 (pos_weight=9, lambda_=0.01) ===\n",
      "   → threshold=0.65 | F1=0.4103\n",
      "   → threshold=0.70 | F1=0.4087\n",
      "   → threshold=0.75 | F1=0.3974\n",
      "   → threshold=0.80 | F1=0.3620\n",
      "\n",
      "=== Run 12/12 (pos_weight=9, lambda_=0.1) ===\n",
      "   → threshold=0.65 | F1=0.3192\n",
      "   → threshold=0.70 | F1=0.3393\n",
      "   → threshold=0.75 | F1=0.3613\n",
      "   → threshold=0.80 | F1=0.3757\n",
      "\n",
      "=== 🏁 Grid Search Complete ===\n",
      "🏆 Best F1 = 0.4156 at pos_weight=9, λ=0.001, threshold=0.7\n"
     ]
    }
   ],
   "source": [
    "best_params, best_f1, results = safe_grid_search(\n",
    "    y_tr, X_tr_encoded,\n",
    "    y_va, X_va_encoded,\n",
    "    pos_weights=[1, 3, 5, 9],\n",
    "    lambdas=[1e-3, 1e-2, 1e-1],\n",
    "    thresholds=[0.65, 0.7, 0.75, 0.8], \n",
    "    gamma = 0.4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd023cd1-7683-46ad-bb26-8001db0faa2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "582b930d-26e6-4aef-af90-7face91a86ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     0 | Loss = 0.693147 | GradNorm = 0.5795\n",
      "Iter   100 | Loss = 0.503150 | GradNorm = 0.4945\n",
      "Iter   200 | Loss = 0.490821 | GradNorm = 0.4270\n",
      "Iter   300 | Loss = 0.485415 | GradNorm = 0.3915\n",
      "Iter   400 | Loss = 0.482506 | GradNorm = 0.3702\n",
      "Iter   500 | Loss = 0.480788 | GradNorm = 0.3569\n",
      "Iter   600 | Loss = 0.479711 | GradNorm = 0.3482\n",
      "Iter   700 | Loss = 0.479004 | GradNorm = 0.3425\n",
      "Iter   800 | Loss = 0.478523 | GradNorm = 0.3386\n",
      "Iter   900 | Loss = 0.478184 | GradNorm = 0.3358\n",
      "Iter  1000 | Loss = 0.477939 | GradNorm = 0.3338\n",
      "Iter  1100 | Loss = 0.477757 | GradNorm = 0.3324\n",
      "Iter  1200 | Loss = 0.477618 | GradNorm = 0.3313\n",
      "Iter  1300 | Loss = 0.477509 | GradNorm = 0.3304\n",
      "Iter  1400 | Loss = 0.477422 | GradNorm = 0.3298\n",
      "Iter  1500 | Loss = 0.477351 | GradNorm = 0.3292\n",
      "Iter  1600 | Loss = 0.477293 | GradNorm = 0.3288\n",
      "Iter  1700 | Loss = 0.477243 | GradNorm = 0.3284\n",
      "Iter  1800 | Loss = 0.477200 | GradNorm = 0.3280\n",
      "Iter  1900 | Loss = 0.477163 | GradNorm = 0.3277\n",
      "Iter  2000 | Loss = 0.477130 | GradNorm = 0.3275\n",
      "Iter  2100 | Loss = 0.477100 | GradNorm = 0.3272\n",
      "Iter  2200 | Loss = 0.477073 | GradNorm = 0.3270\n",
      "Iter  2300 | Loss = 0.477049 | GradNorm = 0.3268\n",
      "Iter  2400 | Loss = 0.477026 | GradNorm = 0.3266\n",
      "Iter  2500 | Loss = 0.477005 | GradNorm = 0.3264\n",
      "Iter  2600 | Loss = 0.476985 | GradNorm = 0.3263\n",
      "Iter  2700 | Loss = 0.476966 | GradNorm = 0.3261\n",
      "Iter  2800 | Loss = 0.476948 | GradNorm = 0.3259\n",
      "Iter  2900 | Loss = 0.476931 | GradNorm = 0.3258\n",
      "Iter  3000 | Loss = 0.476915 | GradNorm = 0.3256\n",
      "Iter  3100 | Loss = 0.476899 | GradNorm = 0.3255\n",
      "Iter  3200 | Loss = 0.476884 | GradNorm = 0.3253\n",
      "Iter  3300 | Loss = 0.476870 | GradNorm = 0.3252\n",
      "Iter  3400 | Loss = 0.476856 | GradNorm = 0.3251\n",
      "Iter  3500 | Loss = 0.476842 | GradNorm = 0.3249\n",
      "Iter  3600 | Loss = 0.476829 | GradNorm = 0.3248\n",
      "Iter  3700 | Loss = 0.476816 | GradNorm = 0.3247\n",
      "Iter  3800 | Loss = 0.476803 | GradNorm = 0.3245\n",
      "Iter  3900 | Loss = 0.476791 | GradNorm = 0.3244\n",
      "Iter  4000 | Loss = 0.476779 | GradNorm = 0.3243\n",
      "Iter  4100 | Loss = 0.476767 | GradNorm = 0.3242\n",
      "Iter  4200 | Loss = 0.476755 | GradNorm = 0.3240\n",
      "Iter  4300 | Loss = 0.476744 | GradNorm = 0.3239\n",
      "Iter  4400 | Loss = 0.476733 | GradNorm = 0.3238\n",
      "Iter  4500 | Loss = 0.476722 | GradNorm = 0.3237\n",
      "Iter  4600 | Loss = 0.476711 | GradNorm = 0.3236\n",
      "Iter  4700 | Loss = 0.476700 | GradNorm = 0.3235\n",
      "Iter  4800 | Loss = 0.476690 | GradNorm = 0.3234\n",
      "Iter  4900 | Loss = 0.476680 | GradNorm = 0.3233\n",
      "Iter  5000 | Loss = 0.476670 | GradNorm = 0.3231\n",
      "Iter  5100 | Loss = 0.476660 | GradNorm = 0.3230\n",
      "Iter  5200 | Loss = 0.476650 | GradNorm = 0.3229\n",
      "Iter  5300 | Loss = 0.476641 | GradNorm = 0.3228\n",
      "Iter  5400 | Loss = 0.476632 | GradNorm = 0.3227\n",
      "Iter  5500 | Loss = 0.476622 | GradNorm = 0.3226\n",
      "Iter  5600 | Loss = 0.476613 | GradNorm = 0.3225\n",
      "Iter  5700 | Loss = 0.476605 | GradNorm = 0.3224\n",
      "Iter  5800 | Loss = 0.476596 | GradNorm = 0.3223\n",
      "Iter  5900 | Loss = 0.476587 | GradNorm = 0.3222\n",
      "Iter  6000 | Loss = 0.476579 | GradNorm = 0.3221\n",
      "Iter  6100 | Loss = 0.476570 | GradNorm = 0.3221\n",
      "Iter  6200 | Loss = 0.476562 | GradNorm = 0.3220\n",
      "Iter  6300 | Loss = 0.476554 | GradNorm = 0.3219\n",
      "Iter  6400 | Loss = 0.476546 | GradNorm = 0.3218\n",
      "Iter  6500 | Loss = 0.476538 | GradNorm = 0.3217\n",
      "Iter  6600 | Loss = 0.476530 | GradNorm = 0.3216\n",
      "Iter  6700 | Loss = 0.476523 | GradNorm = 0.3215\n",
      "Iter  6800 | Loss = 0.476515 | GradNorm = 0.3214\n",
      "Iter  6900 | Loss = 0.476508 | GradNorm = 0.3214\n",
      "Iter  7000 | Loss = 0.476501 | GradNorm = 0.3213\n",
      "Iter  7100 | Loss = 0.476494 | GradNorm = 0.3212\n",
      "Iter  7200 | Loss = 0.476487 | GradNorm = 0.3211\n",
      "Iter  7300 | Loss = 0.476480 | GradNorm = 0.3210\n",
      "Iter  7400 | Loss = 0.476473 | GradNorm = 0.3210\n",
      "Iter  7500 | Loss = 0.476466 | GradNorm = 0.3209\n",
      "Iter  7600 | Loss = 0.476460 | GradNorm = 0.3208\n",
      "Iter  7700 | Loss = 0.476453 | GradNorm = 0.3207\n",
      "Iter  7800 | Loss = 0.476447 | GradNorm = 0.3207\n",
      "Iter  7900 | Loss = 0.476440 | GradNorm = 0.3206\n",
      "Iter  8000 | Loss = 0.476434 | GradNorm = 0.3205\n",
      "Iter  8100 | Loss = 0.476428 | GradNorm = 0.3204\n",
      "Iter  8200 | Loss = 0.476422 | GradNorm = 0.3204\n",
      "Iter  8300 | Loss = 0.476416 | GradNorm = 0.3203\n",
      "Iter  8400 | Loss = 0.476410 | GradNorm = 0.3202\n",
      "Iter  8500 | Loss = 0.476404 | GradNorm = 0.3202\n",
      "Iter  8600 | Loss = 0.476399 | GradNorm = 0.3201\n",
      "Iter  8700 | Loss = 0.476393 | GradNorm = 0.3200\n",
      "Iter  8800 | Loss = 0.476388 | GradNorm = 0.3200\n",
      "Iter  8900 | Loss = 0.476382 | GradNorm = 0.3199\n",
      "Iter  9000 | Loss = 0.476377 | GradNorm = 0.3199\n",
      "Iter  9100 | Loss = 0.476372 | GradNorm = 0.3198\n",
      "Iter  9200 | Loss = 0.476367 | GradNorm = 0.3197\n",
      "Iter  9300 | Loss = 0.476361 | GradNorm = 0.3197\n",
      "Iter  9400 | Loss = 0.476356 | GradNorm = 0.3196\n",
      "Iter  9500 | Loss = 0.476351 | GradNorm = 0.3196\n",
      "Iter  9600 | Loss = 0.476347 | GradNorm = 0.3195\n",
      "Iter  9700 | Loss = 0.476342 | GradNorm = 0.3194\n",
      "Iter  9800 | Loss = 0.476337 | GradNorm = 0.3194\n",
      "Iter  9900 | Loss = 0.476333 | GradNorm = 0.3193\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation data\n",
    "x_tr_final = np.vstack((X_tr_encoded, X_va_encoded))\n",
    "y_tr_final = np.hstack((y_tr, y_va))\n",
    "loss, w = logistic_regression_weighted_gd(\n",
    "    y_tr_final, x_tr_final, lambda_=1e-3, gamma=0.5, pos_weight=9.0, neg_weight=1.0,\n",
    "    max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68b5ff7e-e83a-410e-a9dc-357371903c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304,)\n"
     ]
    }
   ],
   "source": [
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7057ad4e-6bcf-4407-ad4f-69ba97e5bcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 85.84%\n",
      "✅ F1 Score: 0.4241\n"
     ]
    }
   ],
   "source": [
    "# Suppose you've already trained your model\n",
    "# loss, w = logistic_regression_weighted_gd(...)\n",
    "\n",
    "# Evaluate on test or validation data\n",
    "acc, f1 = evaluate_model(y_te, X_te_encoded, w, 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5425fe3f-6923-42ad-a984-70414ee1193d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109379, 303)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b85a7864-13a0-448a-873d-bcc94358ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/4 (pos_weight=7, lambda_=1e-07) ===\n",
      "   ✅ New best F1 = 0.4209\n",
      "   ✅ New best F1 = 0.4209\n",
      "   ✅ New best F1 = 0.4214\n",
      "\n",
      "=== Run 2/4 (pos_weight=8, lambda_=1e-07) ===\n",
      "   ✅ New best F1 = 0.4215\n",
      "\n",
      "=== Run 3/4 (pos_weight=9, lambda_=1e-07) ===\n",
      "   ✅ New best F1 = 0.4217\n",
      "\n",
      "=== Run 4/4 (pos_weight=10, lambda_=1e-07) ===\n",
      "\n",
      "=== 🏁 Grid Search Complete ===\n",
      "🏆 Best F1 = 0.4217 at pos_weight=9, λ=1e-07, threshold=0.7217948717948718\n"
     ]
    }
   ],
   "source": [
    "best_params, best_f1, results = safe_grid_search(\n",
    "    y_tr, X_tr_encoded,\n",
    "    y_va, X_va_encoded,\n",
    "    pos_weights=[7, 8, 9, 10],\n",
    "    lambdas = [1e-7],\n",
    "    thresholds=np.linspace(0.65, 0.85, 40), \n",
    "    gamma = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29f72136-3eba-4f8c-aa60-2f6cce9eb759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     0 | Loss = 0.693147 | GradNorm = 0.5795\n",
      "Iter   100 | Loss = 0.497567 | GradNorm = 0.4519\n",
      "Iter   200 | Loss = 0.483414 | GradNorm = 0.3599\n",
      "Iter   300 | Loss = 0.476542 | GradNorm = 0.2996\n",
      "Iter   400 | Loss = 0.472451 | GradNorm = 0.2542\n",
      "Iter   500 | Loss = 0.469780 | GradNorm = 0.2183\n",
      "Iter   600 | Loss = 0.467930 | GradNorm = 0.1888\n",
      "Iter   700 | Loss = 0.466596 | GradNorm = 0.1640\n",
      "Iter   800 | Loss = 0.465603 | GradNorm = 0.1426\n",
      "Iter   900 | Loss = 0.464846 | GradNorm = 0.1239\n",
      "Iter  1000 | Loss = 0.464259 | GradNorm = 0.1073\n",
      "Iter  1100 | Loss = 0.463799 | GradNorm = 0.0923\n",
      "Iter  1200 | Loss = 0.463434 | GradNorm = 0.0787\n",
      "Iter  1300 | Loss = 0.463146 | GradNorm = 0.0662\n",
      "Iter  1400 | Loss = 0.462920 | GradNorm = 0.0548\n",
      "Iter  1500 | Loss = 0.462745 | GradNorm = 0.0445\n",
      "Iter  1600 | Loss = 0.462610 | GradNorm = 0.0352\n",
      "Iter  1700 | Loss = 0.462510 | GradNorm = 0.0272\n",
      "Iter  1800 | Loss = 0.462435 | GradNorm = 0.0204\n",
      "Iter  1900 | Loss = 0.462379 | GradNorm = 0.0148\n",
      "Iter  2000 | Loss = 0.462337 | GradNorm = 0.0104\n",
      "Iter  2100 | Loss = 0.462304 | GradNorm = 0.0071\n",
      "Iter  2200 | Loss = 0.462276 | GradNorm = 0.0047\n",
      "Iter  2300 | Loss = 0.462252 | GradNorm = 0.0031\n",
      "Iter  2400 | Loss = 0.462231 | GradNorm = 0.0020\n",
      "Iter  2500 | Loss = 0.462211 | GradNorm = 0.0013\n",
      "Iter  2600 | Loss = 0.462193 | GradNorm = 0.0009\n",
      "Iter  2700 | Loss = 0.462176 | GradNorm = 0.0007\n",
      "Iter  2800 | Loss = 0.462160 | GradNorm = 0.0006\n",
      "Iter  2900 | Loss = 0.462145 | GradNorm = 0.0006\n",
      "Iter  3000 | Loss = 0.462130 | GradNorm = 0.0005\n",
      "Iter  3100 | Loss = 0.462117 | GradNorm = 0.0005\n",
      "Iter  3200 | Loss = 0.462104 | GradNorm = 0.0005\n",
      "Iter  3300 | Loss = 0.462092 | GradNorm = 0.0005\n",
      "Iter  3400 | Loss = 0.462081 | GradNorm = 0.0005\n",
      "Iter  3500 | Loss = 0.462070 | GradNorm = 0.0005\n",
      "Iter  3600 | Loss = 0.462060 | GradNorm = 0.0004\n",
      "Iter  3700 | Loss = 0.462050 | GradNorm = 0.0004\n",
      "Iter  3800 | Loss = 0.462040 | GradNorm = 0.0004\n",
      "Iter  3900 | Loss = 0.462031 | GradNorm = 0.0004\n",
      "Iter  4000 | Loss = 0.462023 | GradNorm = 0.0004\n",
      "Iter  4100 | Loss = 0.462015 | GradNorm = 0.0004\n",
      "Iter  4200 | Loss = 0.462007 | GradNorm = 0.0004\n",
      "Iter  4300 | Loss = 0.461999 | GradNorm = 0.0004\n",
      "Iter  4400 | Loss = 0.461992 | GradNorm = 0.0004\n",
      "Iter  4500 | Loss = 0.461985 | GradNorm = 0.0004\n",
      "Iter  4600 | Loss = 0.461978 | GradNorm = 0.0004\n",
      "Iter  4700 | Loss = 0.461971 | GradNorm = 0.0004\n",
      "Iter  4800 | Loss = 0.461965 | GradNorm = 0.0004\n",
      "Iter  4900 | Loss = 0.461959 | GradNorm = 0.0003\n",
      "Iter  5000 | Loss = 0.461953 | GradNorm = 0.0003\n",
      "Iter  5100 | Loss = 0.461948 | GradNorm = 0.0003\n",
      "Iter  5200 | Loss = 0.461942 | GradNorm = 0.0003\n",
      "Iter  5300 | Loss = 0.461937 | GradNorm = 0.0003\n",
      "Iter  5400 | Loss = 0.461932 | GradNorm = 0.0003\n",
      "Iter  5500 | Loss = 0.461926 | GradNorm = 0.0003\n",
      "Iter  5600 | Loss = 0.461922 | GradNorm = 0.0003\n",
      "Iter  5700 | Loss = 0.461917 | GradNorm = 0.0003\n",
      "Iter  5800 | Loss = 0.461912 | GradNorm = 0.0003\n",
      "Iter  5900 | Loss = 0.461908 | GradNorm = 0.0003\n",
      "Iter  6000 | Loss = 0.461903 | GradNorm = 0.0003\n",
      "Iter  6100 | Loss = 0.461899 | GradNorm = 0.0003\n",
      "Iter  6200 | Loss = 0.461895 | GradNorm = 0.0003\n",
      "Iter  6300 | Loss = 0.461891 | GradNorm = 0.0003\n",
      "Iter  6400 | Loss = 0.461887 | GradNorm = 0.0003\n",
      "Iter  6500 | Loss = 0.461883 | GradNorm = 0.0003\n",
      "Iter  6600 | Loss = 0.461879 | GradNorm = 0.0003\n",
      "Iter  6700 | Loss = 0.461876 | GradNorm = 0.0003\n",
      "Iter  6800 | Loss = 0.461872 | GradNorm = 0.0003\n",
      "Iter  6900 | Loss = 0.461869 | GradNorm = 0.0003\n",
      "Iter  7000 | Loss = 0.461865 | GradNorm = 0.0003\n",
      "Iter  7100 | Loss = 0.461862 | GradNorm = 0.0003\n",
      "Iter  7200 | Loss = 0.461858 | GradNorm = 0.0003\n",
      "Iter  7300 | Loss = 0.461855 | GradNorm = 0.0003\n",
      "Iter  7400 | Loss = 0.461852 | GradNorm = 0.0003\n",
      "Iter  7500 | Loss = 0.461849 | GradNorm = 0.0002\n",
      "Iter  7600 | Loss = 0.461846 | GradNorm = 0.0002\n",
      "Iter  7700 | Loss = 0.461843 | GradNorm = 0.0002\n",
      "Iter  7800 | Loss = 0.461840 | GradNorm = 0.0002\n",
      "Iter  7900 | Loss = 0.461837 | GradNorm = 0.0002\n",
      "Iter  8000 | Loss = 0.461834 | GradNorm = 0.0002\n",
      "Iter  8100 | Loss = 0.461831 | GradNorm = 0.0002\n",
      "Iter  8200 | Loss = 0.461828 | GradNorm = 0.0002\n",
      "Iter  8300 | Loss = 0.461826 | GradNorm = 0.0002\n",
      "Iter  8400 | Loss = 0.461823 | GradNorm = 0.0002\n",
      "Iter  8500 | Loss = 0.461820 | GradNorm = 0.0002\n",
      "Iter  8600 | Loss = 0.461818 | GradNorm = 0.0002\n",
      "Iter  8700 | Loss = 0.461815 | GradNorm = 0.0002\n",
      "Iter  8800 | Loss = 0.461813 | GradNorm = 0.0002\n",
      "Iter  8900 | Loss = 0.461810 | GradNorm = 0.0002\n",
      "Iter  9000 | Loss = 0.461808 | GradNorm = 0.0002\n",
      "Iter  9100 | Loss = 0.461806 | GradNorm = 0.0002\n",
      "Iter  9200 | Loss = 0.461803 | GradNorm = 0.0002\n",
      "Iter  9300 | Loss = 0.461801 | GradNorm = 0.0002\n",
      "Iter  9400 | Loss = 0.461799 | GradNorm = 0.0002\n",
      "Iter  9500 | Loss = 0.461796 | GradNorm = 0.0002\n",
      "Iter  9600 | Loss = 0.461794 | GradNorm = 0.0002\n",
      "Iter  9700 | Loss = 0.461792 | GradNorm = 0.0002\n",
      "Iter  9800 | Loss = 0.461790 | GradNorm = 0.0002\n",
      "Iter  9900 | Loss = 0.461788 | GradNorm = 0.0002\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation data\n",
    "x_tr_final = np.vstack((X_tr_encoded, X_va_encoded))\n",
    "y_tr_final = np.hstack((y_tr, y_va))\n",
    "loss, w = logistic_regression_weighted_gd(\n",
    "    y_tr_final, x_tr_final, lambda_=1e-7, gamma=0.5, pos_weight=9.0, neg_weight=1.0,\n",
    "    max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07a8c603-db24-4bdc-85d5-9a5433a2d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 86.44%\n",
      "✅ F1 Score: 0.4264\n"
     ]
    }
   ],
   "source": [
    "# Suppose you've already trained your model\n",
    "# loss, w = logistic_regression_weighted_gd(...)\n",
    "\n",
    "# Evaluate on test or validation data\n",
    "acc, f1 = evaluate_model(y_te, X_te_encoded, w, 0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "84798187-a7a6-409a-a073-811710ce68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final, _ = predict_with_threshold(X_test_encoded, w, 0.77)\n",
    "y_pred_final = 2 * y_pred_final - 1   # converts 0→-1, 1→1\n",
    "\n",
    "create_csv_submission(test_ids, y_pred_final, \"Model5_77preds.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace8aa2-481f-4e51-96df-57063614a725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
