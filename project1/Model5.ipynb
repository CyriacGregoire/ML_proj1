{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b595fcc-e721-491b-b57f-06ade4c6a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from Data_cleaning import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c2e90b-c8a9-46a4-b700-8d98cd92d0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "<>:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "C:\\Users\\janfo\\AppData\\Local\\Temp\\ipykernel_18908\\4219252773.py:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "  x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data\\dataset\\dataset\")\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data\\dataset\\dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9eefd61-d894-40fb-a65e-689f8923fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2, keep_mask = remove_nan_features(x_train, 0.4)\n",
    "X_test = x_test[:, keep_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8654b7-421b-4c3f-a9a2-c78ee5a033cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 163) (109379, 321)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_2.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f069a3bb-001f-4d64-8860-f9c26575925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_ = (y_train + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d228da3a-90f1-4f4d-98fc-57023d32426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_three_way_split(X, y, val_ratio=0.15, test_ratio=0.15, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    idx_pos = np.where(y == 1)[0]\n",
    "    idx_neg = np.where(y == 0)[0]\n",
    "    np.random.shuffle(idx_pos)\n",
    "    np.random.shuffle(idx_neg)\n",
    "\n",
    "    n_pos, n_neg = len(idx_pos), len(idx_neg)\n",
    "    n_val_pos = int(n_pos * val_ratio)\n",
    "    n_test_pos = int(n_pos * test_ratio)\n",
    "    n_val_neg = int(n_neg * val_ratio)\n",
    "    n_test_neg = int(n_neg * test_ratio)\n",
    "\n",
    "    val_idx = np.concatenate([idx_pos[:n_val_pos], idx_neg[:n_val_neg]])\n",
    "    test_idx = np.concatenate([\n",
    "        idx_pos[n_val_pos:n_val_pos + n_test_pos],\n",
    "        idx_neg[n_val_neg:n_val_neg + n_test_neg]\n",
    "    ])\n",
    "    train_idx = np.concatenate([\n",
    "        idx_pos[n_val_pos + n_test_pos:],\n",
    "        idx_neg[n_val_neg + n_test_neg:]\n",
    "    ])\n",
    "\n",
    "    np.random.shuffle(train_idx)\n",
    "    np.random.shuffle(val_idx)\n",
    "    np.random.shuffle(test_idx)\n",
    "\n",
    "    return (\n",
    "        X[train_idx], y[train_idx],\n",
    "        X[val_idx], y[val_idx],\n",
    "        X[test_idx], y[test_idx]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb3faf0-fae2-4cab-a317-cf5ddb4a3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_va, y_va, x_te, y_te = stratified_three_way_split(x_train_2, y_tr_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aefacb86-2b04-47ee-8ed1-e08ae7b7372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 163) (229695, 163) (229695,) (49220, 163) (49220,) (49220, 163) (49220,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_2.shape, x_tr.shape, y_tr.shape, x_va.shape, y_va.shape, x_te.shape, y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d292db8-c256-4547-9b52-0281d38c30e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_split(y_tr, y_val, y_te, name_train=\"Train\", name_val=\"Validation\", name_test=\"Test\"):\n",
    "    \"\"\"\n",
    "    Checks that a stratified 3-way split preserved class proportions and sample counts.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Sanity Check: Stratified Split ===\")\n",
    "    n_total = len(y_tr) + len(y_val) + len(y_te)\n",
    "    print(f\"Total samples: {n_total:,}\")\n",
    "\n",
    "    def report(y, name):\n",
    "        frac1 = np.mean(y)\n",
    "        n = len(y)\n",
    "        print(f\"{name:<12}: n={n:<8} | positives={frac1:.4f} | negatives={1-frac1:.4f}\")\n",
    "\n",
    "    report(y_tr, name_train)\n",
    "    report(y_val, name_val)\n",
    "    report(y_te, name_test)\n",
    "\n",
    "    # quick checks\n",
    "    assert set(np.unique(y_tr)) <= {0, 1}, \"❌ Train labels not binary\"\n",
    "    assert set(np.unique(y_val)) <= {0, 1}, \"❌ Val labels not binary\"\n",
    "    assert set(np.unique(y_te)) <= {0, 1}, \"❌ Test labels not binary\"\n",
    "\n",
    "    fracs = np.array([np.mean(y_tr), np.mean(y_val), np.mean(y_te)])\n",
    "    diff = np.max(fracs) - np.min(fracs)\n",
    "    if diff < 0.01:\n",
    "        print(f\"✅ Class balance preserved (max diff = {diff:.4f})\")\n",
    "    else:\n",
    "        print(f\"⚠️ Class ratio differs across splits (max diff = {diff:.4f})\")\n",
    "\n",
    "    print(\"=====================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43b7cc11-9722-421b-b286-52805208b91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sanity Check: Stratified Split ===\n",
      "Total samples: 328,135\n",
      "Train       : n=229695   | positives=0.0883 | negatives=0.9117\n",
      "Validation  : n=49220    | positives=0.0883 | negatives=0.9117\n",
      "Test        : n=49220    | positives=0.0883 | negatives=0.9117\n",
      "✅ Class balance preserved (max diff = 0.0000)\n",
      "=====================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanity_check_split(y_tr, y_va, y_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d184a51e-bceb-45c6-b3a5-2e6b3c11333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_integer_and_categorical_features(X, unique_threshold=10, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Detects which features in X are integer-like and which are categorical\n",
    "    (integer-like with few unique values).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        2D array of shape (n_samples, n_features).\n",
    "    unique_threshold : int\n",
    "        Maximum number of unique values (excluding NaNs) to consider a feature categorical.\n",
    "    tol : float\n",
    "        Numerical tolerance for detecting integer-like values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int_count : int\n",
    "        Number of integer-like features.\n",
    "    cat_count : int\n",
    "        Number of categorical features (integer-like with ≤ unique_threshold values).\n",
    "    int_mask : np.ndarray (bool)\n",
    "        Mask for integer-like features (True = integer-like).\n",
    "    cat_mask : np.ndarray (bool)\n",
    "        Mask for categorical features (True = categorical).\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    int_mask = np.zeros(n_features, dtype=bool)\n",
    "    cat_mask = np.zeros(n_features, dtype=bool)\n",
    "\n",
    "    for j in range(n_features):\n",
    "        col = X[:, j]\n",
    "        col_nonan = col[~np.isnan(col)]\n",
    "        if len(col_nonan) == 0:\n",
    "            continue\n",
    "\n",
    "        # Check if column is integer-like\n",
    "        if np.all(np.abs(col_nonan - np.round(col_nonan)) < tol):\n",
    "            int_mask[j] = True\n",
    "            # If also low-cardinality, mark as categorical\n",
    "            unique_vals = np.unique(col_nonan)\n",
    "            if len(unique_vals) <= unique_threshold:\n",
    "                cat_mask[j] = True\n",
    "\n",
    "    int_count = np.sum(int_mask)\n",
    "    cat_count = np.sum(cat_mask)\n",
    "    return int_count, cat_count, int_mask, cat_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d44209-4c77-488e-8a46-9bd2a8806332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 102\n"
     ]
    }
   ],
   "source": [
    "int_count, cat_count, int_mask, cat_mask = detect_integer_and_categorical_features(x_train_2, tol = 1e-12)\n",
    "print(int_count, cat_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96879132-3b27-40af-b1fe-cbf2f8734343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(X, cat_mask, reference_stats=None, numeric_strategy=\"median\"):\n",
    "    \"\"\"\n",
    "    Impute NaNs:\n",
    "      - numerical (cat_mask=False): median or mean (choose via numeric_strategy)\n",
    "      - categorical (cat_mask=True): mode (most frequent)\n",
    "\n",
    "    If reference_stats is provided (from training), they are used directly.\n",
    "    Otherwise, stats are computed from X and returned for reuse.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (n_samples, n_features)\n",
    "    cat_mask : np.ndarray of bool, shape (n_features,)\n",
    "    reference_stats : list/np.ndarray or None\n",
    "        Per-column fill values computed on the training set.\n",
    "    numeric_strategy : {\"median\",\"mean\"}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_imp : np.ndarray\n",
    "    stats : list of length n_features (per-column fill values)\n",
    "    \"\"\"\n",
    "    X_imp = X.copy()\n",
    "    n_features = X_imp.shape[1]\n",
    "    if cat_mask.shape[0] != n_features:\n",
    "        raise ValueError(\"cat_mask length must match number of columns in X.\")\n",
    "\n",
    "    if reference_stats is None:\n",
    "        stats = [None] * n_features\n",
    "        for j in range(n_features):\n",
    "            col = X_imp[:, j]\n",
    "            missing = np.isnan(col)\n",
    "            if np.all(missing):\n",
    "                # Degenerate case: all missing. Choose a safe default.\n",
    "                # For categorical, use 0; for numeric, use 0.0\n",
    "                fill = 0.0 if not cat_mask[j] else 0.0\n",
    "            else:\n",
    "                if cat_mask[j]:\n",
    "                    # mode\n",
    "                    vals, counts = np.unique(col[~missing], return_counts=True)\n",
    "                    fill = vals[np.argmax(counts)]\n",
    "                else:\n",
    "                    if numeric_strategy == \"mean\":\n",
    "                        fill = np.nanmean(col)\n",
    "                    else:  # default median\n",
    "                        fill = np.nanmedian(col)\n",
    "            if np.any(missing):\n",
    "                X_imp[missing, j] = fill\n",
    "            stats[j] = float(fill)\n",
    "        return X_imp, stats\n",
    "    else:\n",
    "        # Use provided stats; must match n_features\n",
    "        if len(reference_stats) != n_features:\n",
    "            raise ValueError(\"reference_stats length does not match number of columns in X.\")\n",
    "        for j in range(n_features):\n",
    "            fill = reference_stats[j]\n",
    "            missing = np.isnan(X_imp[:, j])\n",
    "            if np.any(missing):\n",
    "                X_imp[missing, j] = fill\n",
    "        return X_imp, list(reference_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebe626e8-fbc1-4666-a200-1f8d25f30c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training\n",
    "X_tr_imp, impute_stats = impute_missing_values(x_tr, cat_mask, numeric_strategy=\"median\")\n",
    "\n",
    "# Apply to val/test with the same stats\n",
    "X_val_imp, _ = impute_missing_values(x_va, cat_mask, reference_stats=impute_stats)\n",
    "X_te_imp,  _ = impute_missing_values(x_te,  cat_mask, reference_stats=impute_stats)\n",
    "X_test_imp, _ = impute_missing_values(X_test, cat_mask, reference_stats=impute_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4489ed74-5e2b-439a-bae6-46382c715e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Imputation Check ===\n",
      "Train shape: (229695, 163)\n",
      "Val shape:   (49220, 163)\n",
      "Test shape:  (49220, 163)\n",
      "NaNs remaining in train: 0\n",
      "NaNs remaining in val:   0\n",
      "NaNs remaining in test:  0\n",
      "\n",
      "Example fill values:\n",
      "  Feature 0: Numeric, fill value = 29.0\n",
      "  Feature 1: Numeric, fill value = 6.0\n",
      "  Feature 2: Numeric, fill value = 6242015.0\n",
      "  Feature 3: Numeric, fill value = 6.0\n",
      "  Feature 4: Numeric, fill value = 14.0\n",
      "\n",
      "✅ Imputation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ✅ Check that imputation worked correctly\n",
    "print(\"=== Imputation Check ===\")\n",
    "print(f\"Train shape: {X_tr_imp.shape}\")\n",
    "print(f\"Val shape:   {X_val_imp.shape}\")\n",
    "print(f\"Test shape:  {X_te_imp.shape}\")\n",
    "\n",
    "# Check for remaining NaNs\n",
    "print(f\"NaNs remaining in train: {np.isnan(X_tr_imp).sum()}\")\n",
    "print(f\"NaNs remaining in val:   {np.isnan(X_val_imp).sum()}\")\n",
    "print(f\"NaNs remaining in test:  {np.isnan(X_te_imp).sum()}\")\n",
    "\n",
    "# Check some sample statistics\n",
    "print(\"\\nExample fill values:\")\n",
    "for j in range(min(5, len(impute_stats))):\n",
    "    kind = \"Categorical\" if cat_mask[j] else \"Numeric\"\n",
    "    print(f\"  Feature {j}: {kind}, fill value = {impute_stats[j]}\")\n",
    "\n",
    "print(\"\\n✅ Imputation completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5126cc3-3b63-45bd-bc4c-b515184da4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_low_variance_or_correlation(X, y, cat_mask, min_var=1e-8, min_corr=0.01):\n",
    "    \"\"\"\n",
    "    Drops columns with low variance or low correlation with the target.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features)\n",
    "    y : np.ndarray\n",
    "        Target vector (n_samples,)\n",
    "    cat_mask : np.ndarray of bool\n",
    "        Mask indicating which features are categorical\n",
    "    min_var : float\n",
    "        Minimum variance to keep a feature\n",
    "    min_corr : float\n",
    "        Minimum absolute correlation with y to keep a feature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_filtered : np.ndarray\n",
    "        Matrix with uninformative features removed\n",
    "    keep_mask : np.ndarray of bool\n",
    "        Boolean mask of kept features\n",
    "    dropped_info : dict\n",
    "        Information about how many features were dropped\n",
    "    cat_mask_new : np.ndarray of bool\n",
    "        Updated categorical mask after filtering\n",
    "    num_mask_new : np.ndarray of bool\n",
    "        Updated numerical mask after filtering\n",
    "    \"\"\"\n",
    "    X_copy = X.copy()\n",
    "\n",
    "    # 1️⃣ Variance filter\n",
    "    var = np.var(X_copy, axis=0)\n",
    "    var_mask = var > min_var\n",
    "\n",
    "    # 2️⃣ Correlation filter\n",
    "    corrs = np.zeros(X_copy.shape[1])\n",
    "    for j in range(X_copy.shape[1]):\n",
    "        col = X_copy[:, j]\n",
    "        if np.std(col) < 1e-12:\n",
    "            corrs[j] = 0\n",
    "        else:\n",
    "            corr = np.corrcoef(col, y)[0, 1]\n",
    "            corrs[j] = 0 if np.isnan(corr) else abs(corr)\n",
    "    corr_mask = corrs > min_corr\n",
    "\n",
    "    # 3️⃣ Combine filters\n",
    "    keep_mask = var_mask & corr_mask\n",
    "    X_filtered = X_copy[:, keep_mask]\n",
    "\n",
    "    # 4️⃣ Update masks\n",
    "    cat_mask_new = cat_mask[keep_mask]\n",
    "    num_mask_new = ~cat_mask_new\n",
    "\n",
    "    # 5️⃣ Info summary\n",
    "    dropped_info = {\n",
    "        \"total_features\": X.shape[1],\n",
    "        \"kept_features\": int(np.sum(keep_mask)),\n",
    "        \"dropped_low_variance\": int(np.sum(~var_mask)),\n",
    "        \"dropped_low_correlation\": int(np.sum(~corr_mask & var_mask))\n",
    "    }\n",
    "\n",
    "    return X_filtered, keep_mask, dropped_info, cat_mask_new, num_mask_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcb0fbb2-c403-4726-a221-17269511b481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Selection Summary ===\n",
      "Total features before: 163\n",
      "Kept features:         122\n",
      "Dropped (low variance): 0\n",
      "Dropped (low corr):     41\n"
     ]
    }
   ],
   "source": [
    "X_tr_filt, keep_mask, info, cat_mask, num_mask = drop_low_variance_or_correlation(\n",
    "    X_tr_imp, y_tr, cat_mask\n",
    ")\n",
    "\n",
    "print(\"=== Feature Selection Summary ===\")\n",
    "print(f\"Total features before: {info['total_features']}\")\n",
    "print(f\"Kept features:         {info['kept_features']}\")\n",
    "print(f\"Dropped (low variance): {info['dropped_low_variance']}\")\n",
    "print(f\"Dropped (low corr):     {info['dropped_low_correlation']}\")\n",
    "\n",
    "X_val_filt = X_val_imp[:, keep_mask]\n",
    "X_te_filt  = X_te_imp[:, keep_mask]\n",
    "X_test_filt = X_test_imp[:,keep_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5949e936-24b2-4573-80fb-76e89ef28728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def standardize_numeric_features(X, num_mask, reference_stats=None):\n",
    "    \"\"\"\n",
    "    Standardize only numerical features (mean=0, std=1),\n",
    "    leaving categorical ones unchanged.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features)\n",
    "    num_mask : np.ndarray of bool\n",
    "        Mask of numeric features (True = numeric)\n",
    "    reference_stats : dict or None\n",
    "        If None, compute mean/std from X (training set).\n",
    "        If provided, apply them (validation/test sets).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_scaled : np.ndarray\n",
    "        Matrix with standardized numeric features.\n",
    "    stats : dict\n",
    "        {\"mean\": mean_vector, \"std\": std_vector} for reuse on val/test.\n",
    "    \"\"\"\n",
    "    X_scaled = X.astype(float).copy()\n",
    "\n",
    "    # Safety check\n",
    "    if num_mask.shape[0] != X.shape[1]:\n",
    "        raise ValueError(\"num_mask length must match number of columns in X.\")\n",
    "\n",
    "    # --- Compute or apply scaling ---\n",
    "    if reference_stats is None:\n",
    "        mean = np.mean(X[:, num_mask], axis=0)\n",
    "        std = np.std(X[:, num_mask], axis=0)\n",
    "        std[std == 0] = 1.0  # avoid division by zero\n",
    "        X_scaled[:, num_mask] = (X[:, num_mask] - mean) / std\n",
    "        stats = {\"mean\": mean, \"std\": std}\n",
    "    else:\n",
    "        mean = reference_stats[\"mean\"]\n",
    "        std = reference_stats[\"std\"]\n",
    "        X_scaled[:, num_mask] = (X[:, num_mask] - mean) / std\n",
    "        stats = reference_stats\n",
    "\n",
    "    return X_scaled, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c26369b3-e883-4c1b-918e-570129809656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Fit on training data\n",
    "X_tr_scaled, scale_stats = standardize_numeric_features(X_tr_filt, num_mask)\n",
    "\n",
    "# 2️⃣ Apply to validation/test sets (no leakage)\n",
    "X_va_scaled, _ = standardize_numeric_features(X_val_filt, num_mask, scale_stats)\n",
    "X_te_scaled, _  = standardize_numeric_features(X_te_filt,  num_mask, scale_stats)\n",
    "X_test_scaled, _ = standardize_numeric_features(X_test_filt,  num_mask, scale_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70cf9ba5-2965-474e-acfd-acc7716b7592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of numeric features (train): [ 1.42668456e-16  1.50707534e-11  1.50707534e-11 -2.21488756e-17\n",
      "  8.27798760e-17]\n",
      "Std  of numeric features (train): [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of numeric features (train):\", np.mean(X_tr_scaled[:, num_mask], axis=0)[:5])\n",
    "print(\"Std  of numeric features (train):\", np.std(X_tr_scaled[:, num_mask], axis=0)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "098b2fda-a8ab-47be-8666-36c24564a2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numeric features: 46\n",
      "Number of categorical features: 76\n",
      "num_mask shape: (122,)\n",
      "X_tr_filt shape: (229695, 122)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of numeric features:\", np.sum(num_mask))\n",
    "print(\"Number of categorical features:\", np.sum(~num_mask))\n",
    "print(\"num_mask shape:\", num_mask.shape)\n",
    "print(\"X_tr_filt shape:\", X_tr_filt.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "165ad697-b119-4b30-b880-8aab7a632048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_all_categories(X, cat_mask, reference_uniques=None):\n",
    "    \"\"\"\n",
    "    One-hot encodes *all* categorical features (pure binary 0/1),\n",
    "    leaving numeric features untouched.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Data matrix.\n",
    "    cat_mask : np.ndarray of bool\n",
    "        Mask where True marks categorical features.\n",
    "    reference_uniques : list or None\n",
    "        If provided, use these unique values (from training set)\n",
    "        to ensure consistent encoding across val/test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_encoded : np.ndarray\n",
    "        Encoded matrix with only numeric + one-hot binary columns.\n",
    "    uniques_list : list\n",
    "        List of unique categories per categorical feature (for reuse).\n",
    "    \"\"\"\n",
    "\n",
    "    n, d = X.shape\n",
    "    X_parts = []\n",
    "    uniques_list = []\n",
    "\n",
    "    for j in range(d):\n",
    "        col = X[:, j]\n",
    "        if cat_mask[j]:\n",
    "            # Use provided unique values (for val/test) or compute from X\n",
    "            if reference_uniques is None:\n",
    "                uniques = np.unique(col[~np.isnan(col)])  # ignore NaNs\n",
    "            else:\n",
    "                uniques = reference_uniques[j]\n",
    "\n",
    "            uniques_list.append(uniques)\n",
    "\n",
    "            # One-hot encode (drop first category to avoid dummy trap)\n",
    "            one_hot = np.zeros((n, len(uniques) - 1))\n",
    "            for i, u in enumerate(uniques[1:]):\n",
    "                one_hot[:, i] = (col == u).astype(float)\n",
    "\n",
    "            X_parts.append(one_hot)\n",
    "\n",
    "        else:\n",
    "            # Numeric feature → keep as is\n",
    "            X_parts.append(col.reshape(-1, 1))\n",
    "            uniques_list.append(None)\n",
    "\n",
    "    X_encoded = np.concatenate(X_parts, axis=1)\n",
    "    return X_encoded, uniques_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99e472eb-b3cd-4729-a0b3-27338f706e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On training set\n",
    "X_tr_encoded, uniques_list = one_hot_encode_all_categories(X_tr_scaled, cat_mask)\n",
    "\n",
    "# On validation and test (same category mapping)\n",
    "X_va_encoded, _ = one_hot_encode_all_categories(X_va_scaled, cat_mask, uniques_list)\n",
    "X_te_encoded, _ = one_hot_encode_all_categories(X_te_scaled, cat_mask, uniques_list)\n",
    "X_test_encoded, _ = one_hot_encode_all_categories(X_test_scaled, cat_mask, uniques_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aaef55e-9d19-4167-acfd-773c63591c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== One-Hot Encoding Sanity Check ===\n",
      "Train shape: (229695, 303)\n",
      "Val shape:   (49220, 303)\n",
      "Test shape:  (49220, 303)\n",
      "NaNs in train: 0\n",
      "NaNs in val:   0\n",
      "NaNs in test:  0\n",
      "\n",
      "Features one-hot encoded: 57\n",
      "New columns added: 181\n",
      "\n",
      "Data type of X_tr_encoded: float64\n",
      "\n",
      "✅ One-hot encoding looks good!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== One-Hot Encoding Sanity Check ===\")\n",
    "\n",
    "# 1️⃣ Shapes\n",
    "print(f\"Train shape: {X_tr_encoded.shape}\")\n",
    "print(f\"Val shape:   {X_va_encoded.shape}\")\n",
    "print(f\"Test shape:  {X_te_encoded.shape}\")\n",
    "\n",
    "# 2️⃣ Column consistency\n",
    "assert X_tr_encoded.shape[1] == X_va_encoded.shape[1] == X_te_encoded.shape[1], \\\n",
    "    \"❌ Mismatch in feature counts between splits!\"\n",
    "\n",
    "# 3️⃣ Check for NaNs\n",
    "print(f\"NaNs in train: {np.isnan(X_tr_encoded).sum()}\")\n",
    "print(f\"NaNs in val:   {np.isnan(X_va_encoded).sum()}\")\n",
    "print(f\"NaNs in test:  {np.isnan(X_te_encoded).sum()}\")\n",
    "\n",
    "# 4️⃣ How many categorical features were one-hot encoded\n",
    "encoded_features = sum(\n",
    "    (uniques is not None and 3 <= len(uniques) <= 5)\n",
    "    for uniques in uniques_list if uniques is not None\n",
    ")\n",
    "total_added = X_tr_encoded.shape[1] - X_tr_scaled.shape[1]\n",
    "\n",
    "print(f\"\\nFeatures one-hot encoded: {encoded_features}\")\n",
    "print(f\"New columns added: {total_added}\")\n",
    "\n",
    "# 5️⃣ Quick data type check\n",
    "print(f\"\\nData type of X_tr_encoded: {X_tr_encoded.dtype}\")\n",
    "\n",
    "print(\"\\n✅ One-hot encoding looks good!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd717d7d-2fec-46a1-90b2-c7209786d9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded train shape: (229695, 303)\n",
      "Min/Max categorical: -7.575977237441043 203.53083771562615\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded train shape:\", X_tr_encoded.shape)\n",
    "print(\"Min/Max categorical:\", np.min(X_tr_encoded), np.max(X_tr_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "933bfb76-af51-4bd2-973f-6fb24dfbaf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_encoded_masks(num_mask, cat_mask, uniques_list):\n",
    "    \"\"\"\n",
    "    After one-hot encoding, rebuilds masks for the encoded dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_mask : np.ndarray of bool\n",
    "        Original numeric mask (before encoding)\n",
    "    cat_mask : np.ndarray of bool\n",
    "        Original categorical mask (before encoding)\n",
    "    uniques_list : list\n",
    "        List of unique categories per original feature\n",
    "        (from the one-hot encoder)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    num_mask_encoded, cat_mask_encoded : np.ndarray of bool\n",
    "        Boolean masks aligned with encoded data shape\n",
    "    \"\"\"\n",
    "\n",
    "    num_mask_encoded = []\n",
    "    cat_mask_encoded = []\n",
    "\n",
    "    for is_num, is_cat, uniques in zip(num_mask, cat_mask, uniques_list):\n",
    "        if is_num:\n",
    "            # numeric column -> stays numeric\n",
    "            num_mask_encoded.append(True)\n",
    "            cat_mask_encoded.append(False)\n",
    "\n",
    "        elif is_cat:\n",
    "            if uniques is None:\n",
    "                # categorical feature but no encoding? -> error check\n",
    "                raise ValueError(\"Categorical feature without uniques_list entry\")\n",
    "            else:\n",
    "                # one-hot created (len(uniques)-1) binary columns\n",
    "                n_new = len(uniques) - 1\n",
    "                num_mask_encoded.extend([False] * n_new)\n",
    "                cat_mask_encoded.extend([True] * n_new)\n",
    "\n",
    "    return np.array(num_mask_encoded), np.array(cat_mask_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2239fc7-3d39-476f-be46-26a7a8d0350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded masks: (303,) (303,)\n",
      "Numeric mean: 6.55285915048977e-13 std: 1.0\n",
      "Categorical min: 0.0 max: 1.0\n"
     ]
    }
   ],
   "source": [
    "num_mask_encoded, cat_mask_encoded = rebuild_encoded_masks(num_mask, cat_mask, uniques_list)\n",
    "print(\"Encoded masks:\", num_mask_encoded.shape, cat_mask_encoded.shape)\n",
    "num_vals = X_tr_encoded[:, num_mask_encoded]\n",
    "cat_vals = X_tr_encoded[:, cat_mask_encoded]\n",
    "\n",
    "print(\"Numeric mean:\", np.mean(num_vals), \"std:\", np.std(num_vals))\n",
    "print(\"Categorical min:\", np.min(cat_vals), \"max:\", np.max(cat_vals))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "315df0ba-a571-4d40-810d-04b87fcf6ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15121922693160866 0.5293081841950563\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(X_tr_encoded), np.std(X_tr_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d06aeea-4935-4fb8-8cb6-0a97b489f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Numerically stable sigmoid.\"\"\"\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def logistic_loss(y, tx, w):\n",
    "    \"\"\"Compute standard (unpenalized) logistic loss.\"\"\"\n",
    "    pred = sigmoid(tx @ w)\n",
    "    eps = 1e-15  # to avoid log(0)\n",
    "    loss = -np.mean(y * np.log(pred + eps) + (1 - y) * np.log(1 - pred + eps))\n",
    "    return float(loss)\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def weighted_logistic_loss(y, tx, w, lambda_=0.0, pos_weight=1.0, neg_weight=1.0):\n",
    "    \"\"\"\n",
    "    Weighted (and optionally penalized) logistic loss.\n",
    "    Class weights allow balancing for imbalanced datasets.\n",
    "    \"\"\"\n",
    "    p = sigmoid(tx @ w)\n",
    "    eps = 1e-15  # avoid log(0)\n",
    "\n",
    "    # weights per sample\n",
    "    sample_weights = np.where(y == 1, pos_weight, neg_weight)\n",
    "\n",
    "    # weighted average loss\n",
    "    loss = -np.sum(sample_weights * (y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))) / np.sum(sample_weights)\n",
    "    \n",
    "    # no regularization term added to returned loss (for monitoring only)\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "def weighted_gradient_logistic(y, tx, w, lambda_=0.0, pos_weight=1.0, neg_weight=1.0):\n",
    "    \"\"\"\n",
    "    Gradient of the weighted logistic loss with L2 penalty.\n",
    "    \"\"\"\n",
    "    p = sigmoid(tx @ w)\n",
    "    sample_weights = np.where(y == 1, pos_weight, neg_weight)\n",
    "    error = sample_weights * (p - y)\n",
    "    grad = (tx.T @ error) / np.sum(sample_weights)\n",
    "    grad[1:] += 2 * lambda_ * w[1:]  # don't regularize bias\n",
    "    return grad.ravel()\n",
    "\n",
    "\n",
    "\n",
    "def compute_gradient_logistic(y, tx, w, lambda_=0.0):\n",
    "    \"\"\"Compute penalized gradient of logistic loss.\"\"\"\n",
    "    pred = sigmoid(tx @ w)\n",
    "    error = pred - y\n",
    "    grad = (tx.T @ error) / len(y)\n",
    "    grad[1:] += 2 * lambda_ * w[1:]  # don't regularize bias\n",
    "    return grad.ravel()\n",
    "\n",
    "\n",
    "def logistic_regression_penalized(\n",
    "    y, x, lambda_=1e-3, gamma=0.05, max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Logistic regression with L2 penalization in the gradient step only.\n",
    "    Returns (loss, w), where:\n",
    "      - loss = final *unpenalized* logistic loss (float)\n",
    "      - w = final weights (1D np.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    # Add bias column\n",
    "    tx = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    w = np.zeros(tx.shape[1])  # 1D weights\n",
    "    losses = []\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Compute gradient and loss\n",
    "        grad = compute_gradient_logistic(y, tx, w, lambda_)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        if grad_norm > clip_grad:\n",
    "            grad *= clip_grad / grad_norm  # stability\n",
    "\n",
    "        loss = logistic_loss(y, tx, w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Update weights\n",
    "        w -= gamma * grad\n",
    "\n",
    "        # Convergence check\n",
    "        if it > 0 and abs(losses[-1] - losses[-2]) < tol:\n",
    "            if verbose:\n",
    "                print(f\"✅ Converged at iteration {it}\")\n",
    "            break\n",
    "\n",
    "        if verbose and it % 100 == 0:\n",
    "            print(f\"Iter {it:5d} | Loss = {loss:.6f} | GradNorm = {grad_norm:.4f}\")\n",
    "\n",
    "    # Return *last* unpenalized loss and final weights\n",
    "    return losses[-1], w\n",
    "\n",
    "def logistic_regression_weighted_gd(\n",
    "    y, x, lambda_=1e-3, gamma=0.05, pos_weight=1.0, neg_weight=1.0,\n",
    "    max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Logistic regression with class weights and L2 regularization.\n",
    "    Returns (loss, w).\n",
    "    \"\"\"\n",
    "    tx = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    losses = []\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        grad = weighted_gradient_logistic(y, tx, w, lambda_, pos_weight, neg_weight)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        if grad_norm > clip_grad:\n",
    "            grad *= clip_grad / grad_norm\n",
    "\n",
    "        loss = weighted_logistic_loss(y, tx, w, lambda_, pos_weight, neg_weight)\n",
    "        losses.append(loss)\n",
    "\n",
    "        w -= gamma * grad\n",
    "\n",
    "        if it > 0 and abs(losses[-1] - losses[-2]) < tol:\n",
    "            if verbose:\n",
    "                print(f\"✅ Converged at iteration {it}\")\n",
    "            break\n",
    "\n",
    "        if verbose and it % 100 == 0:\n",
    "            print(f\"Iter {it:5d} | Loss = {loss:.6f} | GradNorm = {grad_norm:.4f}\")\n",
    "\n",
    "    return losses[-1], w\n",
    "\n",
    "def accuracy_numpy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy using NumPy.\n",
    "    \"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "\n",
    "# --- Full evaluation wrapper ---\n",
    "def evaluate_model(y_true, X, w, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate trained logistic regression on a dataset.\n",
    "    Returns accuracy and F1 score.\n",
    "    \"\"\"\n",
    "    preds, probs = predict_with_threshold(X, w, threshold=threshold)\n",
    "    acc = accuracy_numpy(y_true, preds)\n",
    "    f1 = f1_score(y_true, preds)\n",
    "    print(f\"✅ Accuracy: {acc*100:.2f}%\")\n",
    "    print(f\"✅ F1 Score: {f1:.4f}\")\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03808747-c2eb-4db3-9b4e-a816f6d72fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_threshold(x, w, threshold=0.5):\n",
    "    tx = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    probs = sigmoid(tx @ w)\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    return preds, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3752962-cd84-4f7a-adab-e857ea203dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute F1 score using only NumPy.\n",
    "    Works for binary classification (0/1).\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-15)\n",
    "    recall = tp / (tp + fn + 1e-15)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-15)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc215321-270b-4349-870f-979863f74571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_grid_search(\n",
    "    y_train, X_train,\n",
    "    y_val, X_val,\n",
    "    pos_weights=[1, 3, 5, 9],\n",
    "    lambdas=[1e-5, 1e-3, 1e-2, 1e-1],\n",
    "    thresholds=[0.3, 0.5, 0.7],\n",
    "    max_iter=10000,\n",
    "    gamma=0.05\n",
    "):\n",
    "    \"\"\"\n",
    "    Safe grid search for weighted penalized logistic regression.\n",
    "    Returns: best_params, best_f1, results_list\n",
    "    \"\"\"\n",
    "    best_f1 = -1\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    total = len(pos_weights) * len(lambdas) * len(thresholds)\n",
    "    run = 0\n",
    "\n",
    "    for pw in pos_weights:\n",
    "        for lam in lambdas:\n",
    "            run += 1\n",
    "            print(f\"\\n=== Run {run}/{total//len(thresholds)} (pos_weight={pw}, lambda_={lam}) ===\")\n",
    "\n",
    "            try:\n",
    "                # Train model\n",
    "                loss, w = logistic_regression_weighted_gd(\n",
    "                    y_train, X_train,\n",
    "                    lambda_=lam,\n",
    "                    gamma=gamma,\n",
    "                    pos_weight=pw,\n",
    "                    neg_weight=1.0,\n",
    "                    max_iter=max_iter,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                # Skip invalid runs\n",
    "                if np.isnan(loss) or np.isinf(loss) or loss > 10:\n",
    "                    print(f\"⚠️  Invalid loss ({loss:.4f}), skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Evaluate all thresholds for this model\n",
    "                for th in thresholds:\n",
    "                    preds, _ = predict_with_threshold(X_val, w, threshold=th)\n",
    "                    f1 = f1_score(y_val, preds)\n",
    "                    results.append((pw, lam, th, f1))\n",
    "\n",
    "                    #print(f\"   → threshold={th:.2f} | F1={f1:.4f}\")\n",
    "\n",
    "                    # Update best model\n",
    "                    if f1 > best_f1:\n",
    "                        best_f1 = f1\n",
    "                        best_params = (pw, lam, th)\n",
    "                        print(f\"   ✅ New best F1 = {best_f1:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error for pos_weight={pw}, lambda_={lam}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Sort results by F1 descending\n",
    "    results.sort(key=lambda t: t[3], reverse=True)\n",
    "\n",
    "    print(\"\\n=== 🏁 Grid Search Complete ===\")\n",
    "    if best_params:\n",
    "        print(f\"🏆 Best F1 = {best_f1:.4f} at pos_weight={best_params[0]}, λ={best_params[1]}, threshold={best_params[2]}\")\n",
    "    else:\n",
    "        print(\"⚠️ No valid runs completed.\")\n",
    "\n",
    "    return best_params, best_f1, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e1962d4-c091-4964-9296-e968371bc144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/12 (pos_weight=1, lambda_=0.001) ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m best_params, best_f1, results = \u001b[43msafe_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_tr_encoded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_va\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_va_encoded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpos_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.65\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.4\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36msafe_grid_search\u001b[39m\u001b[34m(y_train, X_train, y_val, X_val, pos_weights, lambdas, thresholds, max_iter, gamma)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal//\u001b[38;5;28mlen\u001b[39m(thresholds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (pos_weight=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpw\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, lambda_=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     loss, w = \u001b[43mlogistic_regression_weighted_gd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneg_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# Skip invalid runs\u001b[39;00m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.isnan(loss) \u001b[38;5;129;01mor\u001b[39;00m np.isinf(loss) \u001b[38;5;129;01mor\u001b[39;00m loss > \u001b[32m10\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mlogistic_regression_weighted_gd\u001b[39m\u001b[34m(y, x, lambda_, gamma, pos_weight, neg_weight, max_iter, tol, clip_grad, verbose)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grad_norm > clip_grad:\n\u001b[32m    115\u001b[39m     grad *= clip_grad / grad_norm\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m loss = \u001b[43mweighted_logistic_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m losses.append(loss)\n\u001b[32m    120\u001b[39m w -= gamma * grad\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mweighted_logistic_loss\u001b[39m\u001b[34m(y, tx, w, lambda_, pos_weight, neg_weight)\u001b[39m\n\u001b[32m     28\u001b[39m sample_weights = np.where(y == \u001b[32m1\u001b[39m, pos_weight, neg_weight)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# weighted average loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m loss = -np.sum(sample_weights * (y * \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m + (\u001b[32m1\u001b[39m - y) * np.log(\u001b[32m1\u001b[39m - p + eps))) / np.sum(sample_weights)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# no regularization term added to returned loss (for monitoring only)\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(loss)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_params, best_f1, results = safe_grid_search(\n",
    "    y_tr, X_tr_encoded,\n",
    "    y_va, X_va_encoded,\n",
    "    pos_weights=[1, 3, 5, 9],\n",
    "    lambdas=[1e-3, 1e-2, 1e-1],\n",
    "    thresholds=[0.65, 0.7, 0.75, 0.8], \n",
    "    gamma = 0.4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd023cd1-7683-46ad-bb26-8001db0faa2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b930d-26e6-4aef-af90-7face91a86ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     0 | Loss = 0.693147 | GradNorm = 0.5795\n",
      "Iter   100 | Loss = 0.503150 | GradNorm = 0.4945\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m x_tr_final = np.vstack((X_tr_encoded, X_va_encoded))\n\u001b[32m      3\u001b[39m y_tr_final = np.hstack((y_tr, y_va))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m loss, w = \u001b[43mlogistic_regression_weighted_gd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_tr_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tr_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m9.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mlogistic_regression_weighted_gd\u001b[39m\u001b[34m(y, x, lambda_, gamma, pos_weight, neg_weight, max_iter, tol, clip_grad, verbose)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grad_norm > clip_grad:\n\u001b[32m    115\u001b[39m     grad *= clip_grad / grad_norm\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m loss = \u001b[43mweighted_logistic_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m losses.append(loss)\n\u001b[32m    120\u001b[39m w -= gamma * grad\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mweighted_logistic_loss\u001b[39m\u001b[34m(y, tx, w, lambda_, pos_weight, neg_weight)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mweighted_logistic_loss\u001b[39m(y, tx, w, lambda_=\u001b[32m0.0\u001b[39m, pos_weight=\u001b[32m1.0\u001b[39m, neg_weight=\u001b[32m1.0\u001b[39m):\n\u001b[32m     20\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m    Weighted (and optionally penalized) logistic loss.\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    Class weights allow balancing for imbalanced datasets.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     p = \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     eps = \u001b[32m1e-15\u001b[39m  \u001b[38;5;66;03m# avoid log(0)\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# weights per sample\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36msigmoid\u001b[39m\u001b[34m(z)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msigmoid\u001b[39m(z):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     z = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1\u001b[39m / (\u001b[32m1\u001b[39m + np.exp(-z))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janfo\\OneDrive\\Desktop\\universitat\\EPFL\\Machine Learning\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:2330\u001b[39m, in \u001b[36mclip\u001b[39m\u001b[34m(a, a_min, a_max, out, min, max, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mmin\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mmax\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue:\n\u001b[32m   2327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPassing `min` or `max` keyword argument when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2328\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`a_min` and `a_max` are provided is forbidden.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janfo\\OneDrive\\Desktop\\universitat\\EPFL\\Machine Learning\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janfo\\OneDrive\\Desktop\\universitat\\EPFL\\Machine Learning\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:115\u001b[39m, in \u001b[36m_clip\u001b[39m\u001b[34m(a, min, max, out, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m um.maximum(a, \u001b[38;5;28mmin\u001b[39m, out=out, **kwargs)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mum\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Combine training and validation data\n",
    "x_tr_final = np.vstack((X_tr_encoded, X_va_encoded))\n",
    "y_tr_final = np.hstack((y_tr, y_va))\n",
    "loss, w = logistic_regression_weighted_gd(\n",
    "    y_tr_final, x_tr_final, lambda_=1e-3, gamma=0.5, pos_weight=9.0, neg_weight=1.0,\n",
    "    max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5ff7e-e83a-410e-a9dc-357371903c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304,)\n"
     ]
    }
   ],
   "source": [
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057ad4e-6bcf-4407-ad4f-69ba97e5bcb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Suppose you've already trained your model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# loss, w = logistic_regression_weighted_gd(...)\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Evaluate on test or validation data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m acc, f1 = \u001b[43mevaluate_model\u001b[49m(y_te, X_te_encoded, w, \u001b[32m0.75\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'evaluate_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Suppose you've already trained your model\n",
    "# loss, w = logistic_regression_weighted_gd(...)\n",
    "\n",
    "# Evaluate on test or validation data\n",
    "acc, f1 = evaluate_model(y_te, X_te_encoded, w, 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425fe3f-6923-42ad-a984-70414ee1193d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109379, 303)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a7864-13a0-448a-873d-bcc94358ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/4 (pos_weight=7, lambda_=1e-07) ===\n",
      "   ✅ New best F1 = 0.4209\n",
      "   ✅ New best F1 = 0.4209\n",
      "   ✅ New best F1 = 0.4214\n",
      "\n",
      "=== Run 2/4 (pos_weight=8, lambda_=1e-07) ===\n",
      "   ✅ New best F1 = 0.4215\n",
      "\n",
      "=== Run 3/4 (pos_weight=9, lambda_=1e-07) ===\n",
      "   ✅ New best F1 = 0.4217\n",
      "\n",
      "=== Run 4/4 (pos_weight=10, lambda_=1e-07) ===\n",
      "\n",
      "=== 🏁 Grid Search Complete ===\n",
      "🏆 Best F1 = 0.4217 at pos_weight=9, λ=1e-07, threshold=0.7217948717948718\n"
     ]
    }
   ],
   "source": [
    "best_params, best_f1, results = safe_grid_search(\n",
    "    y_tr, X_tr_encoded,\n",
    "    y_va, X_va_encoded,\n",
    "    pos_weights=[7, 8, 9, 10],\n",
    "    lambdas = [1e-7],\n",
    "    thresholds=np.linspace(0.65, 0.85, 40), \n",
    "    gamma = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29f72136-3eba-4f8c-aa60-2f6cce9eb759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     0 | Loss = 0.693147 | GradNorm = 0.5795\n",
      "Iter   100 | Loss = 0.497567 | GradNorm = 0.4519\n",
      "Iter   200 | Loss = 0.483414 | GradNorm = 0.3599\n",
      "Iter   300 | Loss = 0.476542 | GradNorm = 0.2996\n",
      "Iter   400 | Loss = 0.472451 | GradNorm = 0.2542\n",
      "Iter   500 | Loss = 0.469780 | GradNorm = 0.2183\n",
      "Iter   600 | Loss = 0.467930 | GradNorm = 0.1888\n",
      "Iter   700 | Loss = 0.466596 | GradNorm = 0.1640\n",
      "Iter   800 | Loss = 0.465603 | GradNorm = 0.1426\n",
      "Iter   900 | Loss = 0.464846 | GradNorm = 0.1239\n",
      "Iter  1000 | Loss = 0.464259 | GradNorm = 0.1073\n",
      "Iter  1100 | Loss = 0.463799 | GradNorm = 0.0923\n",
      "Iter  1200 | Loss = 0.463434 | GradNorm = 0.0787\n",
      "Iter  1300 | Loss = 0.463146 | GradNorm = 0.0662\n",
      "Iter  1400 | Loss = 0.462920 | GradNorm = 0.0548\n",
      "Iter  1500 | Loss = 0.462745 | GradNorm = 0.0445\n",
      "Iter  1600 | Loss = 0.462610 | GradNorm = 0.0352\n",
      "Iter  1700 | Loss = 0.462510 | GradNorm = 0.0272\n",
      "Iter  1800 | Loss = 0.462435 | GradNorm = 0.0204\n",
      "Iter  1900 | Loss = 0.462379 | GradNorm = 0.0148\n",
      "Iter  2000 | Loss = 0.462337 | GradNorm = 0.0104\n",
      "Iter  2100 | Loss = 0.462304 | GradNorm = 0.0071\n",
      "Iter  2200 | Loss = 0.462276 | GradNorm = 0.0047\n",
      "Iter  2300 | Loss = 0.462252 | GradNorm = 0.0031\n",
      "Iter  2400 | Loss = 0.462231 | GradNorm = 0.0020\n",
      "Iter  2500 | Loss = 0.462211 | GradNorm = 0.0013\n",
      "Iter  2600 | Loss = 0.462193 | GradNorm = 0.0009\n",
      "Iter  2700 | Loss = 0.462176 | GradNorm = 0.0007\n",
      "Iter  2800 | Loss = 0.462160 | GradNorm = 0.0006\n",
      "Iter  2900 | Loss = 0.462145 | GradNorm = 0.0006\n",
      "Iter  3000 | Loss = 0.462130 | GradNorm = 0.0005\n",
      "Iter  3100 | Loss = 0.462117 | GradNorm = 0.0005\n",
      "Iter  3200 | Loss = 0.462104 | GradNorm = 0.0005\n",
      "Iter  3300 | Loss = 0.462092 | GradNorm = 0.0005\n",
      "Iter  3400 | Loss = 0.462081 | GradNorm = 0.0005\n",
      "Iter  3500 | Loss = 0.462070 | GradNorm = 0.0005\n",
      "Iter  3600 | Loss = 0.462060 | GradNorm = 0.0004\n",
      "Iter  3700 | Loss = 0.462050 | GradNorm = 0.0004\n",
      "Iter  3800 | Loss = 0.462040 | GradNorm = 0.0004\n",
      "Iter  3900 | Loss = 0.462031 | GradNorm = 0.0004\n",
      "Iter  4000 | Loss = 0.462023 | GradNorm = 0.0004\n",
      "Iter  4100 | Loss = 0.462015 | GradNorm = 0.0004\n",
      "Iter  4200 | Loss = 0.462007 | GradNorm = 0.0004\n",
      "Iter  4300 | Loss = 0.461999 | GradNorm = 0.0004\n",
      "Iter  4400 | Loss = 0.461992 | GradNorm = 0.0004\n",
      "Iter  4500 | Loss = 0.461985 | GradNorm = 0.0004\n",
      "Iter  4600 | Loss = 0.461978 | GradNorm = 0.0004\n",
      "Iter  4700 | Loss = 0.461971 | GradNorm = 0.0004\n",
      "Iter  4800 | Loss = 0.461965 | GradNorm = 0.0004\n",
      "Iter  4900 | Loss = 0.461959 | GradNorm = 0.0003\n",
      "Iter  5000 | Loss = 0.461953 | GradNorm = 0.0003\n",
      "Iter  5100 | Loss = 0.461948 | GradNorm = 0.0003\n",
      "Iter  5200 | Loss = 0.461942 | GradNorm = 0.0003\n",
      "Iter  5300 | Loss = 0.461937 | GradNorm = 0.0003\n",
      "Iter  5400 | Loss = 0.461932 | GradNorm = 0.0003\n",
      "Iter  5500 | Loss = 0.461926 | GradNorm = 0.0003\n",
      "Iter  5600 | Loss = 0.461922 | GradNorm = 0.0003\n",
      "Iter  5700 | Loss = 0.461917 | GradNorm = 0.0003\n",
      "Iter  5800 | Loss = 0.461912 | GradNorm = 0.0003\n",
      "Iter  5900 | Loss = 0.461908 | GradNorm = 0.0003\n",
      "Iter  6000 | Loss = 0.461903 | GradNorm = 0.0003\n",
      "Iter  6100 | Loss = 0.461899 | GradNorm = 0.0003\n",
      "Iter  6200 | Loss = 0.461895 | GradNorm = 0.0003\n",
      "Iter  6300 | Loss = 0.461891 | GradNorm = 0.0003\n",
      "Iter  6400 | Loss = 0.461887 | GradNorm = 0.0003\n",
      "Iter  6500 | Loss = 0.461883 | GradNorm = 0.0003\n",
      "Iter  6600 | Loss = 0.461879 | GradNorm = 0.0003\n",
      "Iter  6700 | Loss = 0.461876 | GradNorm = 0.0003\n",
      "Iter  6800 | Loss = 0.461872 | GradNorm = 0.0003\n",
      "Iter  6900 | Loss = 0.461869 | GradNorm = 0.0003\n",
      "Iter  7000 | Loss = 0.461865 | GradNorm = 0.0003\n",
      "Iter  7100 | Loss = 0.461862 | GradNorm = 0.0003\n",
      "Iter  7200 | Loss = 0.461858 | GradNorm = 0.0003\n",
      "Iter  7300 | Loss = 0.461855 | GradNorm = 0.0003\n",
      "Iter  7400 | Loss = 0.461852 | GradNorm = 0.0003\n",
      "Iter  7500 | Loss = 0.461849 | GradNorm = 0.0002\n",
      "Iter  7600 | Loss = 0.461846 | GradNorm = 0.0002\n",
      "Iter  7700 | Loss = 0.461843 | GradNorm = 0.0002\n",
      "Iter  7800 | Loss = 0.461840 | GradNorm = 0.0002\n",
      "Iter  7900 | Loss = 0.461837 | GradNorm = 0.0002\n",
      "Iter  8000 | Loss = 0.461834 | GradNorm = 0.0002\n",
      "Iter  8100 | Loss = 0.461831 | GradNorm = 0.0002\n",
      "Iter  8200 | Loss = 0.461828 | GradNorm = 0.0002\n",
      "Iter  8300 | Loss = 0.461826 | GradNorm = 0.0002\n",
      "Iter  8400 | Loss = 0.461823 | GradNorm = 0.0002\n",
      "Iter  8500 | Loss = 0.461820 | GradNorm = 0.0002\n",
      "Iter  8600 | Loss = 0.461818 | GradNorm = 0.0002\n",
      "Iter  8700 | Loss = 0.461815 | GradNorm = 0.0002\n",
      "Iter  8800 | Loss = 0.461813 | GradNorm = 0.0002\n",
      "Iter  8900 | Loss = 0.461810 | GradNorm = 0.0002\n",
      "Iter  9000 | Loss = 0.461808 | GradNorm = 0.0002\n",
      "Iter  9100 | Loss = 0.461806 | GradNorm = 0.0002\n",
      "Iter  9200 | Loss = 0.461803 | GradNorm = 0.0002\n",
      "Iter  9300 | Loss = 0.461801 | GradNorm = 0.0002\n",
      "Iter  9400 | Loss = 0.461799 | GradNorm = 0.0002\n",
      "Iter  9500 | Loss = 0.461796 | GradNorm = 0.0002\n",
      "Iter  9600 | Loss = 0.461794 | GradNorm = 0.0002\n",
      "Iter  9700 | Loss = 0.461792 | GradNorm = 0.0002\n",
      "Iter  9800 | Loss = 0.461790 | GradNorm = 0.0002\n",
      "Iter  9900 | Loss = 0.461788 | GradNorm = 0.0002\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation data\n",
    "x_tr_final = np.vstack((X_tr_encoded, X_va_encoded))\n",
    "y_tr_final = np.hstack((y_tr, y_va))\n",
    "loss, w = logistic_regression_weighted_gd(\n",
    "    y_tr_final, x_tr_final, lambda_=1e-7, gamma=0.5, pos_weight=9.0, neg_weight=1.0,\n",
    "    max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8c603-db24-4bdc-85d5-9a5433a2d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 86.44%\n",
      "✅ F1 Score: 0.4264\n"
     ]
    }
   ],
   "source": [
    "# Suppose you've already trained your model\n",
    "# loss, w = logistic_regression_weighted_gd(...)\n",
    "\n",
    "# Evaluate on test or validation data\n",
    "acc, f1 = evaluate_model(y_te, X_te_encoded, w, 0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84798187-a7a6-409a-a073-811710ce68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final, _ = predict_with_threshold(X_test_encoded, w, 0.77)\n",
    "y_pred_final = 2 * y_pred_final - 1   # converts 0→-1, 1→1\n",
    "\n",
    "create_csv_submission(test_ids, y_pred_final, \"Model5_77preds.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2761c6f",
   "metadata": {},
   "source": [
    "# Remove features from the model\n",
    "In this section, we will try different strategies to reduce the number of features of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de6779",
   "metadata": {},
   "source": [
    "## Strategy I: only significant t-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dace8aa2-481f-4e51-96df-57063614a725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304,)\n",
      "[ 4.28168295e-01  7.58254148e-01 -1.31194156e+00  5.98270576e-01\n",
      " -1.42216390e-03 -1.42216390e-03  1.98747320e+01  4.38222647e+01\n",
      "  4.78051493e-01  8.18682634e-01  5.32066618e-01  6.30248619e-01\n",
      " -8.33699587e+00 -1.68187969e+00  1.31272113e+00 -1.31685626e+00\n",
      "  1.88574146e+01 -6.29779428e+00  8.04704529e-01 -3.82289221e-01\n",
      "  7.71685380e-01  4.43951028e+00  2.10336765e+00  1.37313334e+00\n",
      "  1.12669736e+00  3.99436913e-01  6.52433714e-01 -1.81361562e-01\n",
      " -1.81673962e-01  5.46442403e-01 -4.58363459e-01 -2.35246477e-01\n",
      " -8.82364127e-02  1.23084527e-01 -1.77280016e+01 -1.84461528e+01\n",
      " -3.60150281e-01 -1.51347227e-01 -1.71016776e-01  3.82593437e-03\n",
      "  3.75239289e-01 -2.54255737e-01 -3.92431851e+01  2.67174192e-01\n",
      "  1.91084446e-02  2.48323685e-01  1.46959571e-01 -1.05681831e-01\n",
      " -2.64686367e+00 -2.11572428e+00 -1.37118730e-01  1.76802599e+00\n",
      "  3.40985228e-01  3.97670733e-02 -2.42587659e+01  2.76397073e-02\n",
      " -3.33196145e-01  1.00460166e-01  1.19873874e-01 -2.68971292e-01\n",
      " -7.92703706e+00  5.57799942e-01 -5.91458311e-01 -1.68457397e+01\n",
      " -3.63255778e+00 -2.40418677e-01 -4.92801092e+00 -1.83203035e+01\n",
      " -4.60654580e+00 -1.14235108e+00 -3.69752033e-01 -4.92405608e+01\n",
      " -4.32308877e+00  3.50012068e+00  2.70600764e+00 -1.16560543e+01\n",
      " -2.94273529e+00 -3.76641996e+00  3.61658385e-01  4.70722999e-01\n",
      "  5.06157587e-02  7.09874123e-02  3.21063487e-02  5.60830058e-02\n",
      "  6.39623403e+00  4.88979472e+00 -4.77335954e-01 -1.76010525e-01\n",
      " -6.13566865e+00  2.64360936e-01 -1.25596272e+00  6.98894688e+00\n",
      "  6.67412705e+00  3.08570267e+00  4.98219709e+00  4.28968865e-01\n",
      "  7.69016831e+00  1.08204849e+01  2.55300960e+00 -3.34000992e-03\n",
      "  2.46524416e+00  2.19287262e+00  1.87016438e+00 -6.03793292e+00\n",
      "  1.62805772e+00 -9.90632779e+00 -3.98448519e-01  2.08901829e-01\n",
      "  5.54283366e-01 -9.14211917e-01  1.13827403e-01 -6.97119451e+00\n",
      "  1.66559663e-01 -3.19201989e-01 -2.51405851e+00 -1.57054705e+00\n",
      "  2.82448297e-01 -7.20597303e+00 -8.20786849e-01 -1.51836044e+00\n",
      "  3.21122175e+00  1.07359589e+00  2.09381512e-01 -1.04367699e+00\n",
      "  2.83988275e-01 -6.96563708e-01 -1.32620033e+00 -1.57464078e+00\n",
      " -7.55907508e-01  7.22481008e-01  9.91017882e-01  2.30832443e+00\n",
      "  3.11389410e+00 -8.18746815e-01 -2.27392821e+00 -3.02311373e+00\n",
      "  1.89050837e+00 -1.16330257e+00 -2.46251020e+00 -5.11114288e-01\n",
      "  3.74551591e+00  8.38972791e-01  4.79328129e+00  5.54114056e+00\n",
      "  3.40856066e+00  4.83778098e-01  1.49723643e+00 -1.71017105e-01\n",
      "  5.15324133e+00 -2.93841104e+00 -2.35256951e-01 -2.07324320e+01\n",
      " -1.64355653e+01 -5.48108879e-01 -1.12994673e-02  4.19696521e-03\n",
      " -3.24837832e-02 -5.59864211e-01 -2.17739913e+00  3.12706150e-01\n",
      "  5.04059755e+00  3.15803551e+00  1.06249546e+00 -9.06656121e-01\n",
      " -4.03162277e+00  2.70925784e+00 -5.86434203e+00 -4.83926502e+00\n",
      " -1.18015310e+00 -2.03590520e+00 -2.43051841e+00  2.94214540e+00\n",
      "  1.29673191e+00  1.16460943e+00  4.16781663e+00  5.77840350e-01\n",
      "  8.09483496e-01  7.88428009e-02 -3.60150281e-01 -2.35246477e-01\n",
      " -2.86609041e-01  7.93357660e-01  1.03972530e-01  4.81775012e-01\n",
      "  3.16581207e-02  2.22762031e-01  6.63551245e-02  3.00417995e-01\n",
      "  2.48323685e-01  6.63551245e-02 -3.61794954e-02  5.14290734e-01\n",
      "  2.02556280e-01 -6.87164750e-02  2.04729796e-01 -8.69988227e-01\n",
      "  4.55841266e-01  1.82682929e-01  3.35826811e-02  1.09100800e-01\n",
      "  7.85617266e-02 -6.87164750e-02 -1.48216449e-02  3.35826811e-02\n",
      "  1.09100800e-01  7.85617266e-02 -6.87164750e-02  1.09100800e-01\n",
      " -1.48216449e-02  3.35826811e-02  5.19100679e+00 -8.10584176e-01\n",
      "  5.38862320e-02  8.11244584e+00  1.36419165e+00 -1.73553235e+00\n",
      " -2.40486051e+00 -2.95083054e+00 -1.07596538e+00 -9.61266766e-01\n",
      "  4.58179401e-01 -2.15189781e-01 -7.87498124e-01 -1.14226343e+00\n",
      "  3.76831090e-02  9.30269865e-02 -5.51172207e-02  1.85643768e-01\n",
      "  1.15039978e-01 -2.67085576e-01  7.52223091e-02  3.27281378e-01\n",
      "  8.49993080e-02 -6.48098222e-01  5.06157587e-02  7.09874123e-02\n",
      "  3.21063487e-02  5.60830058e-02 -2.64403032e+00 -3.84289914e+00\n",
      " -5.98820445e+00 -6.64958896e+00 -6.51232736e+00 -1.60806881e+00\n",
      " -4.56221329e+00 -8.11597060e-01  1.76438188e-02  3.96068988e-01\n",
      "  1.47620615e-01  9.43366009e-01 -1.08474856e+00 -7.80110843e+00\n",
      "  1.00682821e+00  8.69474293e-01 -1.10458170e+00 -1.36597672e+00\n",
      " -3.92421470e+00  1.17263330e-01  1.03577782e+00  1.87258888e+00\n",
      " -8.40228685e-01 -1.68295775e+00 -5.51725698e+00 -9.10678773e-01\n",
      " -9.96098620e-01  8.24511801e-01  7.78055262e+00  6.35727431e+00\n",
      " -4.17392874e-01  1.13274670e+00 -3.10531793e+00 -1.62890856e+00\n",
      "  2.47445327e+00 -1.08550231e+00  2.67238191e+00  4.92287140e-03\n",
      " -1.88727469e-02 -4.00132752e-03 -3.00514651e-02  1.93171664e-03\n",
      "  5.66987563e-02  7.20446795e-03  1.93171664e-03  5.66987563e-02\n",
      " -1.75584675e+00 -2.76840279e-02  5.66987563e-02  7.20446795e-03\n",
      " -1.64570235e+00 -5.29133194e-01 -1.41897587e+00  1.02373707e+00\n",
      "  3.47454585e+00 -3.61794954e-02 -1.45905520e+00  3.51435616e-01\n",
      " -3.61794954e-02 -3.32521491e-01 -2.53148242e+00 -1.89983717e+00\n",
      " -3.61794954e-02  2.63592288e+00 -1.12994673e-02 -2.83918351e-02]\n",
      "significant 89 out of 304\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "compute tvalues for the trained weights\n",
    "\"\"\"\"\"\n",
    "\n",
    "tvalues = logistic_tvalues(x_tr_final, w, 0.68)\n",
    "# we calculate t-values with the training set only\n",
    "print(tvalues.shape)\n",
    "print(tvalues)\n",
    "print(\"significant\", np.sum(np.abs(tvalues) > 2), \"out of\", tvalues.shape[0])  # number of significant features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf27360d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(278915, 303) (303,)\n",
      "Iter     0 | Loss = 0.693147 | GradNorm = 0.4176\n",
      "Iter   100 | Loss = 0.502137 | GradNorm = 0.0137\n",
      "Iter   200 | Loss = 0.496586 | GradNorm = 0.0081\n",
      "Iter   300 | Loss = 0.494222 | GradNorm = 0.0058\n",
      "Iter   400 | Loss = 0.492910 | GradNorm = 0.0045\n",
      "Iter   500 | Loss = 0.492089 | GradNorm = 0.0036\n",
      "Iter   600 | Loss = 0.491536 | GradNorm = 0.0030\n",
      "Iter   700 | Loss = 0.491145 | GradNorm = 0.0026\n",
      "Iter   800 | Loss = 0.490858 | GradNorm = 0.0022\n",
      "Iter   900 | Loss = 0.490640 | GradNorm = 0.0020\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(x_tr_final.shape, significant_mask.shape)\n\u001b[32m      7\u001b[39m x_tr_final_sig = x_tr_final[:, significant_mask]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m loss, wsig = \u001b[43mlogistic_regression_weighted_gd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_tr_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tr_final_sig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m9.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mlogistic_regression_weighted_gd\u001b[39m\u001b[34m(y, x, lambda_, gamma, pos_weight, neg_weight, max_iter, tol, clip_grad, verbose)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grad_norm > clip_grad:\n\u001b[32m    115\u001b[39m     grad *= clip_grad / grad_norm\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m loss = \u001b[43mweighted_logistic_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m losses.append(loss)\n\u001b[32m    120\u001b[39m w -= gamma * grad\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mweighted_logistic_loss\u001b[39m\u001b[34m(y, tx, w, lambda_, pos_weight, neg_weight)\u001b[39m\n\u001b[32m     28\u001b[39m sample_weights = np.where(y == \u001b[32m1\u001b[39m, pos_weight, neg_weight)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# weighted average loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m loss = -np.sum(sample_weights * (y * np.log(p + eps) + (\u001b[32m1\u001b[39m - y) * \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m)) / np.sum(sample_weights)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# no regularization term added to returned loss (for monitoring only)\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(loss)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "train the model using the easyest strategy: only relevant features with high t-values\n",
    "\"\"\"\n",
    "significant_mask = np.abs(tvalues) > 2.0\n",
    "significant_mask = significant_mask[1:]  # remove intercept\n",
    "print(x_tr_final.shape, significant_mask.shape)\n",
    "x_tr_final_sig = x_tr_final[:, significant_mask]\n",
    "\n",
    "loss, wsig = logistic_regression_weighted_gd(\n",
    "    y_tr_final, x_tr_final_sig, lambda_=1e-7, gamma=0.5, pos_weight=9.0, neg_weight=1.0,\n",
    "    max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e639a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 86.25%\n",
      "✅ F1 Score: 0.4036\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the reduced model on test data\n",
    "\n",
    "X_te_encoded_sig = X_te_encoded[:, significant_mask]\n",
    "acc, f1 = evaluate_model(y_te, X_te_encoded_sig, wsig, 0.68)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a7270",
   "metadata": {},
   "source": [
    "## Strategy II: Not Correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05664ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Second strategy: remove X correlated features \"\"\"\n",
    "\n",
    "\n",
    "def remove_correlated_non_significant(X, significant, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Remove non-significant features that are highly correlated with others.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (n, d)\n",
    "        Data matrix (rows = samples, columns = features).\n",
    "    significant : np.ndarray, shape (d,)\n",
    "        Boolean array where True marks features that must be kept.\n",
    "    threshold : float\n",
    "        Correlation magnitude above which two features are considered correlated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    keep : np.ndarray, shape (d,), dtype=bool\n",
    "        Boolean mask indicating which features are kept (True) or removed (False).\n",
    "    \"\"\"\n",
    "\n",
    "    d = X.shape[1]\n",
    "    keep = np.ones(d, dtype=bool)\n",
    "    significant = np.array(significant, dtype=bool)\n",
    "\n",
    "    corr_matrix = np.abs(np.corrcoef(X, rowvar=False))\n",
    "    np.fill_diagonal(corr_matrix, 0.0)\n",
    "\n",
    "    corr_adj = corr_matrix > threshold\n",
    "\n",
    "    while True:\n",
    "        counts = np.array([np.sum(corr_adj[i] & keep) for i in range(d)])\n",
    "        counts[~keep] = 0\n",
    "\n",
    "        if counts.max() == 0:\n",
    "            break\n",
    "\n",
    "        i_max = np.argmax(counts)\n",
    "\n",
    "        if significant[i_max]:\n",
    "            counts[i_max] = 0\n",
    "            if counts.max() == 0:\n",
    "                break\n",
    "            i_max = np.argmax(counts)\n",
    "            if significant[i_max]:\n",
    "                break\n",
    "\n",
    "        if not significant[i_max] and keep[i_max]:\n",
    "            keep[i_max] = False\n",
    "            corr_adj[i_max, :] = False\n",
    "            corr_adj[:, i_max] = False\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8530f554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272 features remain out of 303\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"We use the tvalues to determine significant features\"\n",
    "\n",
    "significant_mask = np.abs(tvalues) > 2.0\n",
    "significant_mask = significant_mask[1:]  # remove intercept\n",
    "not_corr_mask = remove_correlated_non_significant(x_tr_final, significant_mask, threshold=0.9)\n",
    "x_tr_final_nocorr = x_tr_final[:, not_corr_mask]\n",
    "print(np.sum(not_corr_mask), \"features remain out of\", not_corr_mask.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19b43fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     0 | Loss = 0.693147 | GradNorm = 0.5195\n",
      "Iter   100 | Loss = 0.471345 | GradNorm = 0.0119\n",
      "Iter   200 | Loss = 0.467230 | GradNorm = 0.0069\n",
      "Iter   300 | Loss = 0.465536 | GradNorm = 0.0049\n",
      "Iter   400 | Loss = 0.464630 | GradNorm = 0.0037\n",
      "Iter   500 | Loss = 0.464078 | GradNorm = 0.0030\n",
      "Iter   600 | Loss = 0.463711 | GradNorm = 0.0025\n",
      "Iter   700 | Loss = 0.463449 | GradNorm = 0.0021\n",
      "Iter   800 | Loss = 0.463254 | GradNorm = 0.0018\n",
      "Iter   900 | Loss = 0.463103 | GradNorm = 0.0016\n",
      "Iter  1000 | Loss = 0.462981 | GradNorm = 0.0015\n",
      "Iter  1100 | Loss = 0.462881 | GradNorm = 0.0014\n",
      "Iter  1200 | Loss = 0.462797 | GradNorm = 0.0012\n",
      "Iter  1300 | Loss = 0.462726 | GradNorm = 0.0012\n",
      "Iter  1400 | Loss = 0.462663 | GradNorm = 0.0011\n",
      "Iter  1500 | Loss = 0.462609 | GradNorm = 0.0010\n",
      "Iter  1600 | Loss = 0.462561 | GradNorm = 0.0010\n",
      "Iter  1700 | Loss = 0.462518 | GradNorm = 0.0009\n",
      "Iter  1800 | Loss = 0.462479 | GradNorm = 0.0009\n",
      "Iter  1900 | Loss = 0.462445 | GradNorm = 0.0008\n",
      "Iter  2000 | Loss = 0.462413 | GradNorm = 0.0008\n",
      "Iter  2100 | Loss = 0.462384 | GradNorm = 0.0007\n",
      "Iter  2200 | Loss = 0.462358 | GradNorm = 0.0007\n",
      "Iter  2300 | Loss = 0.462333 | GradNorm = 0.0007\n",
      "Iter  2400 | Loss = 0.462311 | GradNorm = 0.0007\n",
      "Iter  2500 | Loss = 0.462290 | GradNorm = 0.0006\n",
      "Iter  2600 | Loss = 0.462271 | GradNorm = 0.0006\n",
      "Iter  2700 | Loss = 0.462253 | GradNorm = 0.0006\n",
      "Iter  2800 | Loss = 0.462236 | GradNorm = 0.0006\n",
      "Iter  2900 | Loss = 0.462220 | GradNorm = 0.0006\n",
      "Iter  3000 | Loss = 0.462205 | GradNorm = 0.0005\n",
      "Iter  3100 | Loss = 0.462191 | GradNorm = 0.0005\n",
      "Iter  3200 | Loss = 0.462178 | GradNorm = 0.0005\n",
      "Iter  3300 | Loss = 0.462166 | GradNorm = 0.0005\n",
      "Iter  3400 | Loss = 0.462154 | GradNorm = 0.0005\n",
      "Iter  3500 | Loss = 0.462143 | GradNorm = 0.0005\n",
      "Iter  3600 | Loss = 0.462132 | GradNorm = 0.0005\n",
      "Iter  3700 | Loss = 0.462122 | GradNorm = 0.0004\n",
      "Iter  3800 | Loss = 0.462113 | GradNorm = 0.0004\n",
      "Iter  3900 | Loss = 0.462103 | GradNorm = 0.0004\n",
      "Iter  4000 | Loss = 0.462095 | GradNorm = 0.0004\n",
      "Iter  4100 | Loss = 0.462086 | GradNorm = 0.0004\n",
      "Iter  4200 | Loss = 0.462078 | GradNorm = 0.0004\n",
      "Iter  4300 | Loss = 0.462070 | GradNorm = 0.0004\n",
      "Iter  4400 | Loss = 0.462063 | GradNorm = 0.0004\n",
      "Iter  4500 | Loss = 0.462056 | GradNorm = 0.0004\n",
      "Iter  4600 | Loss = 0.462049 | GradNorm = 0.0004\n",
      "Iter  4700 | Loss = 0.462042 | GradNorm = 0.0004\n",
      "Iter  4800 | Loss = 0.462036 | GradNorm = 0.0004\n",
      "Iter  4900 | Loss = 0.462030 | GradNorm = 0.0003\n",
      "Iter  5000 | Loss = 0.462024 | GradNorm = 0.0003\n",
      "Iter  5100 | Loss = 0.462018 | GradNorm = 0.0003\n",
      "Iter  5200 | Loss = 0.462013 | GradNorm = 0.0003\n",
      "Iter  5300 | Loss = 0.462007 | GradNorm = 0.0003\n",
      "Iter  5400 | Loss = 0.462002 | GradNorm = 0.0003\n",
      "Iter  5500 | Loss = 0.461997 | GradNorm = 0.0003\n",
      "Iter  5600 | Loss = 0.461992 | GradNorm = 0.0003\n",
      "Iter  5700 | Loss = 0.461987 | GradNorm = 0.0003\n",
      "Iter  5800 | Loss = 0.461983 | GradNorm = 0.0003\n",
      "Iter  5900 | Loss = 0.461978 | GradNorm = 0.0003\n",
      "Iter  6000 | Loss = 0.461974 | GradNorm = 0.0003\n",
      "Iter  6100 | Loss = 0.461970 | GradNorm = 0.0003\n",
      "Iter  6200 | Loss = 0.461965 | GradNorm = 0.0003\n",
      "Iter  6300 | Loss = 0.461961 | GradNorm = 0.0003\n",
      "Iter  6400 | Loss = 0.461957 | GradNorm = 0.0003\n",
      "Iter  6500 | Loss = 0.461954 | GradNorm = 0.0003\n",
      "Iter  6600 | Loss = 0.461950 | GradNorm = 0.0003\n",
      "Iter  6700 | Loss = 0.461946 | GradNorm = 0.0003\n",
      "Iter  6800 | Loss = 0.461943 | GradNorm = 0.0003\n",
      "Iter  6900 | Loss = 0.461939 | GradNorm = 0.0003\n",
      "Iter  7000 | Loss = 0.461936 | GradNorm = 0.0003\n",
      "Iter  7100 | Loss = 0.461932 | GradNorm = 0.0003\n",
      "Iter  7200 | Loss = 0.461929 | GradNorm = 0.0003\n",
      "Iter  7300 | Loss = 0.461926 | GradNorm = 0.0003\n",
      "Iter  7400 | Loss = 0.461923 | GradNorm = 0.0003\n",
      "Iter  7500 | Loss = 0.461919 | GradNorm = 0.0002\n",
      "Iter  7600 | Loss = 0.461916 | GradNorm = 0.0002\n",
      "Iter  7700 | Loss = 0.461913 | GradNorm = 0.0002\n",
      "Iter  7800 | Loss = 0.461910 | GradNorm = 0.0002\n",
      "Iter  7900 | Loss = 0.461908 | GradNorm = 0.0002\n",
      "Iter  8000 | Loss = 0.461905 | GradNorm = 0.0002\n",
      "Iter  8100 | Loss = 0.461902 | GradNorm = 0.0002\n",
      "Iter  8200 | Loss = 0.461899 | GradNorm = 0.0002\n",
      "Iter  8300 | Loss = 0.461897 | GradNorm = 0.0002\n",
      "Iter  8400 | Loss = 0.461894 | GradNorm = 0.0002\n",
      "Iter  8500 | Loss = 0.461891 | GradNorm = 0.0002\n",
      "Iter  8600 | Loss = 0.461889 | GradNorm = 0.0002\n",
      "Iter  8700 | Loss = 0.461886 | GradNorm = 0.0002\n",
      "Iter  8800 | Loss = 0.461884 | GradNorm = 0.0002\n",
      "Iter  8900 | Loss = 0.461881 | GradNorm = 0.0002\n",
      "Iter  9000 | Loss = 0.461879 | GradNorm = 0.0002\n",
      "Iter  9100 | Loss = 0.461877 | GradNorm = 0.0002\n",
      "Iter  9200 | Loss = 0.461874 | GradNorm = 0.0002\n",
      "Iter  9300 | Loss = 0.461872 | GradNorm = 0.0002\n",
      "Iter  9400 | Loss = 0.461870 | GradNorm = 0.0002\n",
      "Iter  9500 | Loss = 0.461868 | GradNorm = 0.0002\n",
      "Iter  9600 | Loss = 0.461865 | GradNorm = 0.0002\n",
      "Iter  9700 | Loss = 0.461863 | GradNorm = 0.0002\n",
      "Iter  9800 | Loss = 0.461861 | GradNorm = 0.0002\n",
      "Iter  9900 | Loss = 0.461859 | GradNorm = 0.0002\n"
     ]
    }
   ],
   "source": [
    "\"Train the new model with the reduced features\"\n",
    "\n",
    "loss, w_not_corr = logistic_regression_weighted_gd(\n",
    "    y_tr_final, x_tr_final_nocorr, lambda_=1e-7, gamma=0.5, pos_weight=9.0, neg_weight=1.0,\n",
    "    max_iter=10000, tol=1e-8, clip_grad=10.0, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "567ca35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 86.44%\n",
      "✅ F1 Score: 0.4261\n"
     ]
    }
   ],
   "source": [
    "\"Evaluate the new model with the not correlated features\"\n",
    "\n",
    "X_te_encoded_not_corr = X_te_encoded[:, not_corr_mask]\n",
    "acc, f1 = evaluate_model(y_te, X_te_encoded_not_corr, w_not_corr, 0.68)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
