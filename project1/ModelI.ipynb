{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a00180",
   "metadata": {},
   "source": [
    "## Model description\n",
    "In this model we will apply the simplest approach to the problem: using the ridge logistic model, k-fold cross-validation to find the best lambda, and NaN elimination along with undersampling and balancing for data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6460b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Implemented_functions import *\n",
    "from Data_cleaning import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe2045",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "We will load all the data here. Different approaches to cleaning it may appear in the same script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d0066c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "<>:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "C:\\Users\\janfo\\AppData\\Local\\Temp\\ipykernel_12236\\4294577664.py:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "  X_train_data_full, X_test_data, Y_train_data_full, train_ids, test_ids = load_csv_data(\"data\\dataset\\dataset\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_data shape: (328135, 321)\n",
      "Y_train_data shape: (328135,)\n",
      "X_test_data shape: (109379, 321)\n"
     ]
    }
   ],
   "source": [
    "X_train_data_full, X_test_data, Y_train_data_full, train_ids, test_ids = load_csv_data(\"data\\dataset\\dataset\")\n",
    "print(\"X_train_data shape:\", X_train_data_full.shape)\n",
    "print(\"Y_train_data shape:\", Y_train_data_full.shape)\n",
    "print(\"X_test_data shape:\", X_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6748ac",
   "metadata": {},
   "source": [
    "Set y labels as 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbe25890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of -1 labels in Y_train_data: 299160\n",
      "Number of 1 labels in Y_train_data: 28975\n",
      "Number of -1 labels in Y_train_data_norm: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of -1 labels in Y_train_data:\", np.sum(Y_train_data_full == -1))\n",
    "print(\"Number of 1 labels in Y_train_data:\", np.sum(Y_train_data_full == 1))\n",
    "Y_train_data_norm_full = np.where(Y_train_data_full == -1, 0, Y_train_data_full)\n",
    "print(\"Number of -1 labels in Y_train_data_norm:\", np.sum(Y_train_data_norm_full == -1))\n",
    "\n",
    "X_train_data, Y_train_data = X_train_data_full, Y_train_data_norm_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02cd74a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced X_train_data shape: (10000, 321)\n",
      "Reduced Y_train_data shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "\"Run if needed to reduce dataset size for faster testing\"\n",
    "X_train_data, Y_train_data = X_train_data_full[:10000], Y_train_data_norm_full[:10000]\n",
    "print(\"Reduced X_train_data shape:\", X_train_data.shape)\n",
    "print(\"Reduced Y_train_data shape:\", Y_train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6fa87c",
   "metadata": {},
   "source": [
    "# Need of standarization\n",
    "After removing the NaNs, the first step is to standardize the data. In future models, we’ll explore better techniques for this, but for now, let’s just compare it with non-standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d55827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logistic regression model without standardization\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 808135222153990016.000000\n",
      "Iteration   200, loss = 808135222153990144.000000\n",
      "Iteration   300, loss = 808135222153990016.000000\n",
      "Iteration   400, loss = 808135222153990144.000000\n",
      "Iteration   500, loss = 808135222153990016.000000\n",
      "Iteration   600, loss = 808135222153990144.000000\n",
      "Iteration   700, loss = 808135222153990016.000000\n",
      "Iteration   800, loss = 808135222153990144.000000\n",
      "Iteration   900, loss = 808135222153990016.000000\n",
      "\n",
      "Training logistic regression model with standardization\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.728092\n",
      "Iteration   200, loss = 0.728092\n",
      "Iteration   300, loss = 0.728092\n",
      "Iteration   400, loss = 0.728092\n",
      "Iteration   500, loss = 0.728092\n",
      "Iteration   600, loss = 0.728092\n",
      "Iteration   700, loss = 0.728092\n",
      "Iteration   800, loss = 0.728092\n",
      "Iteration   900, loss = 0.728092\n"
     ]
    }
   ],
   "source": [
    "x_train = impute_missing_values(X_train_data, strategy='mean')\n",
    "print(\"Training logistic regression model without standardization\")\n",
    "# Train logistic regression model without standardization\n",
    "loss, w = logistic_regression_penalized_gradient_descent(\n",
    "        Y_train_data, x_train, max_iter=1000, gamma=0.5, lambda_=0.5, threshold=1e-8)\n",
    "\n",
    "x_train, means, stds = standardize_features(x_train)\n",
    "# Train logistic regression model with standardization\n",
    "\n",
    "print(\"\\nTraining logistic regression model with standardization\")\n",
    "loss, w = logistic_regression_penalized_gradient_descent(\n",
    "        Y_train_data, x_train, max_iter=1000, gamma=0.5, lambda_=0.5, threshold=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348cc24b",
   "metadata": {},
   "source": [
    "So, when data isn’t standardized, feature scales differ widely, making the scalar product w·x huge and the sigmoid saturates. Standardizing keeps values in a reasonable range, stabilizing training and avoiding numerical issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4ef6a",
   "metadata": {},
   "source": [
    "# Impact of balance\n",
    "In this section, we will test the impact of balancing the data. Using k-fold validation and NaN substitution, we will compare the results of different balancing strategies. Note that we will not discuss methods for replacing NaN values in this section. To proceed, we will define two preprocessing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdea1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_balanced_data(x_train, x_val, y_train, y_val):\n",
    "\n",
    "    # replace missing values with mean and standardize features\n",
    "    x_train = impute_missing_values(x_train, strategy='mean')\n",
    "    x_val = impute_missing_values(x_val, strategy='mean')\n",
    "    x_train, means, stds = standardize_features(x_train)\n",
    "    x_val = (x_val - means) / stds\n",
    "    return x_train, x_val, y_train, y_val\n",
    "\n",
    "def balanced_data(x_train, x_val, y_train, y_val):\n",
    "    # replace missing values with mean and standardize features\n",
    "    x_train = impute_missing_values(x_train, strategy='mean')\n",
    "    x_val = impute_missing_values(x_val, strategy='mean')\n",
    "    x_train, means, stds = standardize_features(x_train)\n",
    "    x_val = (x_val - means) / stds\n",
    "    # balance data by undersampling, so that both classes have the same number of samples\n",
    "    x_train, y_train = balance_data(x_train, y_train, method='undersample')\n",
    "    return x_train, x_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b63759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfo\\OneDrive\\Desktop\\universitat\\EPFL\\Machine Learning\\ML_proj1\\project1\\Data_cleaning.py:45: RuntimeWarning: Mean of empty slice\n",
      "  values = np.nanmean(X_imputed, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   100, loss = 0.221028\n",
      "Iteration   200, loss = 0.218579\n",
      "Iteration   300, loss = 0.218082\n",
      "Iteration   400, loss = 0.217874\n",
      "Iteration   500, loss = 0.217757\n",
      "Iteration   600, loss = 0.217683\n",
      "Iteration   700, loss = 0.217633\n",
      "Iteration   800, loss = 0.217598\n",
      "Iteration   900, loss = 0.217574\n",
      "Iteration  1000, loss = 0.217557\n",
      "Iteration  1100, loss = 0.217545\n",
      "Iteration  1200, loss = 0.217536\n",
      "Iteration  1300, loss = 0.217529\n",
      "Iteration  1400, loss = 0.217525\n",
      "Iteration  1500, loss = 0.217521\n",
      "Iteration  1600, loss = 0.217519\n",
      "Iteration  1700, loss = 0.217517\n",
      "Iteration  1800, loss = 0.217515\n",
      "Converged at iteration 1873\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.216906\n",
      "Iteration   200, loss = 0.214285\n",
      "Iteration   300, loss = 0.213749\n",
      "Iteration   400, loss = 0.213540\n",
      "Iteration   500, loss = 0.213429\n",
      "Iteration   600, loss = 0.213362\n",
      "Iteration   700, loss = 0.213318\n",
      "Iteration   800, loss = 0.213289\n",
      "Iteration   900, loss = 0.213268\n",
      "Iteration  1000, loss = 0.213254\n",
      "Iteration  1100, loss = 0.213243\n",
      "Iteration  1200, loss = 0.213236\n",
      "Iteration  1300, loss = 0.213231\n",
      "Iteration  1400, loss = 0.213227\n",
      "Iteration  1500, loss = 0.213224\n",
      "Iteration  1600, loss = 0.213221\n",
      "Iteration  1700, loss = 0.213220\n",
      "Iteration  1800, loss = 0.213218\n",
      "Converged at iteration 1832\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218420\n",
      "Iteration   200, loss = 0.215899\n",
      "Iteration   300, loss = 0.215423\n",
      "Iteration   400, loss = 0.215247\n",
      "Iteration   500, loss = 0.215158\n",
      "Iteration   600, loss = 0.215105\n",
      "Iteration   700, loss = 0.215071\n",
      "Iteration   800, loss = 0.215048\n",
      "Iteration   900, loss = 0.215033\n",
      "Iteration  1000, loss = 0.215022\n",
      "Iteration  1100, loss = 0.215014\n",
      "Iteration  1200, loss = 0.215008\n",
      "Iteration  1300, loss = 0.215004\n",
      "Iteration  1400, loss = 0.215002\n",
      "Iteration  1500, loss = 0.214999\n",
      "Iteration  1600, loss = 0.214998\n",
      "Iteration  1700, loss = 0.214997\n",
      "Converged at iteration 1710\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.222193\n",
      "Iteration   200, loss = 0.219767\n",
      "Iteration   300, loss = 0.219275\n",
      "Iteration   400, loss = 0.219076\n",
      "Iteration   500, loss = 0.218966\n",
      "Iteration   600, loss = 0.218897\n",
      "Iteration   700, loss = 0.218850\n",
      "Iteration   800, loss = 0.218817\n",
      "Iteration   900, loss = 0.218794\n",
      "Iteration  1000, loss = 0.218778\n",
      "Iteration  1100, loss = 0.218766\n",
      "Iteration  1200, loss = 0.218757\n",
      "Iteration  1300, loss = 0.218750\n",
      "Iteration  1400, loss = 0.218745\n",
      "Iteration  1500, loss = 0.218742\n",
      "Iteration  1600, loss = 0.218739\n",
      "Iteration  1700, loss = 0.218737\n",
      "Iteration  1800, loss = 0.218735\n",
      "Iteration  1900, loss = 0.218734\n",
      "Converged at iteration 1910\n",
      " Accuracy: 91.75%\n",
      " F1 Score: 0.1791\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218887\n",
      "Iteration   200, loss = 0.216287\n",
      "Iteration   300, loss = 0.215803\n",
      "Iteration   400, loss = 0.215630\n",
      "Iteration   500, loss = 0.215543\n",
      "Iteration   600, loss = 0.215492\n",
      "Iteration   700, loss = 0.215458\n",
      "Iteration   800, loss = 0.215434\n",
      "Iteration   900, loss = 0.215418\n",
      "Iteration  1000, loss = 0.215406\n",
      "Iteration  1100, loss = 0.215397\n",
      "Iteration  1200, loss = 0.215391\n",
      "Iteration  1300, loss = 0.215386\n",
      "Iteration  1400, loss = 0.215382\n",
      "Iteration  1500, loss = 0.215380\n",
      "Iteration  1600, loss = 0.215378\n",
      "Iteration  1700, loss = 0.215376\n",
      "Iteration  1800, loss = 0.215375\n",
      "Converged at iteration 1810\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.382847\n",
      "Iteration   200, loss = 0.377677\n",
      "Iteration   300, loss = 0.375503\n",
      "Iteration   400, loss = 0.374279\n",
      "Iteration   500, loss = 0.373522\n",
      "Iteration   600, loss = 0.373031\n",
      "Iteration   700, loss = 0.372701\n",
      "Iteration   800, loss = 0.372475\n",
      "Iteration   900, loss = 0.372318\n",
      "Iteration  1000, loss = 0.372206\n",
      "Iteration  1100, loss = 0.372126\n",
      "Iteration  1200, loss = 0.372068\n",
      "Iteration  1300, loss = 0.372026\n",
      "Iteration  1400, loss = 0.371995\n",
      "Iteration  1500, loss = 0.371972\n",
      "Iteration  1600, loss = 0.371955\n",
      "Iteration  1700, loss = 0.371942\n",
      "Iteration  1800, loss = 0.371933\n",
      "Iteration  1900, loss = 0.371926\n",
      "Iteration  2000, loss = 0.371920\n",
      "Iteration  2100, loss = 0.371916\n",
      "Iteration  2200, loss = 0.371913\n",
      "Iteration  2300, loss = 0.371911\n",
      "Iteration  2400, loss = 0.371909\n",
      "Iteration  2500, loss = 0.371907\n",
      "Converged at iteration 2581\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.364363\n",
      "Iteration   200, loss = 0.357849\n",
      "Iteration   300, loss = 0.355491\n",
      "Iteration   400, loss = 0.354365\n",
      "Iteration   500, loss = 0.353749\n",
      "Iteration   600, loss = 0.353383\n",
      "Iteration   700, loss = 0.353152\n",
      "Iteration   800, loss = 0.353000\n",
      "Iteration   900, loss = 0.352897\n",
      "Iteration  1000, loss = 0.352825\n",
      "Iteration  1100, loss = 0.352774\n",
      "Iteration  1200, loss = 0.352737\n",
      "Iteration  1300, loss = 0.352710\n",
      "Iteration  1400, loss = 0.352691\n",
      "Iteration  1500, loss = 0.352676\n",
      "Iteration  1600, loss = 0.352665\n",
      "Iteration  1700, loss = 0.352656\n",
      "Iteration  1800, loss = 0.352650\n",
      "Iteration  1900, loss = 0.352645\n",
      "Iteration  2000, loss = 0.352641\n",
      "Iteration  2100, loss = 0.352638\n",
      "Iteration  2200, loss = 0.352636\n",
      "Iteration  2300, loss = 0.352634\n",
      "Iteration  2400, loss = 0.352633\n",
      "Converged at iteration 2462\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.367902\n",
      "Iteration   200, loss = 0.363557\n",
      "Iteration   300, loss = 0.362158\n",
      "Iteration   400, loss = 0.361496\n",
      "Iteration   500, loss = 0.361131\n",
      "Iteration   600, loss = 0.360912\n",
      "Iteration   700, loss = 0.360772\n",
      "Iteration   800, loss = 0.360679\n",
      "Iteration   900, loss = 0.360615\n",
      "Iteration  1000, loss = 0.360570\n",
      "Iteration  1100, loss = 0.360538\n",
      "Iteration  1200, loss = 0.360515\n",
      "Iteration  1300, loss = 0.360497\n",
      "Iteration  1400, loss = 0.360485\n",
      "Iteration  1500, loss = 0.360475\n",
      "Iteration  1600, loss = 0.360468\n",
      "Iteration  1700, loss = 0.360463\n",
      "Iteration  1800, loss = 0.360459\n",
      "Iteration  1900, loss = 0.360456\n",
      "Iteration  2000, loss = 0.360454\n",
      "Iteration  2100, loss = 0.360452\n",
      "Iteration  2200, loss = 0.360451\n",
      "Converged at iteration 2257\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.373653\n",
      "Iteration   200, loss = 0.367876\n",
      "Iteration   300, loss = 0.365844\n",
      "Iteration   400, loss = 0.364835\n",
      "Iteration   500, loss = 0.364256\n",
      "Iteration   600, loss = 0.363894\n",
      "Iteration   700, loss = 0.363655\n",
      "Iteration   800, loss = 0.363492\n",
      "Iteration   900, loss = 0.363378\n",
      "Iteration  1000, loss = 0.363295\n",
      "Iteration  1100, loss = 0.363236\n",
      "Iteration  1200, loss = 0.363192\n",
      "Iteration  1300, loss = 0.363160\n",
      "Iteration  1400, loss = 0.363135\n",
      "Iteration  1500, loss = 0.363117\n",
      "Iteration  1600, loss = 0.363104\n",
      "Iteration  1700, loss = 0.363093\n",
      "Iteration  1800, loss = 0.363085\n",
      "Iteration  1900, loss = 0.363079\n",
      "Iteration  2000, loss = 0.363075\n",
      "Iteration  2100, loss = 0.363071\n",
      "Iteration  2200, loss = 0.363068\n",
      "Iteration  2300, loss = 0.363066\n",
      "Iteration  2400, loss = 0.363065\n",
      "Iteration  2500, loss = 0.363063\n",
      "Converged at iteration 2536\n",
      " Accuracy: 58.65%\n",
      " F1 Score: 0.2570\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.387809\n",
      "Iteration   200, loss = 0.382998\n",
      "Iteration   300, loss = 0.381450\n",
      "Iteration   400, loss = 0.380730\n",
      "Iteration   500, loss = 0.380329\n",
      "Iteration   600, loss = 0.380079\n",
      "Iteration   700, loss = 0.379912\n",
      "Iteration   800, loss = 0.379794\n",
      "Iteration   900, loss = 0.379710\n",
      "Iteration  1000, loss = 0.379648\n",
      "Iteration  1100, loss = 0.379602\n",
      "Iteration  1200, loss = 0.379567\n",
      "Iteration  1300, loss = 0.379540\n",
      "Iteration  1400, loss = 0.379520\n",
      "Iteration  1500, loss = 0.379505\n",
      "Iteration  1600, loss = 0.379493\n",
      "Iteration  1700, loss = 0.379484\n",
      "Iteration  1800, loss = 0.379477\n",
      "Iteration  1900, loss = 0.379472\n",
      "Iteration  2000, loss = 0.379468\n",
      "Iteration  2100, loss = 0.379465\n",
      "Iteration  2200, loss = 0.379462\n",
      "Iteration  2300, loss = 0.379460\n",
      "Iteration  2400, loss = 0.379459\n",
      "Iteration  2500, loss = 0.379457\n",
      "Converged at iteration 2521\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "acc_no_bal, f1_no_bal = kfold_logistic_ridge(X_train_data, Y_train_data, no_balanced_data, k=5, gamma=0.5, lambda_=1e-3, alpha = 0.7)\n",
    "acc_bal, f1_bal = kfold_logistic_ridge(X_train_data, Y_train_data, balanced_data, k=5, gamma=0.5, lambda_=1e-3, alpha = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3631ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without balancing accuracy: 0.9133000000000001 F1 Score: 0.03582089552238801\n",
      "With balancing accuracy: 0.8471 F1 Score: 0.05139263252470795\n"
     ]
    }
   ],
   "source": [
    "print(\"Without balancing accuracy:\", acc_no_bal, \"F1 Score:\", f1_no_bal)\n",
    "print(\"With balancing accuracy:\", acc_bal, \"F1 Score:\", f1_bal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf6fac6",
   "metadata": {},
   "source": [
    "As we see, F1 score increases with balancing. The downfall of accuracy must be considered carefully, so the not balanced model tends to predict the majority class, without understanding the resons of the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063915c2",
   "metadata": {},
   "source": [
    "# NaN feature removal\n",
    "Now we want to test the following hypothesis: are features with many NaNs relevant? The easiest approach to assess this is by removing the features with a high number of NaNs and observing the impact. Data balancing will be applied, as we have already seen that it is beneficial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab04c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaN_features(x_train, x_val, y_train, y_val):\n",
    "\n",
    "    # replace missing values with mean and standardize features\n",
    "    x_train = impute_missing_values(x_train, strategy='mean')\n",
    "    x_val = impute_missing_values(x_val, strategy='mean')\n",
    "    x_train, means, stds = standardize_features(x_train)\n",
    "    x_val = (x_val - means) / stds\n",
    "    # balance data by undersampling, so that both classes have the same number of samples\n",
    "    x_train, y_train = balance_data(x_train, y_train, method='undersample')\n",
    "    return x_train, x_val, y_train, y_val\n",
    "\n",
    "def no_NaN_features(x_train, x_val, y_train, y_val, threshold=0.3):\n",
    "    \n",
    "    # remove features that contain NaN values in the training set\n",
    "    x_train, keep_mask = remove_nan_features(x_train, threshold=threshold) # we remove features with more than 30% NaN values\n",
    "    x_val = x_val[:, keep_mask]\n",
    "    # replace missing values with mean and standardize features\n",
    "    x_train = impute_missing_values(x_train, strategy='mean')\n",
    "    x_val = impute_missing_values(x_val, strategy='mean')\n",
    "    x_train, means, stds = standardize_features(x_train)\n",
    "    x_val = (x_val - means) / stds\n",
    "    # balance data by undersampling, so that both classes have the same number of samples\n",
    "    x_train, y_train = balance_data(x_train, y_train, method='undersample')\n",
    "    print(\"Number of removes features due to NaN values:\", np.sum(~keep_mask))\n",
    "    return x_train, x_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09905371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.476219\n",
      "Iteration   200, loss = 0.475579\n",
      "Iteration   300, loss = 0.475435\n",
      "Iteration   400, loss = 0.475378\n",
      "Iteration   500, loss = 0.475352\n",
      "Iteration   600, loss = 0.475338\n",
      "Iteration   700, loss = 0.475331\n",
      "Iteration   800, loss = 0.475326\n",
      "Iteration   900, loss = 0.475323\n",
      "Iteration  1000, loss = 0.475321\n",
      "Iteration  1100, loss = 0.475319\n",
      "Converged at iteration 1149\n",
      " Accuracy: 85.28%\n",
      " F1 Score: 0.4096\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.476243\n",
      "Iteration   200, loss = 0.475540\n",
      "Iteration   300, loss = 0.475363\n",
      "Iteration   400, loss = 0.475293\n",
      "Iteration   500, loss = 0.475260\n",
      "Iteration   600, loss = 0.475243\n",
      "Iteration   700, loss = 0.475234\n",
      "Iteration   800, loss = 0.475229\n",
      "Iteration   900, loss = 0.475225\n",
      "Iteration  1000, loss = 0.475223\n",
      "Iteration  1100, loss = 0.475221\n",
      "Converged at iteration 1185\n",
      " Accuracy: 77.96%\n",
      " F1 Score: 0.3812\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.476633\n",
      "Iteration   200, loss = 0.475916\n",
      "Iteration   300, loss = 0.475727\n",
      "Iteration   400, loss = 0.475647\n",
      "Iteration   500, loss = 0.475608\n",
      "Iteration   600, loss = 0.475589\n",
      "Iteration   700, loss = 0.475578\n",
      "Iteration   800, loss = 0.475571\n",
      "Iteration   900, loss = 0.475567\n",
      "Iteration  1000, loss = 0.475565\n",
      "Iteration  1100, loss = 0.475563\n",
      "Iteration  1200, loss = 0.475562\n",
      "Converged at iteration 1201\n",
      " Accuracy: 89.74%\n",
      " F1 Score: 0.3861\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.470298\n",
      "Iteration   200, loss = 0.469570\n",
      "Iteration   300, loss = 0.469389\n",
      "Iteration   400, loss = 0.469313\n",
      "Iteration   500, loss = 0.469277\n",
      "Iteration   600, loss = 0.469258\n",
      "Iteration   700, loss = 0.469247\n",
      "Iteration   800, loss = 0.469241\n",
      "Iteration   900, loss = 0.469237\n",
      "Iteration  1000, loss = 0.469234\n",
      "Iteration  1100, loss = 0.469232\n",
      "Iteration  1200, loss = 0.469231\n",
      "Converged at iteration 1232\n",
      " Accuracy: 85.04%\n",
      " F1 Score: 0.4112\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.473245\n",
      "Iteration   200, loss = 0.472628\n",
      "Iteration   300, loss = 0.472481\n",
      "Iteration   400, loss = 0.472417\n",
      "Iteration   500, loss = 0.472385\n",
      "Iteration   600, loss = 0.472368\n",
      "Iteration   700, loss = 0.472357\n",
      "Iteration   800, loss = 0.472351\n",
      "Iteration   900, loss = 0.472347\n",
      "Iteration  1000, loss = 0.472345\n",
      "Iteration  1100, loss = 0.472343\n",
      "Iteration  1200, loss = 0.472341\n",
      "Converged at iteration 1230\n",
      " Accuracy: 80.14%\n",
      " F1 Score: 0.3819\n",
      "Start cleaning batch 1 out of 5\n",
      "Number of removes features due to NaN values: 177\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.480207\n",
      "Iteration   200, loss = 0.479701\n",
      "Iteration   300, loss = 0.479586\n",
      "Iteration   400, loss = 0.479540\n",
      "Iteration   500, loss = 0.479519\n",
      "Iteration   600, loss = 0.479508\n",
      "Iteration   700, loss = 0.479502\n",
      "Iteration   800, loss = 0.479498\n",
      "Iteration   900, loss = 0.479496\n",
      "Iteration  1000, loss = 0.479494\n",
      "Converged at iteration 1095\n",
      " Accuracy: 85.17%\n",
      " F1 Score: 0.4083\n",
      "Start cleaning batch 2 out of 5\n",
      "Number of removes features due to NaN values: 177\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.480584\n",
      "Iteration   200, loss = 0.480027\n",
      "Iteration   300, loss = 0.479884\n",
      "Iteration   400, loss = 0.479826\n",
      "Iteration   500, loss = 0.479800\n",
      "Iteration   600, loss = 0.479786\n",
      "Iteration   700, loss = 0.479778\n",
      "Iteration   800, loss = 0.479773\n",
      "Iteration   900, loss = 0.479770\n",
      "Iteration  1000, loss = 0.479768\n",
      "Iteration  1100, loss = 0.479766\n",
      "Converged at iteration 1194\n",
      " Accuracy: 85.29%\n",
      " F1 Score: 0.4087\n",
      "Start cleaning batch 3 out of 5\n",
      "Number of removes features due to NaN values: 177\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.480762\n",
      "Iteration   200, loss = 0.480175\n",
      "Iteration   300, loss = 0.480024\n",
      "Iteration   400, loss = 0.479962\n",
      "Iteration   500, loss = 0.479934\n",
      "Iteration   600, loss = 0.479920\n",
      "Iteration   700, loss = 0.479912\n",
      "Iteration   800, loss = 0.479908\n",
      "Iteration   900, loss = 0.479905\n",
      "Iteration  1000, loss = 0.479903\n",
      "Converged at iteration 1095\n",
      " Accuracy: 85.33%\n",
      " F1 Score: 0.4098\n",
      "Start cleaning batch 4 out of 5\n",
      "Number of removes features due to NaN values: 177\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.474105\n",
      "Iteration   200, loss = 0.473509\n",
      "Iteration   300, loss = 0.473364\n",
      "Iteration   400, loss = 0.473305\n",
      "Iteration   500, loss = 0.473278\n",
      "Iteration   600, loss = 0.473265\n",
      "Iteration   700, loss = 0.473257\n",
      "Iteration   800, loss = 0.473253\n",
      "Iteration   900, loss = 0.473250\n",
      "Iteration  1000, loss = 0.473248\n",
      "Iteration  1100, loss = 0.473246\n",
      "Converged at iteration 1140\n",
      " Accuracy: 85.05%\n",
      " F1 Score: 0.4084\n",
      "Start cleaning batch 5 out of 5\n",
      "Number of removes features due to NaN values: 177\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.477456\n",
      "Iteration   200, loss = 0.476967\n",
      "Iteration   300, loss = 0.476842\n",
      "Iteration   400, loss = 0.476788\n",
      "Iteration   500, loss = 0.476761\n",
      "Iteration   600, loss = 0.476747\n",
      "Iteration   700, loss = 0.476739\n",
      "Iteration   800, loss = 0.476734\n",
      "Iteration   900, loss = 0.476730\n",
      "Iteration  1000, loss = 0.476728\n",
      "Iteration  1100, loss = 0.476727\n",
      "Converged at iteration 1181\n",
      " Accuracy: 85.10%\n",
      " F1 Score: 0.4004\n"
     ]
    }
   ],
   "source": [
    "acc_NaN, f1_NaN = kfold_logistic_ridge(X_train_data, Y_train_data, NaN_features, k=5, gamma=0.5, lambda_=1e-3, alpha = 0.7)\n",
    "acc_no_NaN, f1_no_NaN = kfold_logistic_ridge(X_train_data, Y_train_data, no_NaN_features, k=5, gamma=0.5, lambda_=1e-3, alpha = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "904ef622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With NaN features accuracy: 0.8363265119539214 F1 Score: 0.3940022237887491\n",
      "Without NaN features accuracy: 0.8518963231596752 F1 Score: 0.4071321665799156\n"
     ]
    }
   ],
   "source": [
    "print(\"With NaN features accuracy:\", acc_NaN, \"F1 Score:\", f1_NaN)\n",
    "print(\"Without NaN features accuracy:\", acc_no_NaN, \"F1 Score:\", f1_no_NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9a824",
   "metadata": {},
   "source": [
    "Removing NaN features seems to improve the model, as we’ve seen. Let’s try to tune the percentage of removed NaNs by looking at the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1678cce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArcAAAHWCAYAAABt3aEVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAATxNJREFUeJzt3QucTfX+//HPDGPI/S6FHBQhtyKliyiViqiTUgmHLkqoRCXpRio5JNU5RU7kHL/SRSclRBfCRKUmUXLNNffLmMz+P97f32/Pf8/MHmbM7LVnr3k9H4/N7Mvs9d3ru/bs9/6uz/quuEAgEDAAAADAB+Kj3QAAAAAgvxBuAQAA4BuEWwAAAPgG4RYAAAC+QbgFAACAbxBuAQAA4BuEWwAAAPgG4RYAAAC+QbgFAACAbxBugQg67bTT7LbbbmMdR9izzz5rf/nLX6xIkSLWtGlT1jdwDHFxcfbYY4+xjuBbhFsghyZPnuw+FJYtWxb2/osvvtgaNWqU5/X53//+lw+eXPjkk09s8ODBdv7559ukSZPs6aefzvax+qKhPjzrrLMs3JnHdd/dd999Qv2m39Xl+eefz/W2E/TZZ5+lP48uCQkJLrTfeuut9uuvv1qs27x5s9u2V6xYEe2mxBRtF126dLFq1apZsWLFrEqVKnb11VfbO++8E+2mAQUS4RaIoFWrVtk//vGPXIfbESNGRKxNfjNv3jyLj4+31157zYXAK6+88ri/8/3330csGGgU+eDBg3l6jv79+9u//vUve/XVV61jx47273//28455xwXDmOZ2q9tm3Cbc8OHD7e2bdvaypUr7fbbb7eXX37ZHnjgAdu/f7917drVpk2bFsEeA2IT4RaIoMTERDf6FksOHDhgsWTbtm1WokQJN6KVE3rs6aefbo8//njY0du8UEnE1q1bXQDJiwsuuMBuvvlm69mzp40fP96ee+45++OPP+yNN94odP3rZ9r+Dh06lO39//M//+O20+uuu85++OEH98WgV69eLtzOnz/fZs+ebWXKlPG0zUAsINwCHtbcpqamug+oevXqWfHixa1ixYrWpk0bmzNnjrtfj50wYYL7OXT3dGgwue+++6xGjRouOJ9xxhku+GQOafrA1OhfpUqVrHTp0nbNNdfYpk2bstTa6Wfd9uOPP9pNN91k5cuXd+2R7777zrVHu8XVVu0S1Qfrzp07Mywr+Bw///yzC2Rly5a1ypUr27Bhw1y7NmzYYJ06dXIfwnqOcLvtw/nzzz/tiSeesDp16rjXqnX50EMPWUpKSvpjtFyVImi9BNeVSgCORaO8jzzyiHt9M2fOPOZjjxw5Yo8++qi1aNHCva6SJUu64KlgEY5KIy655BIbPXr0MUNLbuk5Ze3atem3ffTRR64tapP6WCO8CkCh1H+lSpWyX375xY1o63Hdu3d396Wlpdnf//53a9y4setf9dnll1+epXTizTffdK9fXwoqVKhg3bp1c30ariRH25FGGU866SQ75ZRT3HoI3bWu0WdRaM/cX59//rldf/31VrNmTdff2sYHDhwYdj3OmDHDzjzzTNduLVf9qNeqbSSUXuPYsWOtYcOG7rFVq1Z1o5+7du067joPrjuVg3To0MGt5+rVq4f9UpTT5ah9V111lX388cd29tlnu3X6yiuvZNsGvYe0zl9//fWwX5LVLj1f6Be93r17u+WrHU2aNMnRF6Jw6y70vR2udCfYB3oNrVu3dntDRK+nbt26bvnaLn777bdcbytAXhFugVzas2eP7dixI8tFwfV49GGhcKs/6i+++KI9/PDD7sP8m2++cffrA/HSSy91P2u3dPAi+kBVSH3hhRdcCBkzZowLtxrFGTRoUJYPK434KdA888wz7gNI4Sc7ChXala561T59+rjbFLj1wR4cPVSomT59unvOcCOeN9xwg/uQHzVqlLVq1cqefPJJ94Gv16MPL7VDH3r333+/LVy48Ljr6m9/+5sLls2bN3ev+aKLLrKRI0e6dgRp3SjgKQwF19WFF1543OdWkNcXjOON3u7du9f++c9/ug9ktV/9t337dhcqstu1rsdo9HbixImWXxRORV+GRK9T/anwpXYpBCks6ItJ5jChLwlqr+o09UVIu7JFIWjAgAEuROo5hgwZ4gLJ4sWL03/3qaeecqUeWlfa3vT4uXPnunW8e/fuDMtRkNN2qUClLzD169e3Bx980IVwadCggVvf0rdv3yz9pbCkbfDOO+9025varP+1/FAffvih29YU9rQ9qBZVryUpKSnLetP7Se8PfelQkNe2PHXqVPfcOXm/Hj161L0mhUWFL4V8lQnocqLLUanSjTfe6N4Xemx2B0CuXr3afvrpJ+vcubP7UnI8+hKg7VTrVF9gVB6jL2T6W6Dl5Cd9EdGX7B49erjtPTk52YVsfTEfN26c3XXXXW59LFq0yH0hzux42wqQZwEAOTJp0iSloGNeGjZsmOF3atWqFejRo0f69SZNmgQ6dux4zOX069fPPVdm7777rrv9ySefzHD7ddddF4iLiwusWbPGXU9KSnKPGzBgQIbH3Xbbbe724cOHp9+mn3XbjTfemGV5Bw8ezHLbW2+95R6/cOHCLM/Rt2/f9Nv+/PPPwKmnnuraNWrUqPTbd+3aFShRokSGdRLOihUr3HP+7W9/y3D7/fff726fN29e+m16rpIlSx7z+cI99o033nDP9c4776Tfr+ta/6GvIyUlJcNz6DVUrVo10KtXrwy3h/5u27ZtA9WqVUtfh8FtZ+nSpcds3/z5893jXn/99cD27dsDmzdvDnz44YeB0047za1L/f6+ffsC5cqVC/Tp0yfD727ZsiVQtmzZDLfr9er5hgwZkuGxWn+6vX///lnakJaW5v7/7bffAkWKFAk89dRTGe7//vvvA0WLFs1w+0UXXeSeb8qUKem3ab1pHXTt2jX9NrVfj9P6yMn2NnLkSPe6161bl35b48aN3bal9RD02WefuefV+y3o888/d7dNnTo1w3POnj077O2ZBdfdPffck2Hd6P1brFgx1z+5XY7ap9t03/G899577rEvvPBCICfGjh3rHv/mm2+m33bkyJFA69atA6VKlQrs3bs3/fbMfwf0WkPXXeb3dihdT0xMDKxduzb9tldeecXdrv4OXc7QoUPd7aGPzem2AuQFI7dALml0QqOamS86Av94ypUr53Yda1Qmt3Sgmaa6UrlBKI2g6DMnOOqhOjzR6Emoe+65J9vnvuOOO7LcptHeoMOHD7vR6XPPPdddD440Zx5pDVI7tdtV7dKoWujr12jz8Y7812uVzCPSeq3B0bu80ujW8UZv9TqCtbwalVbdq0ZC9drCrYMgjWZt2bLlhGtvNdqlMgHtBtcIrcoutHtZy9W2plFTjf6F7jlQWzViHq5kQqOhod5++223eznzCKQEd0PrgDu95r/+9a8ZlqPSEq23zMvRKLLKUoK03lq2bJnjWR5Ctze9Xi3rvPPOc32zfPny9APStPtbo7laXpBG9VVeEUojwRq51AhpaPs1+qrfza60JLPQ2TOCu+RVrvLpp5+e0HJq167tRnSPR3sNJCejtsH3jPpG20WQRrf190IHny1YsMDyS7t27TKUMWi7E+0VCG1v8PbM20BetxXgeIoe9xEAMtAfYYWMzFSvqg+1Y1GQUv2pDmhS3Zl2zd1yyy05Csbr1q1zYSfzh5129wbvD/6vulJ9iIZSSUB2Mj9WFORUQqFSBNXyZS7NyEzlFaH0ga/d3Kr7zXx75rrdzIKvIXOb9eGtgBx8rXmhMKjaW+1afffdd+3aa68N+ziFSu061S7i0F3M4dZZkHa1q/REu7LDfXE4HpVjqNxCbdT6Ux8XLfq/f66DX4yCdbiZZT7ASL936qmnZilz0Lakes7saDkKlgqy4WSuAdUyMtdn6j2h2uacWL9+vXvd77//fpZa1eD2Fuz3cNuybgv9wqH26/dUjhFO5m06HG2DqjkPpfeuBMs/crucY2034fpx3759OXq81o36Sm0+1t+H/BDuvS4qcQl3e+b+zOu2AhwP4RbwkEKPgsV7773n5mdVPafqSTXCFzry6bXQUbMgjdh99dVXrnZOdYEabdFIngK5/s9MQSwnt0lOZynI/AGY3zR6q4PW9KVDtY2Z6WAq1SzqPq0HBRi9JtV6Butgs6NRUdVA6gAbBfLc0Chk+/btw94XXPeqrVTYzywYgoNUj5w58OSElqP1rz0C4foxdOQ0r32t2laNfOoLlWovVYOpA7h0EKTWf7jtLSftV3+p9jUcjYznh9wuJ9x7LRytAwkeqBVJ2b3P1C/hZNfXOd0G8vp3ATgewi3gMY2W6YATXbS7UIFXu7GD4Ta7D5patWq5XaEayQkdvdWIYvD+4P/6wNWR9aGjbmvWrMlxGzXSogOHNHKr0bSgEymnOBHB16DlBUeeRAdqaZd88LXm1+itApS+cISbikkjd9pFH9ov4XbnZ6Zd5cED0ULXYV5p9ghRoMouAOfkOXTEvsJkdqO3eozChkYag6OVeZXdtq0Ap9k2NEoeegBZcBaRoGC/h9uWM9+m9uv9ooO8chooM9M2qF3loa9f7ZTgbvn8WE44WqZKeLRd6oCwzF8mMtO60cin2hz6ZSbz34dwNGqa+QDB/B7tBbxEzS3gocy74/WBpd2podNbacRKMn/YaJYCjaRoloVQGvlVaLjiiivc9WA930svvZThcTryPKeCIyuZR1I0+4EXgidiyLw8HbEvx5r5IbdU+6c+CHfijHDr4euvv3ZHgedEsPZWJ2PIL+pf7bLWzBbhjvjXbA7Ho9pIvaZwrzn4WjULgV6/HpN5O9D145WWhJPdth1uPevnzEf5q5RC5TxTpkxxXwyDVE+aeYRTex70ftHIfGaqmw4X5sIJfb+pTbqukgzVnebncsLRutd61hdfPVdm2vsza9as9PeMtjWd8CN0+Xrf6++MvmxlRwFdpRWhZQG///77cafKAwoqRm4BD2leSI3m6WATjZhpTlGNDoYetKL7RAeCKMjog1/TX+l0m6rj1PRhqvfTNDr6cNPIjqZoCo7o6fcVXhQM9cGog8D04R8cccrJrn6FJ40oq2ZUAUpTeWlZofOsRpJem2phFQoVDvTBvGTJEjeypxIBrYf8ovWrdaqR9Mw0vZFGbVWPq0Ct168SEvVjaLjKjtqtS34ezKO+0TRjqtXWNGnaNrTrWzWrOtBOI4iZvwBlpvWn39e0TRodD5aaaIon3aftUduTpnMbOnSo296CU1JpHSj0aDovTeuWG3pOlWhoHeq5FHZ10JF2wes+PZ9KEfQaddBbuPloFepVt67XqT7TY/R6FXpD+0TrXVN0qYRE07ZddtllLpTq9eogMAVnnRzhWFQzrgM0tS2qnSrR0DrWfMvBcoP8WE52NOWZQrumZNNBdTpYTCOwel+rXdq7EjxDmfpDJTDaC6Fp0TSyrL8tX375pftbcKwD07QNqRxE27n+7mhKNm1jGj0+1oGTQIGVp7kWgELkeNM5aYqb400Fpmm8WrZs6aZy0pRY9evXd1Mqacqe0OmnNP1Q5cqV3TRIoW9TTX80cODAQPXq1QMJCQmBevXqBZ599tn06ZuCDhw44KalqlChgpsGqHPnzoFVq1a55wqdmis41U9wWqNQGzduDFx77bWurZpi6vrrr3dTU2U3nVjm58huiq5w6ymc1NTUwIgRIwK1a9d2r7VGjRpuaqHDhw/naDnhZPdYLatOnTpZpgLTen366addP2r6o2bNmgVmzZoVduqkzL+beXqv3EwFNmPGjOO+Fj22Q4cOrm+KFy/u2q/p3pYtW3bc1xvczrTtaBvU1Fba3q644go3lVyot99+O9CmTRv3PLro8Xqd2p6O16fh1pOmuDrzzDPddGKh04L9+OOPgfbt27vttVKlSm5Ks2+//Tbs1GHTp0937VCfNGrUKPD++++7aaR0W2avvvpqoEWLFu79Vrp0aTeV2ODBg922fCzBdffLL78ELrvsssBJJ53kpoDT9n706NETWo7WxfGmAgxn7ty5gU6dOgWqVKni1pv66uqrr3brMtTWrVsDPXv2dOtPfao2hJt2LfN7WD755BO3LvV7Z5xxhptSLLupwDJv55rqS7drezre9pybbQU4UXH6J9oBG0DkaVSpWbNm7iCp4FmqAL/QQY8aTc1cp3uiNAKqkc+cjNADKFiouQV8KNwpS7VrUgea5OQMXkBBpTKZzPWnOrXvt99+60p+AICaW8CHVCurujvVT2pqKNUK6qK6vMxzUQKxRDW5miVCBwLqADPNBqAaXk2LdiJzCgPwH8It4EM6s5N2z+oIbu1W1aTrOnJfB04BsUzTVumgSc0RrZkhdFCaDvYbNWqUVaxYMdrNA1AAUHMLAAAA36DmFgAAAL4R1XC7cOFCN3en6qY096bO7x560IDm3dNpKLXbSY/RmWs2b96c4Tl0hh0d+a15ETV/Yu/evTm6FQAAoJCKas3tgQMH3GTtvXr1cmfDCaVJpDV59LBhw9xjNFH3vffea9dcc42b+D5IwVZnUlF9oQKxJvXWQTPBia1zQpOXKzRrkutIn8seAAAAuafZa3UKeg14hp5mOtwDCwQ1ZebMmcd8zJIlS9zj1q1blz7pd+aJ0T/66CM38f2mTZtyvOwNGzakT7LOhXXANsA2wDbANsA2wDbANmAFdh0otx1LTM2WoHNfa2RV5Qei87vr57PPPjv9MZoiRmle53/XqQTDSUlJcZeg4Hks1q1b58obIk0jxTt27LBKlSod+5sHCiz6MLbRf7GPPox99GHsS/M4z+zdu9edgvpYp5OWmAm3hw8fdjW4Ord2MIBu2bLFqlSpkuFxmtOzQoUK7r7s6BzgI0aMyHK7Aq+W48XGcPToUbcswm1sog9jG/0X++jD2Ecfxr40j/NMcGDyeCWkMRFuVUv717/+1Y2wTpw4Mc/PN3ToUBs0aFCGbwKa2F6nbvRq5FYdo+URbmMTfRjb6L/YRx/GPvow9qV5nGeKFy+eo8cVjZVgq5KBefPmZQifOiPNtm3bMjxep2XUDAq6LzuJiYnukpk6xquwqY3By+Uh/9GHsY3+i330YeyjD2NfnId5JqfLiI+FYLt69Wr79NNPs5x9pnXr1rZ79253mtEgBWB9k2jVqlUUWgwAAIBoiurIrU4LumbNmvTra9eutRUrVria2ZNPPtmuu+46Nx3YrFmzXE1HsI5W9xcrVswaNGhgl19+ufXp08edW1xh+O6777Zu3bq5aSIAAABQuEQ13Gq+2rZt26ZfD9bB9ujRwx577DF7//333fWmTZtm+L358+fbxRdf7H6eOnWqC7Tt2rVzw9Vdu3a1cePGefo6AAAAUDBENdwqoAan4QrnWPcFaRQ3NydsAAAAgH8V6JpbAAAAIDcItwAAAPANwi0AAAB8g3ALAAAA3yDcAgAAwDcItwAAAPANwi0AAAB8g3ALAAAA34jqSRwKs+3bt9umTZssLi7Ok+VVqlTJatas6cmyAAAAooVwGwUbNmywO+68yxYv+srS0tI8WWbxEifZqp+SCbgAAMDXCLdRsGPHDks9kmIVOw60IhVqRHx5qTs32M5Zz7vlMnoLAAD8jHAbRQkVa1hC1brRbAIAAICvcEAZAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfCOq4XbhwoV29dVXW/Xq1S0uLs7efffdDPcHAgF79NFH7eSTT7YSJUpY+/btbfXq1Rke88cff1j37t2tTJkyVq5cOevdu7ft37/f41cCAAAAK+zh9sCBA9akSRObMGFC2PtHjx5t48aNs5dfftm+/vprK1mypHXo0MEOHz6c/hgF2x9++MHmzJljs2bNcoG5b9++Hr4KAAAAFBRFo7nwK664wl3C0ajt2LFj7ZFHHrFOnTq526ZMmWJVq1Z1I7zdunWz5ORkmz17ti1dutTOPvts95jx48fblVdeac8995wbEQYAAEDhEdVweyxr1661LVu2uFKEoLJly1qrVq1s0aJFLtzqf5UiBIOt6PHx8fFupPfaa68N+9wpKSnuErR37173f1pamrtEmoK7yjDi4zR0Hoj48txy4uPdcr14fYWB1iPrM3bRf7GPPox99GHsS/P4szCnyymw4VbBVjRSG0rXg/fp/ypVqmS4v2jRolahQoX0x4QzcuRIGzFiRJbbt2/fnqHkIVK0jLp161r5qiWsSLnIh9tUK2GntGjhlrtt27aIL68w0Btsz5497k2tLw6ILfRf7KMPYx99GPvSPP4s3LdvX2yH20gaOnSoDRo0KMPIbY0aNaxy5cruwLRI27hxo61Zs8Y2J7a0hEBcxJeXsvWQbUlKsuLFi2f5MoATf0Nr9F3bDOE29tB/sY8+jH30YexL8/izUDkmpsNttWrV3P9bt251syUE6XrTpk3TH5N5JPLPP/90MygEfz+cxMREd8lMHeNF52hDcMP4AbM0i3y4dcv5vw2QIJZ/guuTdRqb6L/YRx/GPvow9sV5+FmY02UU2P2ptWvXdgF17ty5GUZYVUvbunVrd13/796925KSktIfM2/ePBfkVJsLAACAwiWqI7eaj1a750MPIluxYoWrma1Zs6YNGDDAnnzySatXr54Lu8OGDXMzIHTu3Nk9vkGDBnb55Zdbnz593HRhqampdvfdd7uDzZgpAQAAoPCJarhdtmyZtW3bNv16sA62R48eNnnyZBs8eLCbC1fz1mqEtk2bNm7qr9Cai6lTp7pA265dOzdc3bVrVzc3LgAAAAqfqIbbiy++2NWeHquO4/HHH3eX7GiUd9q0aRFqIQAAAGJJga25BQAAAHKLcAsAAADfINwCAADANwi3AAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAAKLzhdsOGDbZx48b060uWLLEBAwbYq6++mt9tAwAAACIbbm+66SabP3+++3nLli126aWXuoD78MMP2+OPP2756ejRozZs2DCrXbu2lShRwurUqWNPPPGEBQKB9Mfo50cffdROPvlk95j27dvb6tWr87UdAAAA8Gm4XblypbVs2dL9/J///McaNWpkX331lU2dOtUmT56cr4175plnbOLEifbiiy9acnKyuz569GgbP358+mN0fdy4cfbyyy/b119/bSVLlrQOHTrY4cOH87UtAAAAKPiK5vYXUlNTLTEx0f386aef2jXXXON+rl+/vv3+++/52jiF5k6dOlnHjh3d9dNOO83eeustN1IcHLUdO3asPfLII+5xMmXKFKtataq9++671q1bt3xtDwAAAHwWbhs2bOhGSRU458yZ48oEZPPmzVaxYsV8bdx5553nanl//vlnO/300+3bb7+1L774wsaMGePuX7t2rSuNUClCUNmyZa1Vq1a2aNGibMNtSkqKuwTt3bvX/Z+WluYukaZQHhcXZ/FxGjr//yUWkeKWEx/vluvF6ysMtB5Zn7GL/ot99GHsow9jX5rHn4U5XU6uw61KA6699lp79tlnrUePHtakSRN3+/vvv59erpBfhgwZ4oKnRoWLFCnianCfeuop6969u7tfwVY0UhtK14P3hTNy5EgbMWJEltu3b9/uSTmDllG3bl0rX7WEFSkX+XCbaiXslBYt3HK3bdsW8eUVBnqD7dmzx72p9cUBsYX+i330YeyjD2Nfmsefhfv27YtMuL344ottx44dLnSWL18+/fa+ffvaSSedZPlJNb2q5Z02bZobMV6xYoWbmaF69eouWJ+ooUOH2qBBg9Kv67XUqFHDKleubGXKlLFI02wTa9assc2JLS0hEBfx5aVsPWRbkpKsePHiVqVKlYgvr7C8oTX6rm2GcBt76L/YRx/GPvow9qV5/FmoHBORcCsaRQ0NtsF62Pz2wAMPuNHbYHlB48aNbd26dW7kVeG2WrVq7vatW7e62RKCdL1p06bZPq9qhoN1w6HUMV50jjYEN4wfMEuzyIdbt5z/2wAJYvknuD5Zp7GJ/ot99GHsow9jX5yHn4U5XUauW6LgeMstt7jR06JFi7qgG3rJTwcPHszyQrSMYM2FpghTwJ07d26GUVjNmtC6det8bQsAAAAKvlyP3N522222fv16N/+sRkuV2CPl6quvdjW2NWvWdGUJy5cvdweT9erVy92vZatM4cknn7R69eq5sKt2KXh37tw5Yu0CAACAT8KtZiv4/PPPj7nbP79oPluF1bvuussdCKXQevvtt7uTNgQNHjzYDhw44Gp+d+/ebW3atLHZs2fnuC4DAAAAhTjc6sCr0DOERVLp0qXdPLa6ZEejtzozWn6fHQ0AAACxJ9c1twqaOsjrt99+i0yLAAAAAK9Gbm+44QZ3oFedOnXc1F8JCQkZ7v/jjz9OtC0AAACAt+H2WCUCAAAAQEyF27ycPAEAAACIpBM6iYNo9gJdMp/n96yzzsqPdgEAAACRD7dJSUlu9DY5OTnLrAmaueDo0aO5bwUAAAAQjXCrEyicfvrp9tprr1nVqlUjehIHAAAAIKLh9tdff7W3337b6tatm9tfBQAAAArWPLft2rWzb7/9NjKtAQAAALwcuf3nP//pam5XrlxpjRo1yjLP7TXXXJOX9gAAAADehdtFixbZl19+aR999FGW+zigDAAAADFVlnDPPffYzTffbL///rubBiz0wkwJAAAAiKlwu3PnThs4cKCbKQEAAACI6XDbpUsXmz9/fmRaAwAAAHhZc6s5bocOHWpffPGFNW7cOMsBZf37989LewAAAABvZ0soVaqULViwwF0yH1BGuAUAAEDMhNu1a9dGpiUAAACA1zW3AAAAgG9Gbnv16nXM+19//fW8tAcAAADwLtzu2rUrw/XU1FR3trLdu3fbJZdccuItAQAAALwOtzNnzsxym07gcOedd1qdOnXy2h4AAAAgujW38fHxNmjQIHvhhRfy4+kAAACA6B5Q9ssvv9iff/6ZX08HAAAARL4sQSO0oQKBgP3+++/24YcfWo8ePXLfAgAAACBa4Xb58uVZShIqV65szz///HFnUgAAAAAKVLidP39+ZFoCAAAA5BEncQAAAEDhGrlt1qyZxcXF5egJv/nmm7y2CQAAAIhcuO3cufOJPTsAAABQ0MLt8OHDI98SAAAAwOsDyoKSkpIsOTnZ/dywYUNXugAAAIDoWL9+ve3YscOz5VWsWNESExMt5sPttm3brFu3bvbZZ59ZuXLl3G27d++2tm3b2vTp0920YAAAAPA22J5Rv4EdPnTQs2WeVLKULV70lVWpUsViOtzec889tm/fPvvhhx+sQYMG7rYff/zRncChf//+9tZbb0WinQAAAMiGRmwVbCtedZ8lVKwR8fWUunOD7frvC7Z3794C1ye5DrezZ8+2Tz/9ND3YyplnnmkTJkywyy67LL/bBwAAgBxKqFjDEqvVLdTrK9fz3KalpVlCQkKW23Wb7gMAAABiJtxecskldu+999rmzZvTb9u0aZMNHDjQ2rVrl9/tAwAAACIXbl988UVXX3HaaadZnTp13KV27drutvHjx+f26QAAAIDo1dzWqFHDnYVMdbc//fSTu031t+3bt8+/VgEAAABezXOrU/Feeuml7gIAAADEXFnCvHnz3KwI4aZ82LNnjzuRw+eff57f7QMAAADyP9yOHTvW+vTpY2XKlMlyX9myZe3222+3MWPG5HzJAAAAQLTC7bfffmuXX355tvdrjludkhcAAAAo8OF269atYee3DSpatKht3749v9oFAAAA5FqOw+0pp5xiK1euzPb+7777zk4++eTctwAAAADwOtxeeeWVNmzYMDt8+HCW+w4dOmTDhw+3q666Kr/aBQAAAERuKrBHHnnE3nnnHTv99NPt7rvvtjPOOMPdrrluJ0yYYEePHrWHH3449y0AAAAAvA63VatWta+++sruvPNOGzp0qAUCgfQ5bzt06OACrh4DAAAAxMRJHGrVqmX//e9/bdeuXbZmzRoXcOvVq2fly5ePXAsBAACASJ6hTGH2nHPOOZFfBQAAAKJ/QBkAAABQ0BFuAQAA4BuEWwAAABSucNu8eXN3EJk8/vjjdvDgwUi3CwAAAIhMuE1OTrYDBw64n0eMGGH79+83r2zatMluvvlmq1ixopUoUcIaN25sy5YtS79fMzY8+uij7uxour99+/a2evVqz9oHAACAGJstoWnTptazZ09r06aNC5PPPfeclSpVKuxjFTTzi0aLzz//fGvbtq199NFHVrlyZRdcQ6ceGz16tI0bN87eeOMNq127tjuLmubd/fHHH6148eL51hYAAAD4JNxOnjzZnV531qxZ7qQNCppFi2b9Vd2Xn+H2mWeesRo1atikSZPSb1OADVLQHjt2rDt7WqdOndxtU6ZMcSeTePfdd61bt25hnzclJcVdgvbu3ev+T0tLc5dIU7u1ruLjNHT+vyfDiCS3nPh4t1wvXl9hoPXI+oxd9F/sow9jH32Yv/SZpM/6eB9ni5wuJ0fhVqfanT59uvtZL2Tu3LlWpUoVi7T333/fjcJef/31tmDBAjvllFPsrrvusj59+rj7165da1u2bHGlCEFly5a1Vq1a2aJFi7INtyNHjnTlFZlt377dDh8+bJGmZdStW9fKVy1hRcpFfgNMtRJ2SosWbrnbtm2L+PIKA73B9uzZk/7HBLGF/ot99GHsow/zlz7jW7RoYeWqlrCE8t5ki1ObN7cjR464bOHFZ+G+ffsicxIHL0f+fv31V5s4caINGjTIHnroIVu6dKn179/fihUrZj169HDBVjKf9lfXg/eFo9MH6zlDR241QqyyhzJlylikbdy40Z3hbXNiS0sIxEV8eSlbD9mWpCRXpuHFl5LCQO8Djb5rmyHcxh76L/bRh7GPPsz/Y5SSkpKsWsPulmjeZItt33zjMpmyhRefhTktNz2hM5T98ssvrhxAB5rJmWeeaffee6/VqVPH8nvDP/vss+3pp59215s1a2YrV660l19+2YXbE5WYmOgumbnhfA86R6HIDeMHzNI82ADdcv4vjBHE8k9wfbJOYxP9F/vow9hHH+bvunTllYHoZAsvPgtzuoxct+Tjjz92YXbJkiV21llnucvXX39tDRs2tDlz5lh+0gwIWlaoBg0a2Pr1693P1apVc/9v3bo1w2N0PXgfAAAACo9cj9wOGTLEBg4caKNGjcpy+4MPPmiXXnppvjVOMyWsWrUqw20///yz1apVK/3gMoVY1QBrRodgiYHC9p133plv7QAAAEBsyPXIrUoRevfuneX2Xr16uem38pNC9OLFi11ZgmpUp02bZq+++qr169fP3a+h8AEDBtiTTz7pDj77/vvv7dZbb7Xq1atb586d87UtAAAA8OHIrQ6gWbFihdWrVy/D7botvw9WOuecc2zmzJnuADCdGU0jtar17d69e/pjBg8e7E4w0bdvX9u9e7ebi3f27NnMcQsAAFAI5TrcahouBUnNZHDeeee527788ks3J23oDAT55aqrrnKX7Gj0VsFXFwAAABRuuQ63OgNY6dKl7fnnn3cjqqIygMcee8xN0wUAAADETLjVSKlqYXUJTqarsAsAAABE2wnNcxtEqAUAAEBBwnlDAQAA4BuEWwAAAPgG4RYAAACFM9ympqZau3btbPXq1ZFrEQAAAOBFuE1ISLDvvvvuRJcFAAAAFKyyhJtvvtlee+21yLQGAAAA8HIqsD///NNef/11+/TTT61FixZWsmTJDPePGTMmL+0BAAAAvAu3K1eutObNm7uff/755ywneAAAAABiJtzOnz8/Mi0BAAAAojUV2Jo1a+zjjz+2Q4cOueuBQCCvbQEAAAC8Dbc7d+5004GdfvrpduWVV9rvv//ubu/du7fdd999eWsNAAAA4GW4HThwoJsSbP369XbSSSel337DDTfY7Nmz89IWAAAAwNua208++cSVI5x66qkZbq9Xr56tW7cub60BAAAAvBy5PXDgQIYR26A//vjDEhMT89IWAAAAwNtwe8EFF9iUKVMyTP+VlpZmo0ePtrZt2+atNQAAAICXZQkKsTqgbNmyZXbkyBEbPHiw/fDDD27k9ssvv8xLWwAAAABvR24bNWrkTt7Qpk0b69SpkytT6NKliy1fvtzq1KmTt9YAAAAAXo7cStmyZe3hhx/Oy3IBAACAghFud+3aZa+99polJye762eeeab17NnTKlSokN/tAwAAACJXlrBw4UI77bTTbNy4cS7k6qKfa9eu7e4DAAAAYmbktl+/fu6EDRMnTrQiRYq4244ePWp33XWXu+/777+PRDsBAACA/B+5XbNmjTvNbjDYin4eNGiQuw8AAACImXDbvHnz9FrbULqtSZMm+dUuAAAAIDJlCd999136z/3797d7773XjdKee+657rbFixfbhAkTbNSoUblvAQAAAOBluG3atKk7E1kgEEi/TSdvyOymm25y9bgAAABAgQ23a9eujXxLAAAAAC/Cba1atfK6HAAAAKBgnsRh8+bN9sUXX9i2bdssLS0tw32qyQUAAABiItxOnjzZbr/9ditWrJhVrFjR1eIG6WfCLQAAAGIm3A4bNsweffRRGzp0qMXH53omMQAAACBicp1ODx48aN26dSPYAgAAIPbDbe/evW3GjBmRaQ0AAADgZVnCyJEj7aqrrrLZs2db48aNLSEhIcP9Y8aMyUt7AAAAAG/D7ccff2xnnHGGu575gDIAAAAgZsLt888/b6+//rrddtttkWkRAAAA4FXNbWJiop1//vmscAAAAMR+uL333ntt/PjxkWkNAAAA4GVZwpIlS2zevHk2a9Ysa9iwYZYDyt555528tAcAAADwLtyWK1fOunTpcuJLBAAAAApKuJ00aVJkWgIAAADkEefPBQAAQOEdua1du/Yx57P99ddf89omAAAAwJtwO2DAgAzXU1NTbfny5e6MZQ888MCJtQIAAACIRrjVVGDhTJgwwZYtW5YfbQIAAACiW3N7xRVX2Ntvv51fTwcAAABEL9z+z//8j1WoUCG/ng4AAACIfFlCs2bNMhxQFggEbMuWLbZ9+3Z76aWXct8CAAAAIFrhtnPnzhmux8fHW+XKle3iiy+2+vXr51e7AAAAgMiH2+HDh+d+KQAAAIAHYuokDqNGjXIlEaHTkR0+fNj69etnFStWtFKlSlnXrl1t69atUW0nAAAACni4VflBkSJFjnkpWjTXA8E5tnTpUnvllVfsrLPOynD7wIED7YMPPrAZM2bYggULbPPmzdalS5eItQMAAAAFV47T6MyZM7O9b9GiRTZu3DhLS0uzSNi/f791797d/vGPf9iTTz6ZfvuePXvstddes2nTptkll1zibps0aZI1aNDAFi9ebOeee25E2gMAAIAYD7edOnXKctuqVatsyJAhbuRU4fPxxx+3SFDZQceOHa19+/YZwm1SUpI7Q5puD9JBbTVr1nSBO7twm5KS4i5Be/fudf8rnEcqoIfSDBMqr4iP09B5IOLLc8uJj3fL9eL1FQZaj6zP2EX/xT76MPbRh/lLn0n6rI/3cbbI6XJOqI5Au/51YNkbb7xhHTp0sBUrVlijRo0sEqZPn27ffPONK0vITFOQFStWzMqVK5fh9qpVq7r7sjNy5EgbMWJElts1nZlqeCNNy6hbt66Vr1rCipSL/AaYaiXslBYt3HK3bdsW8eUVBnqDac9B8I8JYgv9F/vow9hHH+Yvfca3aNHCylUtYQnlvckWpzZvbkeOHHHZwovPwn379uV/uNWH+dNPP23jx4+3pk2b2ty5c+2CCy6wSNmwYYM73e+cOXOsePHi+fa8Q4cOtUGDBmUYua1Ro4ab0qxMmTIWaRs3brQ1a9bY5sSWlhD4/3MGR0rK1kO2JSnJrcMqVapEfHmF5Y+yRt+1zRBuYw/9F/vow9hHH+avTZs2uT3a1Rp2t0TzJlts++YbN8iobOHFZ2FOs2COw+3o0aPtmWeesWrVqtlbb70Vtkwhv6mT9G2gefPm6bcdPXrUFi5caC+++KJ9/PHH7hvD7t27M4zearYEtTM7iYmJ7pKZG873oHMUitwwfsAszYMN0C3n/8IYQSz/BNcn6zQ20X+xjz6MffRh/q5LV14ZiE628OKzMKfLyHG4VW1tiRIl3O50lSPoEs4777xj+aVdu3b2/fffZ7itZ8+erq72wQcfdKOtCQkJbgRZU4AF64DXr19vrVu3zrd2AAAAIDbkONzeeuutGU6764XSpUtnqeUtWbKkm9M2eHvv3r1diUGFChVcScE999zjgi0zJQAAABQ+OQ63kydPtoLohRdecMPUGrnVDAg6wO2ll16KdrMAAAAQBZE760KEfPbZZ1mKiydMmOAuAAAAKNyYwwgAAAC+QbgFAACAbxBuAQAA4BuEWwAAAPgG4RYAAAC+QbgFAACAbxBuAQAA4BuEWwAAAPgG4RYAAAC+QbgFAACAbxBuAQAA4BuEWwAAAPgG4RYAAAC+QbgFAACAbxBuAQAA4BuEWwAAAPgG4RYAAAC+QbgFAACAbxBuAQAA4BuEWwAAAPgG4RYAAAC+UTTaDQAAAPCj9evX244dOzxZVnJysifLiQWEWwAAgAgE2zPqN7DDhw6ybj1GuAUAAMhnGrFVsK141X2WULFGxNfvoV+X2Z7P34z4cmIB4RYAACBCFGwTq9WN+PpN3bkh4suIFRxQBgAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3ika7AfBOcnKyZ8uqVKmS1axZ07PlAQAACOG2EDi6f5dZXJzdfPPNni2zeImTbNVPyQRcAADgKcJtIZCWst8sELCKV91nCRVrRHx5qTs32M5Zz9uOHTsItwAAwFOE20JEwTaxWt1oNwMAACBiOKAMAAAAvkG4BQAAgG8QbgEAAOAbhFsAAAD4BuEWAAAAvkG4BQAAgG8U6HA7cuRIO+ecc6x06dJWpUoV69y5s61atSrDYw4fPmz9+vWzihUrWqlSpaxr1662devWqLUZAAAA0VOgw+2CBQtccF28eLHNmTPHUlNT7bLLLrMDBw6kP2bgwIH2wQcf2IwZM9zjN2/ebF26dIlquwEAABAdBfokDrNnz85wffLkyW4ENykpyS688ELbs2ePvfbaazZt2jS75JJL3GMmTZpkDRo0cIH43HPPjVLLAQAAEA0FOtxmpjArFSpUcP8r5Go0t3379umPqV+/vjvl66JFi7INtykpKe4StHfvXvd/Wlqau0RaIBCwuLg4i4/T0Hkg4stzy4mP93x5ep1erM9o0Ovy8+vzO/ov9tGHsc/vfajXFo3P3ngff9bndDkxE271ggYMGGDnn3++NWrUyN22ZcsWK1asmJUrVy7DY6tWreruO1Yt74gRI7Lcvn37dlfDG2laRt26da181RJWpFzkN8DDNSrYvhYtrFzVEpZQPvLLS7USdkqLFu51btu2zfxI26O+bAX/eCG20H+xjz6MfX7vQ30GtvDwszcan/WnNm9uR44ccZ/1XvThvn37/BVuVXu7cuVK++KLL/L8XEOHDrVBgwZlGLmtUaOGVa5c2cqUKWORtnHjRluzZo1tTmxpCYG4iC9v/4Y/bGdSklVr2N0SLfLLS9l6yLYkJVnx4sVdGYlf/yhr9F3bjB//KPsd/Rf76MPY5/c+3LRpk9vD7NVnbzQ+67d9840bZNRnvRd9qFzhm3B7991326xZs2zhwoV26qmnpt9erVo1941h9+7dGUZvNVuC7stOYmKiu2TmhvM96By9md0wfsAszYMN0C1HJRceL8+VXvjwD1ZQ8PX5+TX6Gf0X++jD2OfnPtRri8Znb1qUPuu96MOcLqNAb00KgAq2M2fOtHnz5lnt2rUz3K/h/oSEBJs7d276bZoqbP369da6desotBgAAADRVLSglyJoJoT33nvPzXUbrKMtW7aslShRwv3fu3dvV2Kgg8xUUnDPPfe4YMtMCQAAAIVPgQ63EydOdP9ffPHFGW7XdF+33Xab+/mFF15ww9Q6eYNmQOjQoYO99NJLUWkvAAAAoqtoQS9LyElx8YQJE9wFAAAAhVuBrrkFAAAAcoNwCwAAAN8g3AIAAMA3CLcAAADwDcItAAAAfINwCwAAAN8g3AIAAMA3CLcAAADwjQJ9EgcAQOxYv3697dixw7OT/BQrVsyqVKniyfIAxA7CLQAgX4LtGfUb2OFDBz1Zmzrt+rmtz7NpU9+0WrVqebJMALGBcAsAyDON2CrYVrzqPkuoWCPia/ToHxssdctCt1zCLYBQhFsAQL5RsE2sVjfiazQ1zsy2RHwxAGIQB5QBAADANwi3AAAA8A3KEgBE/ch3qVSpktWsWZPeAADkCeEWQNSPfJfiJU6yVT8lE3ABAHlCuAUQ9SPfU3dusJ2znnfLZfQWAJAXhFsAUT/yHQD8Vm6VnJzsyXKQFeEWAAD4XjTKrRAdhFsAAOB7XpdbHfp1me35/M2ILwdZEW4BAECh4dmJRnZuiPgyEB7z3AIAAMA3CLcAAADwDcItAAAAfIOaWwCF0vbt223Tpk0WFxfnyfI4AxsAeINwC6DQ2bBhg91x5122eNFXlpaW5skyOQMbAHiDcAugUE4JlHokxSp2HGhFKnAGNgDwE8ItgEI9JVBCVc7ABgB+wgFlAAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAA8A3CLQAAAHyDcAsAAADfINwCAADANwi3AAAA8A3OUAagwEhOTvZkOT/99JMnywEAeI9wCyDqju7fZRYXZzfffLMny4uPj7cWLVp4siwAgLcItwCiLi1lv1kgYBWvus8SKtaI+PJS1iaZHWL0FgD8iHALoMBQsE2sVjfiyzn6xwbCLVAAbN++3TZt2mRxcXG+KXtC9BFuAQCA5zZs2GB33HmXLV70laWlpdEDyDeEWwAA4LkdO3ZY6pEUq9hxoBWpEPlypEO/LrM9n78Z8eUg+gi38MUuoEqVKlnNmjU9Wx6AgkEzX3ixS1tSUlIsMTHRk2VFY3nR+luqcqSEqpEvR0rduSHiy0DBQLhFzB/5LsVLnGSrfkom4AKF6e+Mmd16663e7dKOizcLeLj73Ovl8bcUPkG4Rcwf+a5v4ztnPe92cTF6CxSivzNmnu/S9urvmtfLE/6Wwi8It4j5I98BhLd+/Xr3pc8L0ToS3etd2l79XfN6eYCfEG4BwKfB9oz6DezwoYPRbgoAeIpwCwA+pBFbBVuvd6MDQLQRbgHAx7zejQ4A0RYf7QYAAAAA+YVwCwAAAN8g3AIAAMA3fFNzO2HCBHv22Wdty5Yt1qRJExs/fry1bNky2s0C8k1hmNbJ77xcr/QhCvq2o7PLAZHgi3D773//2wYNGmQvv/yytWrVysaOHWsdOnSwVatWWZUqVaLdPCDPmNYptkXjrH1AQd9O4+PjrUWLFp4sC4WLL8LtmDFjrE+fPtazZ093XSH3ww8/tNdff92GDBkS7eYBeca0TrHN67P2CVNzoaBvpylrk8wOMXqL/Bfz4fbIkSOWlJRkQ4cOzfBtsH379rZo0aKwv5OSkuIuQXv27HH/796925NzlO/bt88tJ3XrGks7cjjiyzv6x0aLi4tzy7NU/y0v9f+Wp+1A69YLgUDAbXvFihVzy460n3/++X+X82eKJ+s07miqr7eZP//YFJX3oFf9J/Rh/vL739GobKdHU/ks9MFn7/79+11+UvaKtL1796Z/Bh9TIMZt2rRJrzDw1VdfZbj9gQceCLRs2TLs7wwfPtz9DhfWAdsA2wDbANsA2wDbANuAxdQ62LBhwzGzYcyP3J4IjfKqRjdIIzh//PGHVaxY0ZNROH3zqFGjhm3YsMHKlCkT8eUh/9GHsY3+i330YeyjD2PfXo/zjEZstYe2evXqx3xczIfbSpUqWZEiRWzr1q0Zbtf1atWqhf2dxMREdwlVrlw585o2BMJtbKMPYxv9F/vow9hHH8a+Mh7mmbJly/p/nlvVPOpoy7lz52YYidX11q1bR7VtAAAA8FbMj9yKSgx69OhhZ599tpvbVlOBHThwIH32BAAAABQOvgi3N9xwg23fvt0effRRdxKHpk2b2uzZs61q1apWEKkkYvjw4VlKIxA76MPYRv/FPvow9tGHsS+xgOaZOB1VFu1GAAAAAPkh5mtuAQAAgCDCLQAAAHyDcAsAAADfINwCAADANwi3HpswYYKddtppVrx4cWvVqpUtWbLE6yYgh0aOHGnnnHOOlS5d2qpUqWKdO3e2VatWZXjM4cOHrV+/fu7sdqVKlbKuXbtmOaEICoZRo0a5MxAOGDAg/Tb6r+DbtGmT3Xzzze49VqJECWvcuLEtW7Ys/X4dE62Zck4++WR3f/v27W316tVRbTP+v6NHj9qwYcOsdu3arn/q1KljTzzxhOu3IPqwYFm4cKFdffXV7ixg+pv57rvvZrg/J/2ls752797dndhBJ8nq3bu37d+/37PXQLj10L///W83J6+mzfjmm2+sSZMm1qFDB9u2bZuXzUAOLViwwAXXxYsX25w5cyw1NdUuu+wyN4dy0MCBA+2DDz6wGTNmuMdv3rzZunTpwjouYJYuXWqvvPKKnXXWWRlup/8Ktl27dtn5559vCQkJ9tFHH9mPP/5ozz//vJUvXz79MaNHj7Zx48bZyy+/bF9//bWVLFnS/V3VFxdE3zPPPGMTJ060F1980ZKTk9119dn48ePTH0MfFiwHDhxw+USDceHkpL8UbH/44Qf32Tlr1iwXmPv27evdi9BUYPBGy5YtA/369Uu/fvTo0UD16tUDI0eOpAtiwLZt2zTUEFiwYIG7vnv37kBCQkJgxowZ6Y9JTk52j1m0aFEUW4pQ+/btC9SrVy8wZ86cwEUXXRS499573e30X8H34IMPBtq0aZPt/WlpaYFq1aoFnn322fTb1K+JiYmBt956y6NW4lg6duwY6NWrV4bbunTpEujevbv7mT4s2MwsMHPmzPTrOemvH3/80f3e0qVL0x/z0UcfBeLi4gKbNm3ypN2M3HrkyJEjlpSU5Ibvg+Lj4931RYsWedUM5MGePXvc/xUqVHD/qz81mhvap/Xr17eaNWvSpwWIRt87duyYoZ+E/iv43n//fXfmyeuvv96VBjVr1sz+8Y9/pN+/du1ad+Ke0L7VeedV8sXf1YLhvPPOs7lz59rPP//srn/77bf2xRdf2BVXXOGu04exZW0O3nP6X6UIeu8G6fHKPBrp9YIvzlAWC3bs2OFqjzKfNU3Xf/rpp6i1CzmTlpbmajW1i7RRo0buNr3BixUr5t7EmftU9yH6pk+f7kqAVJaQGf1X8P36669ul7bKuR566CHXj/3793fvO51yPfg+C/d3lfdgwTBkyBDbu3ev++JfpEgR9zn41FNPud3WQh/Gli05eM/pf30ZDVW0aFE3MOTV+5JwC+Rw9G/lypVuxAGxYcOGDXbvvfe6mi8dwInY/FKp0Z+nn37aXdfIrd6HqvVTuEXB95///MemTp1q06ZNs4YNG9qKFSvcQIEOVqIPESmUJXikUqVK7ltr5iPpdb1atWpeNQMn4O6773YF8fPnz7dTTz01/Xb1m8pNdu/eneHx9GnBoLIDHazZvHlzN2qgiw7604EQ+lkjDfRfwaajsc8888wMtzVo0MDWr1/vfg7+7eTvasH1wAMPuNHbbt26uZkubrnlFncgp2ajEfowtlTLwXtO/2c+UP7PP/90Myh4lXcItx7RbrQWLVq42qPQUQldb926tVfNQC6oll7BdubMmTZv3jw3lU0o9aeO4g7tU00Vpg9e+jT62rVrZ99//70bKQpeNAqo3aHBn+m/gk1lQJmn31PtZq1atdzPek/qwzL0Pahd4Krr4z1YMBw8eNDVWobSQI8+/4Q+jC21c/Ce0/8a9NEAQ5A+Q9Xnqs31hCeHrcGZPn26O6Jw8uTJ7mjCvn37BsqVKxfYsmULa6gAuvPOOwNly5YNfPbZZ4Hff/89/XLw4MH0x9xxxx2BmjVrBubNmxdYtmxZoHXr1u6Cgil0tgSh/wq2JUuWBIoWLRp46qmnAqtXrw5MnTo1cNJJJwXefPPN9MeMGjXK/R197733At99912gU6dOgdq1awcOHToU1bbjf/Xo0SNwyimnBGbNmhVYu3Zt4J133glUqlQpMHjw4PRVRB8WvBlmli9f7i6KiWPGjHE/r1u3Lsf9dfnllweaNWsW+PrrrwNffPGFm7Hmxhtv9Ow1EG49Nn78eBeGihUr5qYGW7x4sddNQA7pTR3uMmnSpPTH6M181113BcqXL+8+dK+99loXgBEb4Zb+K/g++OCDQKNGjdzAQP369QOvvvpqhvs1NdGwYcMCVatWdY9p165dYNWqVVFrLzLau3eve8/pc6948eKBv/zlL4GHH344kJKSkv4Y+rBgmT9/ftjPPn1RyWl/7dy504XZUqVKBcqUKRPo2bOnC81eidM/3owRAwAAAJFFzS0AAAB8g3ALAAAA3yDcAgAAwDcItwAAAPANwi0AAAB8g3ALAAAA3yDcAgAAwDcItwAAAPANwi0AxJDHHnvMqlatanFxcfbuu++an912223WuXPnaDfD5s6daw0aNLCjR4+e8HPMnj3bmjZtamlpafnaNgBZEW4B5DmAKGjpUqxYMatbt649/vjj9ueffxb4NRtrATE5OdlGjBhhr7zyiv3+++92xRVXZHnMb7/95l5XlSpVbN++fRnuU7hSOM6p/HyuWDZ48GB75JFHrEiRIu768uXLrVmzZlaqVCm7+uqr7Y8//kh/rLb7Fi1a2JIlSzI8x+WXX24JCQk2depUz9sPFDaEWwB5pg9uha3Vq1fbfffd50LPs88+e0LPpdExRrfC++WXX9z/nTp1smrVqlliYmK261Fh9LnnnjuhPojkc8WaL774wq33rl27pt/2t7/9zS655BL75ptvbM+ePfb000+n3/f888/b+eefby1btgz7RXDcuHGetR0orAi3APJMIUthq1atWnbnnXda+/bt7f3333f3paSk2P3332+nnHKKlSxZ0lq1amWfffZZ+u9OnjzZypUr5x5/5plnuudav369+70HH3zQatSo4W7TiPBrr72W/nsrV650I5caPdNu+ltuucV27NiRfv/FF19s/fv3d6NuFSpUcO0LHWk87bTT3P/XXnutG50MXleQUXjUc+q5zznnHPv0008zvF4F+Y4dO1qJEiWsdu3aNm3aNPf7Y8eOTX/M7t27XQiqXLmylSlTxoWhb7/99pjr8fvvv3eP0/NWrFjR+vbta/v373f3qe0aJZT4+HjX5mO55557bMyYMbZt27ZsH/Ovf/3Lzj77bCtdurRbPzfddFPYx+fkuUL9/PPPrn0//fRThttfeOEFq1OnTvqXmN69e7v1p9d7xhln2N///vdjPm/mdRxuBPl4610/t23b1r1m3a9R1mXLlmW7zOnTp9ull15qxYsXzzCC3qdPHzv99NPtxhtvdNfl119/ddvoU089Ffa51H9aVvBLCoDIINwCyHcKK0eOHHE/33333bZo0SIXEr777ju7/vrr3UivRnmDDh48aM8884z985//tB9++MHtBr/11lvtrbfeciNdCg/aFa+wGQwwCi3aNaywoHrGrVu32l//+tcM7XjjjTdcoP76669t9OjRrlxizpw57r6lS5e6/ydNmuTCavC6wuSVV17p6iy1+1ltVShR4A5S2zZv3uxC+ttvv22vvvpqluCn16nbPvroI0tKSrLmzZtbu3btMuzCDnXgwAHr0KGDlS9f3rVlxowZLlRr/Ym+IKitovbqciwKXcESkeykpqbaE0884QKfyjNUhqDRxRN5rlAKfQrNmXfB67oCtGh0/tRTT3Wv88cff7RHH33UHnroIfvPf/5jeXG89d69e3e3XK1j3T9kyBBXLpCdzz//3L2WUE2aNHHbkUoQtJ2cddZZ7vY77rjDbWcKzuHUrFnTfWnScwKIoAAA5EGPHj0CnTp1cj+npaUF5syZE0hMTAzcf//9gXXr1gWKFCkS2LRpU4bfadeuXWDo0KHu50mTJgX0p2jFihXp969atcrdpucK54knnghcdtllGW7bsGGD+x39rlx00UWBNm3aZHjMOeecE3jwwQfTr+vxM2fOPO5rbNiwYWD8+PHu5+TkZPd7S5cuTb9/9erV7rYXXnjBXf/8888DZcqUCRw+fDjD89SpUyfwyiuvhF3Gq6++Gihfvnxg//796bd9+OGHgfj4+MCWLVvcdbX1eH+2165d6x6zfPnywOzZswMJCQmBNWvWuPuaNGkSGD58eLa/q9ek3923b1+en0vrQq83c59q/WWnX79+ga5du4bdtqRWrVrp6zgotB05We+lS5cOTJ48OZBTZcuWDUyZMiXDbStXrgxceOGFgZo1awZuvPHGwJ49e9xj1NaNGze6bVPLfPjhh7M8X7NmzQKPPfZYjpcPIPcYuQWQZ7NmzXKjqtp1q1KBG264we0q1m527X7WSJ7uD14WLFiQYdesDkQLjn7JihUr3ME7F110UdjlaaRx/vz5GZ6zfv367r7Q5w19Tjn55JOPu2tdI7caJdXR8SqX0HNr5Dg4crtq1SorWrSoGxEM0qimRlxD26fnUWlBaBvXrl2b7S5pLUMjghppDlLtpkY4tcwToZHgNm3a2LBhw8Ler5FLjUprRFGjjcH1HTpKndPnyqxbt25uJHjx4sXpo7ZaZ8F+kgkTJriyAJUQaP1oBDzcsnMqJ+t90KBBrmxBpTOjRo06bonAoUOHMpQkSMOGDd02vG7dOleSohHw4cOH24svvuhKOM477zzXlnfeecc++OCDLHs1tKcCQOQUjeBzAygkVMM4ceJEF1KrV6/uwp8oaCikKkQFjzQPCpYYBD/wQ2tIdf1Y9LwKZSplyEwBNijz7mYt43gHqynYapezDqBSaFVbrrvuuvQyi5xQ+9SO0NriIAVmLynAtW7d2h544IGwZRC6KHgqYCpY6np2rzW75wpHNbwqHVH4O/fcc93/qscOUpmK1rUOwNJzKlzrIESVkGRHtcb/O+D+/ylY5ma960uXSiM+/PBDV7qgUKq2qPY6nEqVKtmuXbuO+VoVmAcMGODKHbTsJ5980n1JUV22rgdrpUXlEVrXACKHcAsgz/RBriCYmWpiNXKr0dILLrggx8/XuHFjF0I1OqYRtsw0AqhaVx1gFAzSJ0LhN/PcpV9++aWrOw2GHQUmjUAG6cAn1VqqHlejjrJmzZoMAUjt27Jli2tb8EC149FIsQ6uU+gMjt6qLQp0WuaJ0lH7Xbp0cbWloXSw186dO11g1UF7cqwDq471XNlRfasO6FPNrg620mhukF6bRjjvuuuu9NuON4qqUBhaa7x37143Kpvb9a49CboMHDjQtU21zNmFW23DqgnOjmpuNeoerIfW9hQM3KHBWw4fPuxeo54TQORQlgAgYhQgFHB0AJZ20SqIaP7PkSNHupGz7CiY9OjRw3r16uUOdNLvaQQseLBRv3793AiYgokODFJg+Pjjj61nz565mmhfy1E4USAKhtN69eq5tqo0QruWNcoXOtqr3eoK3JrJQK9FIVc/h44+636NRuoEBJ988okLx1999ZU9/PDD2QZIrSft/tbr1kwQKrvQLm7NAqGDkPJCR+/PmzcvQ3mDShE00j5+/HgXPDVbhQ4uO5Hnyo6CsKYR04itRvc1qh+k9ax1oX7T7Aoqdwge1JcdjQRrhgcdkKWSF62r0D0Cx1vvKjHQAXrallRSoICtZeqLRXY0kq3pwMJRWNXzqZxCX0KCpSQqt9C2oy9guh6kEg3N/KE2Aogcwi2AiNKIlsKt5r/VCKSChwKFwtWxqMxB5QAa2VOg1NRLGtUUhSQFEwXZyy67zI30arewdj0HQ0ZOaJe4ShA0chkcTdOUV6qf1aiidicr3ITW18qUKVNc4LzwwgvdiJ/apt3qwdpMhdz//ve/7n4FboV8jVoqUGUXVE866SQX9BTaNf2YXruO8lcdZ15p+fqioDAWOgqqkWLNVqAp2DSCm5O5bMM9V3a0TrQOFfQU3kPdfvvtLvyqPlvTw2kUOXQUN5yhQ4e6uuCrrrrK7fLXthScWiwn611BWMvR9qj7NLuGasR1YozsqN2awSNcmNfvqR2ajixIs3voi5HaoNceOj+uZv/Q86mvAUROnI4qi+DzA4Dvbdy40QVkTd2lQAp/UY2xSiA0Hd2J0hzM+nKnEWTN7Qsgchi5BYBc0m557cZXuYR2e2t0UCUOGq2D/6isQScoycuZ81Qi8dJLLxFsAQ8wcgsAuaTyAZVZqFZVu95VwqAzZykAAQCii3ALAAAA36AsAQAAAL5BuAUAAIBvEG4BAADgG4RbAAAA+AbhFgAAAL5BuAUAAIBvEG4BAADgG4RbAAAAmF/8P0tSfOVZ0he8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Compute percentage of NaNs per column\n",
    "nan_percentage = np.isnan(X_train_data).sum(axis=0) / X_train_data.shape[0] * 100\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(nan_percentage, bins=20, edgecolor='black')\n",
    "plt.title(\"Histogram of NaN Percentage per Column\")\n",
    "plt.xlabel(\"Percentage of NaN values (%)\")\n",
    "plt.ylabel(\"Number of Columns\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f7db29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start cleaning batch 1 out of 5\n",
      "Number of removes features due to NaN values: 178\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.480217\n",
      "Iteration   200, loss = 0.479708\n",
      "Iteration   300, loss = 0.479593\n",
      "Iteration   400, loss = 0.479547\n",
      "Iteration   500, loss = 0.479526\n",
      "Iteration   600, loss = 0.479515\n",
      "Iteration   700, loss = 0.479509\n",
      "Iteration   800, loss = 0.479505\n",
      "Iteration   900, loss = 0.479503\n",
      "Iteration  1000, loss = 0.479501\n",
      "Converged at iteration 1096\n",
      " Accuracy: 85.18%\n",
      " F1 Score: 0.4084\n",
      "Start cleaning batch 2 out of 5\n",
      "Number of removes features due to NaN values: 178\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.480586\n",
      "Iteration   200, loss = 0.480027\n",
      "Iteration   300, loss = 0.479884\n",
      "Iteration   400, loss = 0.479827\n",
      "Iteration   500, loss = 0.479800\n",
      "Iteration   600, loss = 0.479786\n",
      "Iteration   700, loss = 0.479778\n",
      "Iteration   800, loss = 0.479773\n",
      "Iteration   900, loss = 0.479770\n",
      "Iteration  1000, loss = 0.479768\n",
      "Iteration  1100, loss = 0.479767\n",
      "Converged at iteration 1194\n",
      " Accuracy: 85.29%\n",
      " F1 Score: 0.4086\n",
      "Start cleaning batch 3 out of 5\n",
      "Number of removes features due to NaN values: 178\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.480763\n",
      "Iteration   200, loss = 0.480175\n",
      "Iteration   300, loss = 0.480024\n",
      "Iteration   400, loss = 0.479962\n",
      "Iteration   500, loss = 0.479934\n",
      "Iteration   600, loss = 0.479920\n",
      "Iteration   700, loss = 0.479912\n",
      "Iteration   800, loss = 0.479908\n",
      "Iteration   900, loss = 0.479905\n",
      "Iteration  1000, loss = 0.479904\n",
      "Converged at iteration 1095\n",
      " Accuracy: 85.32%\n",
      " F1 Score: 0.4096\n",
      "Start cleaning batch 4 out of 5\n",
      "Number of removes features due to NaN values: 178\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.474108\n",
      "Iteration   200, loss = 0.473511\n",
      "Iteration   300, loss = 0.473366\n",
      "Iteration   400, loss = 0.473307\n",
      "Iteration   500, loss = 0.473280\n",
      "Iteration   600, loss = 0.473267\n",
      "Iteration   700, loss = 0.473259\n",
      "Iteration   800, loss = 0.473255\n",
      "Iteration   900, loss = 0.473252\n",
      "Iteration  1000, loss = 0.473250\n",
      "Iteration  1100, loss = 0.473248\n",
      "Converged at iteration 1141\n",
      " Accuracy: 85.05%\n",
      " F1 Score: 0.4083\n",
      "Start cleaning batch 5 out of 5\n",
      "Number of removes features due to NaN values: 178\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.477520\n",
      "Iteration   200, loss = 0.477025\n",
      "Iteration   300, loss = 0.476900\n",
      "Iteration   400, loss = 0.476846\n",
      "Iteration   500, loss = 0.476819\n",
      "Iteration   600, loss = 0.476805\n",
      "Iteration   700, loss = 0.476797\n",
      "Iteration   800, loss = 0.476791\n",
      "Iteration   900, loss = 0.476788\n",
      "Iteration  1000, loss = 0.476786\n",
      "Iteration  1100, loss = 0.476784\n",
      "Converged at iteration 1183\n",
      " Accuracy: 85.12%\n",
      " F1 Score: 0.4008\n",
      "\n",
      "With 20% NaN threshold accuracy: 0.8519115607905283 F1 Score: 0.40715678097308483\n"
     ]
    }
   ],
   "source": [
    "\"For 20% NaN threshold\"\n",
    "\n",
    "def no_NaN_20(x_train, x_val, y_train, y_val):\n",
    "    \n",
    "    return no_NaN_features(x_train, x_val, y_train, y_val, threshold=0.2)\n",
    "\n",
    "acc_20_NaN, f1_20_NaN = kfold_logistic_ridge(X_train_data, Y_train_data, no_NaN_20, k=5, gamma=0.5, lambda_=1e-3, alpha = 0.7)\n",
    "print(\"\\nWith 20% NaN threshold accuracy:\", acc_20_NaN, \"F1 Score:\", f1_20_NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "621c984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start cleaning batch 1 out of 5\n",
      "Number of removes features due to NaN values: 117\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.479010\n",
      "Iteration   200, loss = 0.478374\n",
      "Iteration   300, loss = 0.478230\n",
      "Iteration   400, loss = 0.478174\n",
      "Iteration   500, loss = 0.478148\n",
      "Iteration   600, loss = 0.478135\n",
      "Iteration   700, loss = 0.478127\n",
      "Iteration   800, loss = 0.478123\n",
      "Iteration   900, loss = 0.478120\n",
      "Iteration  1000, loss = 0.478118\n",
      "Iteration  1100, loss = 0.478116\n",
      "Converged at iteration 1145\n",
      " Accuracy: 85.18%\n",
      " F1 Score: 0.4087\n",
      "Start cleaning batch 2 out of 5\n",
      "Number of removes features due to NaN values: 117\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.479331\n",
      "Iteration   200, loss = 0.478630\n",
      "Iteration   300, loss = 0.478455\n",
      "Iteration   400, loss = 0.478386\n",
      "Iteration   500, loss = 0.478355\n",
      "Iteration   600, loss = 0.478339\n",
      "Iteration   700, loss = 0.478330\n",
      "Iteration   800, loss = 0.478325\n",
      "Iteration   900, loss = 0.478322\n",
      "Iteration  1000, loss = 0.478319\n",
      "Iteration  1100, loss = 0.478318\n",
      "Converged at iteration 1183\n",
      " Accuracy: 85.28%\n",
      " F1 Score: 0.4087\n",
      "Start cleaning batch 3 out of 5\n",
      "Number of removes features due to NaN values: 117\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.479527\n",
      "Iteration   200, loss = 0.478814\n",
      "Iteration   300, loss = 0.478629\n",
      "Iteration   400, loss = 0.478552\n",
      "Iteration   500, loss = 0.478515\n",
      "Iteration   600, loss = 0.478496\n",
      "Iteration   700, loss = 0.478486\n",
      "Iteration   800, loss = 0.478479\n",
      "Iteration   900, loss = 0.478475\n",
      "Iteration  1000, loss = 0.478473\n",
      "Iteration  1100, loss = 0.478471\n",
      "Iteration  1200, loss = 0.478470\n",
      "Converged at iteration 1200\n",
      " Accuracy: 85.36%\n",
      " F1 Score: 0.4107\n",
      "Start cleaning batch 4 out of 5\n",
      "Number of removes features due to NaN values: 117\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.472990\n",
      "Iteration   200, loss = 0.472268\n",
      "Iteration   300, loss = 0.472091\n",
      "Iteration   400, loss = 0.472018\n",
      "Iteration   500, loss = 0.471983\n",
      "Iteration   600, loss = 0.471965\n",
      "Iteration   700, loss = 0.471954\n",
      "Iteration   800, loss = 0.471948\n",
      "Iteration   900, loss = 0.471944\n",
      "Iteration  1000, loss = 0.471941\n",
      "Iteration  1100, loss = 0.471939\n",
      "Iteration  1200, loss = 0.471938\n",
      "Converged at iteration 1231\n",
      " Accuracy: 85.08%\n",
      " F1 Score: 0.4104\n",
      "Start cleaning batch 5 out of 5\n",
      "Number of removes features due to NaN values: 117\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.476228\n",
      "Iteration   200, loss = 0.475617\n",
      "Iteration   300, loss = 0.475470\n",
      "Iteration   400, loss = 0.475407\n",
      "Iteration   500, loss = 0.475375\n",
      "Iteration   600, loss = 0.475358\n",
      "Iteration   700, loss = 0.475348\n",
      "Iteration   800, loss = 0.475342\n",
      "Iteration   900, loss = 0.475338\n",
      "Iteration  1000, loss = 0.475335\n",
      "Iteration  1100, loss = 0.475333\n",
      "Iteration  1200, loss = 0.475332\n",
      "Converged at iteration 1232\n",
      " Accuracy: 84.97%\n",
      " F1 Score: 0.3980\n",
      "\n",
      "With 75% NaN threshold accuracy: 0.851765279534338 F1 Score: 0.4072961185753517\n"
     ]
    }
   ],
   "source": [
    "\"For 75% NaN threshold\"\n",
    "\n",
    "def no_NaN_75(x_train, x_val, y_train, y_val):\n",
    "    \n",
    "    return no_NaN_features(x_train, x_val, y_train, y_val, threshold=0.75)\n",
    "\n",
    "acc_75_NaN, f1_75_NaN = kfold_logistic_ridge(X_train_data, Y_train_data, no_NaN_75, k=5, gamma=0.5, lambda_=1e-3, alpha = 0.7)\n",
    "print(\"\\nWith 75% NaN threshold accuracy:\", acc_75_NaN, \"F1 Score:\", f1_75_NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b559af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start cleaning batch 1 out of 5\n",
      "Number of removes features due to NaN values: 206\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.488448\n",
      "Iteration   200, loss = 0.488064\n",
      "Iteration   300, loss = 0.487994\n",
      "Iteration   400, loss = 0.487965\n",
      "Iteration   500, loss = 0.487950\n",
      "Iteration   600, loss = 0.487942\n",
      "Iteration   700, loss = 0.487937\n",
      "Iteration   800, loss = 0.487935\n",
      "Iteration   900, loss = 0.487933\n",
      "Iteration  1000, loss = 0.487931\n",
      "Converged at iteration 1014\n",
      " Accuracy: 85.03%\n",
      " F1 Score: 0.3991\n",
      "Start cleaning batch 2 out of 5\n",
      "Number of removes features due to NaN values: 206\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.489224\n",
      "Iteration   200, loss = 0.488876\n",
      "Iteration   300, loss = 0.488805\n",
      "Iteration   400, loss = 0.488773\n",
      "Iteration   500, loss = 0.488757\n",
      "Iteration   600, loss = 0.488747\n",
      "Iteration   700, loss = 0.488742\n",
      "Iteration   800, loss = 0.488738\n",
      "Iteration   900, loss = 0.488736\n",
      "Iteration  1000, loss = 0.488734\n",
      "Iteration  1100, loss = 0.488733\n",
      "Converged at iteration 1122\n",
      " Accuracy: 85.12%\n",
      " F1 Score: 0.4010\n",
      "Start cleaning batch 3 out of 5\n",
      "Number of removes features due to NaN values: 206\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.489241\n",
      "Iteration   200, loss = 0.488832\n",
      "Iteration   300, loss = 0.488739\n",
      "Iteration   400, loss = 0.488697\n",
      "Iteration   500, loss = 0.488676\n",
      "Iteration   600, loss = 0.488665\n",
      "Iteration   700, loss = 0.488658\n",
      "Iteration   800, loss = 0.488655\n",
      "Iteration   900, loss = 0.488652\n",
      "Iteration  1000, loss = 0.488651\n",
      "Converged at iteration 1072\n",
      " Accuracy: 85.17%\n",
      " F1 Score: 0.4020\n",
      "Start cleaning batch 4 out of 5\n",
      "Number of removes features due to NaN values: 206\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.482114\n",
      "Iteration   200, loss = 0.481676\n",
      "Iteration   300, loss = 0.481586\n",
      "Iteration   400, loss = 0.481547\n",
      "Iteration   500, loss = 0.481527\n",
      "Iteration   600, loss = 0.481517\n",
      "Iteration   700, loss = 0.481511\n",
      "Iteration   800, loss = 0.481507\n",
      "Iteration   900, loss = 0.481504\n",
      "Iteration  1000, loss = 0.481503\n",
      "Iteration  1100, loss = 0.481502\n",
      "Converged at iteration 1113\n",
      " Accuracy: 84.74%\n",
      " F1 Score: 0.3977\n",
      "Start cleaning batch 5 out of 5\n",
      "Number of removes features due to NaN values: 206\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.485181\n",
      "Iteration   200, loss = 0.484817\n",
      "Iteration   300, loss = 0.484740\n",
      "Iteration   400, loss = 0.484704\n",
      "Iteration   500, loss = 0.484685\n",
      "Iteration   600, loss = 0.484675\n",
      "Iteration   700, loss = 0.484668\n",
      "Iteration   800, loss = 0.484665\n",
      "Iteration   900, loss = 0.484662\n",
      "Iteration  1000, loss = 0.484660\n",
      "Converged at iteration 1090\n",
      " Accuracy: 84.80%\n",
      " F1 Score: 0.3903\n",
      "\n",
      "With 20% NaN threshold accuracy: 0.8497051518429914 F1 Score: 0.39803061810632534\n"
     ]
    }
   ],
   "source": [
    "\"For 5% NaN threshold\"\n",
    "\n",
    "def no_NaN_5(x_train, x_val, y_train, y_val):\n",
    "    \n",
    "    return no_NaN_features(x_train, x_val, y_train, y_val, threshold=0.05)\n",
    "\n",
    "acc_5_NaN, f1_5_NaN = kfold_logistic_ridge(X_train_data, Y_train_data, no_NaN_5, k=5, gamma=0.5, lambda_=1e-3, alpha = 0.7)\n",
    "print(\"\\nWith 20% NaN threshold accuracy:\", acc_5_NaN, \"F1 Score:\", f1_5_NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f154804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With NaN features accuracy: 0.8363265119539214 F1 Score: 0.3940022237887491\n",
      "Without more than 5% NaN features accuracy: 0.8497051518429914 F1 Score: 0.39803061810632534\n",
      "Without more than 20% NaN threshold accuracy: 0.8519115607905283 F1 Score: 0.40715678097308483\n",
      "Without more than 30% NaN features accuracy: 0.8518963231596752 F1 Score: 0.4071321665799156\n",
      "Without more than 75% NaN threshold accuracy: 0.851765279534338 F1 Score: 0.4072961185753517\n"
     ]
    }
   ],
   "source": [
    "print(\"With NaN features accuracy:\", acc_NaN, \"F1 Score:\", f1_NaN)\n",
    "print(\"Without more than 5% NaN features accuracy:\", acc_5_NaN, \"F1 Score:\", f1_5_NaN)\n",
    "print(\"Without more than 20% NaN threshold accuracy:\", acc_20_NaN, \"F1 Score:\", f1_20_NaN)\n",
    "print(\"Without more than 30% NaN features accuracy:\", acc_no_NaN, \"F1 Score:\", f1_no_NaN)\n",
    "print(\"Without more than 75% NaN threshold accuracy:\", acc_75_NaN, \"F1 Score:\", f1_75_NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c0122d",
   "metadata": {},
   "source": [
    "As we see, it seems that the features with more than 20% of NaNs aren't improving our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fff07",
   "metadata": {},
   "source": [
    "## Effect of overfitting\n",
    "Now we will check if this is due to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08029fc",
   "metadata": {},
   "source": [
    "# Alpha choice\n",
    "Now we will use the previous section to compare the effect of the alpha threshold for the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ca4f74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.221028\n",
      "Iteration   200, loss = 0.218579\n",
      "Iteration   300, loss = 0.218082\n",
      "Iteration   400, loss = 0.217874\n",
      "Iteration   500, loss = 0.217757\n",
      "Iteration   600, loss = 0.217683\n",
      "Iteration   700, loss = 0.217633\n",
      "Iteration   800, loss = 0.217598\n",
      "Iteration   900, loss = 0.217574\n",
      "Iteration  1000, loss = 0.217557\n",
      "Iteration  1100, loss = 0.217545\n",
      "Iteration  1200, loss = 0.217536\n",
      "Iteration  1300, loss = 0.217529\n",
      "Iteration  1400, loss = 0.217525\n",
      "Iteration  1500, loss = 0.217521\n",
      "Iteration  1600, loss = 0.217519\n",
      "Iteration  1700, loss = 0.217517\n",
      "Iteration  1800, loss = 0.217515\n",
      "Converged at iteration 1873\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.216906\n",
      "Iteration   200, loss = 0.214285\n",
      "Iteration   300, loss = 0.213749\n",
      "Iteration   400, loss = 0.213540\n",
      "Iteration   500, loss = 0.213429\n",
      "Iteration   600, loss = 0.213362\n",
      "Iteration   700, loss = 0.213318\n",
      "Iteration   800, loss = 0.213289\n",
      "Iteration   900, loss = 0.213268\n",
      "Iteration  1000, loss = 0.213254\n",
      "Iteration  1100, loss = 0.213243\n",
      "Iteration  1200, loss = 0.213236\n",
      "Iteration  1300, loss = 0.213231\n",
      "Iteration  1400, loss = 0.213227\n",
      "Iteration  1500, loss = 0.213224\n",
      "Iteration  1600, loss = 0.213221\n",
      "Iteration  1700, loss = 0.213220\n",
      "Iteration  1800, loss = 0.213218\n",
      "Converged at iteration 1832\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218420\n",
      "Iteration   200, loss = 0.215899\n",
      "Iteration   300, loss = 0.215423\n",
      "Iteration   400, loss = 0.215247\n",
      "Iteration   500, loss = 0.215158\n",
      "Iteration   600, loss = 0.215105\n",
      "Iteration   700, loss = 0.215071\n",
      "Iteration   800, loss = 0.215048\n",
      "Iteration   900, loss = 0.215033\n",
      "Iteration  1000, loss = 0.215022\n",
      "Iteration  1100, loss = 0.215014\n",
      "Iteration  1200, loss = 0.215008\n",
      "Iteration  1300, loss = 0.215004\n",
      "Iteration  1400, loss = 0.215002\n",
      "Iteration  1500, loss = 0.214999\n",
      "Iteration  1600, loss = 0.214998\n",
      "Iteration  1700, loss = 0.214997\n",
      "Converged at iteration 1710\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.222193\n",
      "Iteration   200, loss = 0.219767\n",
      "Iteration   300, loss = 0.219275\n",
      "Iteration   400, loss = 0.219076\n",
      "Iteration   500, loss = 0.218966\n",
      "Iteration   600, loss = 0.218897\n",
      "Iteration   700, loss = 0.218850\n",
      "Iteration   800, loss = 0.218817\n",
      "Iteration   900, loss = 0.218794\n",
      "Iteration  1000, loss = 0.218778\n",
      "Iteration  1100, loss = 0.218766\n",
      "Iteration  1200, loss = 0.218757\n",
      "Iteration  1300, loss = 0.218750\n",
      "Iteration  1400, loss = 0.218745\n",
      "Iteration  1500, loss = 0.218742\n",
      "Iteration  1600, loss = 0.218739\n",
      "Iteration  1700, loss = 0.218737\n",
      "Iteration  1800, loss = 0.218735\n",
      "Iteration  1900, loss = 0.218734\n",
      "Converged at iteration 1910\n",
      " Accuracy: 8.50%\n",
      " F1 Score: 0.1567\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218887\n",
      "Iteration   200, loss = 0.216287\n",
      "Iteration   300, loss = 0.215803\n",
      "Iteration   400, loss = 0.215630\n",
      "Iteration   500, loss = 0.215543\n",
      "Iteration   600, loss = 0.215492\n",
      "Iteration   700, loss = 0.215458\n",
      "Iteration   800, loss = 0.215434\n",
      "Iteration   900, loss = 0.215418\n",
      "Iteration  1000, loss = 0.215406\n",
      "Iteration  1100, loss = 0.215397\n",
      "Iteration  1200, loss = 0.215391\n",
      "Iteration  1300, loss = 0.215386\n",
      "Iteration  1400, loss = 0.215382\n",
      "Iteration  1500, loss = 0.215380\n",
      "Iteration  1600, loss = 0.215378\n",
      "Iteration  1700, loss = 0.215376\n",
      "Iteration  1800, loss = 0.215375\n",
      "Converged at iteration 1810\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.382847\n",
      "Iteration   200, loss = 0.377677\n",
      "Iteration   300, loss = 0.375503\n",
      "Iteration   400, loss = 0.374279\n",
      "Iteration   500, loss = 0.373522\n",
      "Iteration   600, loss = 0.373031\n",
      "Iteration   700, loss = 0.372701\n",
      "Iteration   800, loss = 0.372475\n",
      "Iteration   900, loss = 0.372318\n",
      "Iteration  1000, loss = 0.372206\n",
      "Iteration  1100, loss = 0.372126\n",
      "Iteration  1200, loss = 0.372068\n",
      "Iteration  1300, loss = 0.372026\n",
      "Iteration  1400, loss = 0.371995\n",
      "Iteration  1500, loss = 0.371972\n",
      "Iteration  1600, loss = 0.371955\n",
      "Iteration  1700, loss = 0.371942\n",
      "Iteration  1800, loss = 0.371933\n",
      "Iteration  1900, loss = 0.371926\n",
      "Iteration  2000, loss = 0.371920\n",
      "Iteration  2100, loss = 0.371916\n",
      "Iteration  2200, loss = 0.371913\n",
      "Iteration  2300, loss = 0.371911\n",
      "Iteration  2400, loss = 0.371909\n",
      "Iteration  2500, loss = 0.371907\n",
      "Converged at iteration 2581\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.364363\n",
      "Iteration   200, loss = 0.357849\n",
      "Iteration   300, loss = 0.355491\n",
      "Iteration   400, loss = 0.354365\n",
      "Iteration   500, loss = 0.353749\n",
      "Iteration   600, loss = 0.353383\n",
      "Iteration   700, loss = 0.353152\n",
      "Iteration   800, loss = 0.353000\n",
      "Iteration   900, loss = 0.352897\n",
      "Iteration  1000, loss = 0.352825\n",
      "Iteration  1100, loss = 0.352774\n",
      "Iteration  1200, loss = 0.352737\n",
      "Iteration  1300, loss = 0.352710\n",
      "Iteration  1400, loss = 0.352691\n",
      "Iteration  1500, loss = 0.352676\n",
      "Iteration  1600, loss = 0.352665\n",
      "Iteration  1700, loss = 0.352656\n",
      "Iteration  1800, loss = 0.352650\n",
      "Iteration  1900, loss = 0.352645\n",
      "Iteration  2000, loss = 0.352641\n",
      "Iteration  2100, loss = 0.352638\n",
      "Iteration  2200, loss = 0.352636\n",
      "Iteration  2300, loss = 0.352634\n",
      "Iteration  2400, loss = 0.352633\n",
      "Converged at iteration 2462\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.367902\n",
      "Iteration   200, loss = 0.363557\n",
      "Iteration   300, loss = 0.362158\n",
      "Iteration   400, loss = 0.361496\n",
      "Iteration   500, loss = 0.361131\n",
      "Iteration   600, loss = 0.360912\n",
      "Iteration   700, loss = 0.360772\n",
      "Iteration   800, loss = 0.360679\n",
      "Iteration   900, loss = 0.360615\n",
      "Iteration  1000, loss = 0.360570\n",
      "Iteration  1100, loss = 0.360538\n",
      "Iteration  1200, loss = 0.360515\n",
      "Iteration  1300, loss = 0.360497\n",
      "Iteration  1400, loss = 0.360485\n",
      "Iteration  1500, loss = 0.360475\n",
      "Iteration  1600, loss = 0.360468\n",
      "Iteration  1700, loss = 0.360463\n",
      "Iteration  1800, loss = 0.360459\n",
      "Iteration  1900, loss = 0.360456\n",
      "Iteration  2000, loss = 0.360454\n",
      "Iteration  2100, loss = 0.360452\n",
      "Iteration  2200, loss = 0.360451\n",
      "Converged at iteration 2257\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.373653\n",
      "Iteration   200, loss = 0.367876\n",
      "Iteration   300, loss = 0.365844\n",
      "Iteration   400, loss = 0.364835\n",
      "Iteration   500, loss = 0.364256\n",
      "Iteration   600, loss = 0.363894\n",
      "Iteration   700, loss = 0.363655\n",
      "Iteration   800, loss = 0.363492\n",
      "Iteration   900, loss = 0.363378\n",
      "Iteration  1000, loss = 0.363295\n",
      "Iteration  1100, loss = 0.363236\n",
      "Iteration  1200, loss = 0.363192\n",
      "Iteration  1300, loss = 0.363160\n",
      "Iteration  1400, loss = 0.363135\n",
      "Iteration  1500, loss = 0.363117\n",
      "Iteration  1600, loss = 0.363104\n",
      "Iteration  1700, loss = 0.363093\n",
      "Iteration  1800, loss = 0.363085\n",
      "Iteration  1900, loss = 0.363079\n",
      "Iteration  2000, loss = 0.363075\n",
      "Iteration  2100, loss = 0.363071\n",
      "Iteration  2200, loss = 0.363068\n",
      "Iteration  2300, loss = 0.363066\n",
      "Iteration  2400, loss = 0.363065\n",
      "Iteration  2500, loss = 0.363063\n",
      "Converged at iteration 2536\n",
      " Accuracy: 8.50%\n",
      " F1 Score: 0.1567\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.387809\n",
      "Iteration   200, loss = 0.382998\n",
      "Iteration   300, loss = 0.381450\n",
      "Iteration   400, loss = 0.380730\n",
      "Iteration   500, loss = 0.380329\n",
      "Iteration   600, loss = 0.380079\n",
      "Iteration   700, loss = 0.379912\n",
      "Iteration   800, loss = 0.379794\n",
      "Iteration   900, loss = 0.379710\n",
      "Iteration  1000, loss = 0.379648\n",
      "Iteration  1100, loss = 0.379602\n",
      "Iteration  1200, loss = 0.379567\n",
      "Iteration  1300, loss = 0.379540\n",
      "Iteration  1400, loss = 0.379520\n",
      "Iteration  1500, loss = 0.379505\n",
      "Iteration  1600, loss = 0.379493\n",
      "Iteration  1700, loss = 0.379484\n",
      "Iteration  1800, loss = 0.379477\n",
      "Iteration  1900, loss = 0.379472\n",
      "Iteration  2000, loss = 0.379468\n",
      "Iteration  2100, loss = 0.379465\n",
      "Iteration  2200, loss = 0.379462\n",
      "Iteration  2300, loss = 0.379460\n",
      "Iteration  2400, loss = 0.379459\n",
      "Iteration  2500, loss = 0.379457\n",
      "Converged at iteration 2521\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.221028\n",
      "Iteration   200, loss = 0.218579\n",
      "Iteration   300, loss = 0.218082\n",
      "Iteration   400, loss = 0.217874\n",
      "Iteration   500, loss = 0.217757\n",
      "Iteration   600, loss = 0.217683\n",
      "Iteration   700, loss = 0.217633\n",
      "Iteration   800, loss = 0.217598\n",
      "Iteration   900, loss = 0.217574\n",
      "Iteration  1000, loss = 0.217557\n",
      "Iteration  1100, loss = 0.217545\n",
      "Iteration  1200, loss = 0.217536\n",
      "Iteration  1300, loss = 0.217529\n",
      "Iteration  1400, loss = 0.217525\n",
      "Iteration  1500, loss = 0.217521\n",
      "Iteration  1600, loss = 0.217519\n",
      "Iteration  1700, loss = 0.217517\n",
      "Iteration  1800, loss = 0.217515\n",
      "Converged at iteration 1873\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.216906\n",
      "Iteration   200, loss = 0.214285\n",
      "Iteration   300, loss = 0.213749\n",
      "Iteration   400, loss = 0.213540\n",
      "Iteration   500, loss = 0.213429\n",
      "Iteration   600, loss = 0.213362\n",
      "Iteration   700, loss = 0.213318\n",
      "Iteration   800, loss = 0.213289\n",
      "Iteration   900, loss = 0.213268\n",
      "Iteration  1000, loss = 0.213254\n",
      "Iteration  1100, loss = 0.213243\n",
      "Iteration  1200, loss = 0.213236\n",
      "Iteration  1300, loss = 0.213231\n",
      "Iteration  1400, loss = 0.213227\n",
      "Iteration  1500, loss = 0.213224\n",
      "Iteration  1600, loss = 0.213221\n",
      "Iteration  1700, loss = 0.213220\n",
      "Iteration  1800, loss = 0.213218\n",
      "Converged at iteration 1832\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218420\n",
      "Iteration   200, loss = 0.215899\n",
      "Iteration   300, loss = 0.215423\n",
      "Iteration   400, loss = 0.215247\n",
      "Iteration   500, loss = 0.215158\n",
      "Iteration   600, loss = 0.215105\n",
      "Iteration   700, loss = 0.215071\n",
      "Iteration   800, loss = 0.215048\n",
      "Iteration   900, loss = 0.215033\n",
      "Iteration  1000, loss = 0.215022\n",
      "Iteration  1100, loss = 0.215014\n",
      "Iteration  1200, loss = 0.215008\n",
      "Iteration  1300, loss = 0.215004\n",
      "Iteration  1400, loss = 0.215002\n",
      "Iteration  1500, loss = 0.214999\n",
      "Iteration  1600, loss = 0.214998\n",
      "Iteration  1700, loss = 0.214997\n",
      "Converged at iteration 1710\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.222193\n",
      "Iteration   200, loss = 0.219767\n",
      "Iteration   300, loss = 0.219275\n",
      "Iteration   400, loss = 0.219076\n",
      "Iteration   500, loss = 0.218966\n",
      "Iteration   600, loss = 0.218897\n",
      "Iteration   700, loss = 0.218850\n",
      "Iteration   800, loss = 0.218817\n",
      "Iteration   900, loss = 0.218794\n",
      "Iteration  1000, loss = 0.218778\n",
      "Iteration  1100, loss = 0.218766\n",
      "Iteration  1200, loss = 0.218757\n",
      "Iteration  1300, loss = 0.218750\n",
      "Iteration  1400, loss = 0.218745\n",
      "Iteration  1500, loss = 0.218742\n",
      "Iteration  1600, loss = 0.218739\n",
      "Iteration  1700, loss = 0.218737\n",
      "Iteration  1800, loss = 0.218735\n",
      "Iteration  1900, loss = 0.218734\n",
      "Converged at iteration 1910\n",
      " Accuracy: 57.00%\n",
      " F1 Score: 0.2712\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218887\n",
      "Iteration   200, loss = 0.216287\n",
      "Iteration   300, loss = 0.215803\n",
      "Iteration   400, loss = 0.215630\n",
      "Iteration   500, loss = 0.215543\n",
      "Iteration   600, loss = 0.215492\n",
      "Iteration   700, loss = 0.215458\n",
      "Iteration   800, loss = 0.215434\n",
      "Iteration   900, loss = 0.215418\n",
      "Iteration  1000, loss = 0.215406\n",
      "Iteration  1100, loss = 0.215397\n",
      "Iteration  1200, loss = 0.215391\n",
      "Iteration  1300, loss = 0.215386\n",
      "Iteration  1400, loss = 0.215382\n",
      "Iteration  1500, loss = 0.215380\n",
      "Iteration  1600, loss = 0.215378\n",
      "Iteration  1700, loss = 0.215376\n",
      "Iteration  1800, loss = 0.215375\n",
      "Converged at iteration 1810\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.382847\n",
      "Iteration   200, loss = 0.377677\n",
      "Iteration   300, loss = 0.375503\n",
      "Iteration   400, loss = 0.374279\n",
      "Iteration   500, loss = 0.373522\n",
      "Iteration   600, loss = 0.373031\n",
      "Iteration   700, loss = 0.372701\n",
      "Iteration   800, loss = 0.372475\n",
      "Iteration   900, loss = 0.372318\n",
      "Iteration  1000, loss = 0.372206\n",
      "Iteration  1100, loss = 0.372126\n",
      "Iteration  1200, loss = 0.372068\n",
      "Iteration  1300, loss = 0.372026\n",
      "Iteration  1400, loss = 0.371995\n",
      "Iteration  1500, loss = 0.371972\n",
      "Iteration  1600, loss = 0.371955\n",
      "Iteration  1700, loss = 0.371942\n",
      "Iteration  1800, loss = 0.371933\n",
      "Iteration  1900, loss = 0.371926\n",
      "Iteration  2000, loss = 0.371920\n",
      "Iteration  2100, loss = 0.371916\n",
      "Iteration  2200, loss = 0.371913\n",
      "Iteration  2300, loss = 0.371911\n",
      "Iteration  2400, loss = 0.371909\n",
      "Iteration  2500, loss = 0.371907\n",
      "Converged at iteration 2581\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.364363\n",
      "Iteration   200, loss = 0.357849\n",
      "Iteration   300, loss = 0.355491\n",
      "Iteration   400, loss = 0.354365\n",
      "Iteration   500, loss = 0.353749\n",
      "Iteration   600, loss = 0.353383\n",
      "Iteration   700, loss = 0.353152\n",
      "Iteration   800, loss = 0.353000\n",
      "Iteration   900, loss = 0.352897\n",
      "Iteration  1000, loss = 0.352825\n",
      "Iteration  1100, loss = 0.352774\n",
      "Iteration  1200, loss = 0.352737\n",
      "Iteration  1300, loss = 0.352710\n",
      "Iteration  1400, loss = 0.352691\n",
      "Iteration  1500, loss = 0.352676\n",
      "Iteration  1600, loss = 0.352665\n",
      "Iteration  1700, loss = 0.352656\n",
      "Iteration  1800, loss = 0.352650\n",
      "Iteration  1900, loss = 0.352645\n",
      "Iteration  2000, loss = 0.352641\n",
      "Iteration  2100, loss = 0.352638\n",
      "Iteration  2200, loss = 0.352636\n",
      "Iteration  2300, loss = 0.352634\n",
      "Iteration  2400, loss = 0.352633\n",
      "Converged at iteration 2462\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.367902\n",
      "Iteration   200, loss = 0.363557\n",
      "Iteration   300, loss = 0.362158\n",
      "Iteration   400, loss = 0.361496\n",
      "Iteration   500, loss = 0.361131\n",
      "Iteration   600, loss = 0.360912\n",
      "Iteration   700, loss = 0.360772\n",
      "Iteration   800, loss = 0.360679\n",
      "Iteration   900, loss = 0.360615\n",
      "Iteration  1000, loss = 0.360570\n",
      "Iteration  1100, loss = 0.360538\n",
      "Iteration  1200, loss = 0.360515\n",
      "Iteration  1300, loss = 0.360497\n",
      "Iteration  1400, loss = 0.360485\n",
      "Iteration  1500, loss = 0.360475\n",
      "Iteration  1600, loss = 0.360468\n",
      "Iteration  1700, loss = 0.360463\n",
      "Iteration  1800, loss = 0.360459\n",
      "Iteration  1900, loss = 0.360456\n",
      "Iteration  2000, loss = 0.360454\n",
      "Iteration  2100, loss = 0.360452\n",
      "Iteration  2200, loss = 0.360451\n",
      "Converged at iteration 2257\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.373653\n",
      "Iteration   200, loss = 0.367876\n",
      "Iteration   300, loss = 0.365844\n",
      "Iteration   400, loss = 0.364835\n",
      "Iteration   500, loss = 0.364256\n",
      "Iteration   600, loss = 0.363894\n",
      "Iteration   700, loss = 0.363655\n",
      "Iteration   800, loss = 0.363492\n",
      "Iteration   900, loss = 0.363378\n",
      "Iteration  1000, loss = 0.363295\n",
      "Iteration  1100, loss = 0.363236\n",
      "Iteration  1200, loss = 0.363192\n",
      "Iteration  1300, loss = 0.363160\n",
      "Iteration  1400, loss = 0.363135\n",
      "Iteration  1500, loss = 0.363117\n",
      "Iteration  1600, loss = 0.363104\n",
      "Iteration  1700, loss = 0.363093\n",
      "Iteration  1800, loss = 0.363085\n",
      "Iteration  1900, loss = 0.363079\n",
      "Iteration  2000, loss = 0.363075\n",
      "Iteration  2100, loss = 0.363071\n",
      "Iteration  2200, loss = 0.363068\n",
      "Iteration  2300, loss = 0.363066\n",
      "Iteration  2400, loss = 0.363065\n",
      "Iteration  2500, loss = 0.363063\n",
      "Converged at iteration 2536\n",
      " Accuracy: 13.25%\n",
      " F1 Score: 0.1614\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.387809\n",
      "Iteration   200, loss = 0.382998\n",
      "Iteration   300, loss = 0.381450\n",
      "Iteration   400, loss = 0.380730\n",
      "Iteration   500, loss = 0.380329\n",
      "Iteration   600, loss = 0.380079\n",
      "Iteration   700, loss = 0.379912\n",
      "Iteration   800, loss = 0.379794\n",
      "Iteration   900, loss = 0.379710\n",
      "Iteration  1000, loss = 0.379648\n",
      "Iteration  1100, loss = 0.379602\n",
      "Iteration  1200, loss = 0.379567\n",
      "Iteration  1300, loss = 0.379540\n",
      "Iteration  1400, loss = 0.379520\n",
      "Iteration  1500, loss = 0.379505\n",
      "Iteration  1600, loss = 0.379493\n",
      "Iteration  1700, loss = 0.379484\n",
      "Iteration  1800, loss = 0.379477\n",
      "Iteration  1900, loss = 0.379472\n",
      "Iteration  2000, loss = 0.379468\n",
      "Iteration  2100, loss = 0.379465\n",
      "Iteration  2200, loss = 0.379462\n",
      "Iteration  2300, loss = 0.379460\n",
      "Iteration  2400, loss = 0.379459\n",
      "Iteration  2500, loss = 0.379457\n",
      "Converged at iteration 2521\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.221028\n",
      "Iteration   200, loss = 0.218579\n",
      "Iteration   300, loss = 0.218082\n",
      "Iteration   400, loss = 0.217874\n",
      "Iteration   500, loss = 0.217757\n",
      "Iteration   600, loss = 0.217683\n",
      "Iteration   700, loss = 0.217633\n",
      "Iteration   800, loss = 0.217598\n",
      "Iteration   900, loss = 0.217574\n",
      "Iteration  1000, loss = 0.217557\n",
      "Iteration  1100, loss = 0.217545\n",
      "Iteration  1200, loss = 0.217536\n",
      "Iteration  1300, loss = 0.217529\n",
      "Iteration  1400, loss = 0.217525\n",
      "Iteration  1500, loss = 0.217521\n",
      "Iteration  1600, loss = 0.217519\n",
      "Iteration  1700, loss = 0.217517\n",
      "Iteration  1800, loss = 0.217515\n",
      "Converged at iteration 1873\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.216906\n",
      "Iteration   200, loss = 0.214285\n",
      "Iteration   300, loss = 0.213749\n",
      "Iteration   400, loss = 0.213540\n",
      "Iteration   500, loss = 0.213429\n",
      "Iteration   600, loss = 0.213362\n",
      "Iteration   700, loss = 0.213318\n",
      "Iteration   800, loss = 0.213289\n",
      "Iteration   900, loss = 0.213268\n",
      "Iteration  1000, loss = 0.213254\n",
      "Iteration  1100, loss = 0.213243\n",
      "Iteration  1200, loss = 0.213236\n",
      "Iteration  1300, loss = 0.213231\n",
      "Iteration  1400, loss = 0.213227\n",
      "Iteration  1500, loss = 0.213224\n",
      "Iteration  1600, loss = 0.213221\n",
      "Iteration  1700, loss = 0.213220\n",
      "Iteration  1800, loss = 0.213218\n",
      "Converged at iteration 1832\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218420\n",
      "Iteration   200, loss = 0.215899\n",
      "Iteration   300, loss = 0.215423\n",
      "Iteration   400, loss = 0.215247\n",
      "Iteration   500, loss = 0.215158\n",
      "Iteration   600, loss = 0.215105\n",
      "Iteration   700, loss = 0.215071\n",
      "Iteration   800, loss = 0.215048\n",
      "Iteration   900, loss = 0.215033\n",
      "Iteration  1000, loss = 0.215022\n",
      "Iteration  1100, loss = 0.215014\n",
      "Iteration  1200, loss = 0.215008\n",
      "Iteration  1300, loss = 0.215004\n",
      "Iteration  1400, loss = 0.215002\n",
      "Iteration  1500, loss = 0.214999\n",
      "Iteration  1600, loss = 0.214998\n",
      "Iteration  1700, loss = 0.214997\n",
      "Converged at iteration 1710\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.222193\n",
      "Iteration   200, loss = 0.219767\n",
      "Iteration   300, loss = 0.219275\n",
      "Iteration   400, loss = 0.219076\n",
      "Iteration   500, loss = 0.218966\n",
      "Iteration   600, loss = 0.218897\n",
      "Iteration   700, loss = 0.218850\n",
      "Iteration   800, loss = 0.218817\n",
      "Iteration   900, loss = 0.218794\n",
      "Iteration  1000, loss = 0.218778\n",
      "Iteration  1100, loss = 0.218766\n",
      "Iteration  1200, loss = 0.218757\n",
      "Iteration  1300, loss = 0.218750\n",
      "Iteration  1400, loss = 0.218745\n",
      "Iteration  1500, loss = 0.218742\n",
      "Iteration  1600, loss = 0.218739\n",
      "Iteration  1700, loss = 0.218737\n",
      "Iteration  1800, loss = 0.218735\n",
      "Iteration  1900, loss = 0.218734\n",
      "Converged at iteration 1910\n",
      " Accuracy: 71.60%\n",
      " F1 Score: 0.3395\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218887\n",
      "Iteration   200, loss = 0.216287\n",
      "Iteration   300, loss = 0.215803\n",
      "Iteration   400, loss = 0.215630\n",
      "Iteration   500, loss = 0.215543\n",
      "Iteration   600, loss = 0.215492\n",
      "Iteration   700, loss = 0.215458\n",
      "Iteration   800, loss = 0.215434\n",
      "Iteration   900, loss = 0.215418\n",
      "Iteration  1000, loss = 0.215406\n",
      "Iteration  1100, loss = 0.215397\n",
      "Iteration  1200, loss = 0.215391\n",
      "Iteration  1300, loss = 0.215386\n",
      "Iteration  1400, loss = 0.215382\n",
      "Iteration  1500, loss = 0.215380\n",
      "Iteration  1600, loss = 0.215378\n",
      "Iteration  1700, loss = 0.215376\n",
      "Iteration  1800, loss = 0.215375\n",
      "Converged at iteration 1810\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.382847\n",
      "Iteration   200, loss = 0.377677\n",
      "Iteration   300, loss = 0.375503\n",
      "Iteration   400, loss = 0.374279\n",
      "Iteration   500, loss = 0.373522\n",
      "Iteration   600, loss = 0.373031\n",
      "Iteration   700, loss = 0.372701\n",
      "Iteration   800, loss = 0.372475\n",
      "Iteration   900, loss = 0.372318\n",
      "Iteration  1000, loss = 0.372206\n",
      "Iteration  1100, loss = 0.372126\n",
      "Iteration  1200, loss = 0.372068\n",
      "Iteration  1300, loss = 0.372026\n",
      "Iteration  1400, loss = 0.371995\n",
      "Iteration  1500, loss = 0.371972\n",
      "Iteration  1600, loss = 0.371955\n",
      "Iteration  1700, loss = 0.371942\n",
      "Iteration  1800, loss = 0.371933\n",
      "Iteration  1900, loss = 0.371926\n",
      "Iteration  2000, loss = 0.371920\n",
      "Iteration  2100, loss = 0.371916\n",
      "Iteration  2200, loss = 0.371913\n",
      "Iteration  2300, loss = 0.371911\n",
      "Iteration  2400, loss = 0.371909\n",
      "Iteration  2500, loss = 0.371907\n",
      "Converged at iteration 2581\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.364363\n",
      "Iteration   200, loss = 0.357849\n",
      "Iteration   300, loss = 0.355491\n",
      "Iteration   400, loss = 0.354365\n",
      "Iteration   500, loss = 0.353749\n",
      "Iteration   600, loss = 0.353383\n",
      "Iteration   700, loss = 0.353152\n",
      "Iteration   800, loss = 0.353000\n",
      "Iteration   900, loss = 0.352897\n",
      "Iteration  1000, loss = 0.352825\n",
      "Iteration  1100, loss = 0.352774\n",
      "Iteration  1200, loss = 0.352737\n",
      "Iteration  1300, loss = 0.352710\n",
      "Iteration  1400, loss = 0.352691\n",
      "Iteration  1500, loss = 0.352676\n",
      "Iteration  1600, loss = 0.352665\n",
      "Iteration  1700, loss = 0.352656\n",
      "Iteration  1800, loss = 0.352650\n",
      "Iteration  1900, loss = 0.352645\n",
      "Iteration  2000, loss = 0.352641\n",
      "Iteration  2100, loss = 0.352638\n",
      "Iteration  2200, loss = 0.352636\n",
      "Iteration  2300, loss = 0.352634\n",
      "Iteration  2400, loss = 0.352633\n",
      "Converged at iteration 2462\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.367902\n",
      "Iteration   200, loss = 0.363557\n",
      "Iteration   300, loss = 0.362158\n",
      "Iteration   400, loss = 0.361496\n",
      "Iteration   500, loss = 0.361131\n",
      "Iteration   600, loss = 0.360912\n",
      "Iteration   700, loss = 0.360772\n",
      "Iteration   800, loss = 0.360679\n",
      "Iteration   900, loss = 0.360615\n",
      "Iteration  1000, loss = 0.360570\n",
      "Iteration  1100, loss = 0.360538\n",
      "Iteration  1200, loss = 0.360515\n",
      "Iteration  1300, loss = 0.360497\n",
      "Iteration  1400, loss = 0.360485\n",
      "Iteration  1500, loss = 0.360475\n",
      "Iteration  1600, loss = 0.360468\n",
      "Iteration  1700, loss = 0.360463\n",
      "Iteration  1800, loss = 0.360459\n",
      "Iteration  1900, loss = 0.360456\n",
      "Iteration  2000, loss = 0.360454\n",
      "Iteration  2100, loss = 0.360452\n",
      "Iteration  2200, loss = 0.360451\n",
      "Converged at iteration 2257\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.373653\n",
      "Iteration   200, loss = 0.367876\n",
      "Iteration   300, loss = 0.365844\n",
      "Iteration   400, loss = 0.364835\n",
      "Iteration   500, loss = 0.364256\n",
      "Iteration   600, loss = 0.363894\n",
      "Iteration   700, loss = 0.363655\n",
      "Iteration   800, loss = 0.363492\n",
      "Iteration   900, loss = 0.363378\n",
      "Iteration  1000, loss = 0.363295\n",
      "Iteration  1100, loss = 0.363236\n",
      "Iteration  1200, loss = 0.363192\n",
      "Iteration  1300, loss = 0.363160\n",
      "Iteration  1400, loss = 0.363135\n",
      "Iteration  1500, loss = 0.363117\n",
      "Iteration  1600, loss = 0.363104\n",
      "Iteration  1700, loss = 0.363093\n",
      "Iteration  1800, loss = 0.363085\n",
      "Iteration  1900, loss = 0.363079\n",
      "Iteration  2000, loss = 0.363075\n",
      "Iteration  2100, loss = 0.363071\n",
      "Iteration  2200, loss = 0.363068\n",
      "Iteration  2300, loss = 0.363066\n",
      "Iteration  2400, loss = 0.363065\n",
      "Iteration  2500, loss = 0.363063\n",
      "Converged at iteration 2536\n",
      " Accuracy: 18.60%\n",
      " F1 Score: 0.1694\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.387809\n",
      "Iteration   200, loss = 0.382998\n",
      "Iteration   300, loss = 0.381450\n",
      "Iteration   400, loss = 0.380730\n",
      "Iteration   500, loss = 0.380329\n",
      "Iteration   600, loss = 0.380079\n",
      "Iteration   700, loss = 0.379912\n",
      "Iteration   800, loss = 0.379794\n",
      "Iteration   900, loss = 0.379710\n",
      "Iteration  1000, loss = 0.379648\n",
      "Iteration  1100, loss = 0.379602\n",
      "Iteration  1200, loss = 0.379567\n",
      "Iteration  1300, loss = 0.379540\n",
      "Iteration  1400, loss = 0.379520\n",
      "Iteration  1500, loss = 0.379505\n",
      "Iteration  1600, loss = 0.379493\n",
      "Iteration  1700, loss = 0.379484\n",
      "Iteration  1800, loss = 0.379477\n",
      "Iteration  1900, loss = 0.379472\n",
      "Iteration  2000, loss = 0.379468\n",
      "Iteration  2100, loss = 0.379465\n",
      "Iteration  2200, loss = 0.379462\n",
      "Iteration  2300, loss = 0.379460\n",
      "Iteration  2400, loss = 0.379459\n",
      "Iteration  2500, loss = 0.379457\n",
      "Converged at iteration 2521\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.221028\n",
      "Iteration   200, loss = 0.218579\n",
      "Iteration   300, loss = 0.218082\n",
      "Iteration   400, loss = 0.217874\n",
      "Iteration   500, loss = 0.217757\n",
      "Iteration   600, loss = 0.217683\n",
      "Iteration   700, loss = 0.217633\n",
      "Iteration   800, loss = 0.217598\n",
      "Iteration   900, loss = 0.217574\n",
      "Iteration  1000, loss = 0.217557\n",
      "Iteration  1100, loss = 0.217545\n",
      "Iteration  1200, loss = 0.217536\n",
      "Iteration  1300, loss = 0.217529\n",
      "Iteration  1400, loss = 0.217525\n",
      "Iteration  1500, loss = 0.217521\n",
      "Iteration  1600, loss = 0.217519\n",
      "Iteration  1700, loss = 0.217517\n",
      "Iteration  1800, loss = 0.217515\n",
      "Converged at iteration 1873\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.216906\n",
      "Iteration   200, loss = 0.214285\n",
      "Iteration   300, loss = 0.213749\n",
      "Iteration   400, loss = 0.213540\n",
      "Iteration   500, loss = 0.213429\n",
      "Iteration   600, loss = 0.213362\n",
      "Iteration   700, loss = 0.213318\n",
      "Iteration   800, loss = 0.213289\n",
      "Iteration   900, loss = 0.213268\n",
      "Iteration  1000, loss = 0.213254\n",
      "Iteration  1100, loss = 0.213243\n",
      "Iteration  1200, loss = 0.213236\n",
      "Iteration  1300, loss = 0.213231\n",
      "Iteration  1400, loss = 0.213227\n",
      "Iteration  1500, loss = 0.213224\n",
      "Iteration  1600, loss = 0.213221\n",
      "Iteration  1700, loss = 0.213220\n",
      "Iteration  1800, loss = 0.213218\n",
      "Converged at iteration 1832\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218420\n",
      "Iteration   200, loss = 0.215899\n",
      "Iteration   300, loss = 0.215423\n",
      "Iteration   400, loss = 0.215247\n",
      "Iteration   500, loss = 0.215158\n",
      "Iteration   600, loss = 0.215105\n",
      "Iteration   700, loss = 0.215071\n",
      "Iteration   800, loss = 0.215048\n",
      "Iteration   900, loss = 0.215033\n",
      "Iteration  1000, loss = 0.215022\n",
      "Iteration  1100, loss = 0.215014\n",
      "Iteration  1200, loss = 0.215008\n",
      "Iteration  1300, loss = 0.215004\n",
      "Iteration  1400, loss = 0.215002\n",
      "Iteration  1500, loss = 0.214999\n",
      "Iteration  1600, loss = 0.214998\n",
      "Iteration  1700, loss = 0.214997\n",
      "Converged at iteration 1710\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.222193\n",
      "Iteration   200, loss = 0.219767\n",
      "Iteration   300, loss = 0.219275\n",
      "Iteration   400, loss = 0.219076\n",
      "Iteration   500, loss = 0.218966\n",
      "Iteration   600, loss = 0.218897\n",
      "Iteration   700, loss = 0.218850\n",
      "Iteration   800, loss = 0.218817\n",
      "Iteration   900, loss = 0.218794\n",
      "Iteration  1000, loss = 0.218778\n",
      "Iteration  1100, loss = 0.218766\n",
      "Iteration  1200, loss = 0.218757\n",
      "Iteration  1300, loss = 0.218750\n",
      "Iteration  1400, loss = 0.218745\n",
      "Iteration  1500, loss = 0.218742\n",
      "Iteration  1600, loss = 0.218739\n",
      "Iteration  1700, loss = 0.218737\n",
      "Iteration  1800, loss = 0.218735\n",
      "Iteration  1900, loss = 0.218734\n",
      "Converged at iteration 1910\n",
      " Accuracy: 78.40%\n",
      " F1 Score: 0.3703\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218887\n",
      "Iteration   200, loss = 0.216287\n",
      "Iteration   300, loss = 0.215803\n",
      "Iteration   400, loss = 0.215630\n",
      "Iteration   500, loss = 0.215543\n",
      "Iteration   600, loss = 0.215492\n",
      "Iteration   700, loss = 0.215458\n",
      "Iteration   800, loss = 0.215434\n",
      "Iteration   900, loss = 0.215418\n",
      "Iteration  1000, loss = 0.215406\n",
      "Iteration  1100, loss = 0.215397\n",
      "Iteration  1200, loss = 0.215391\n",
      "Iteration  1300, loss = 0.215386\n",
      "Iteration  1400, loss = 0.215382\n",
      "Iteration  1500, loss = 0.215380\n",
      "Iteration  1600, loss = 0.215378\n",
      "Iteration  1700, loss = 0.215376\n",
      "Iteration  1800, loss = 0.215375\n",
      "Converged at iteration 1810\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.382847\n",
      "Iteration   200, loss = 0.377677\n",
      "Iteration   300, loss = 0.375503\n",
      "Iteration   400, loss = 0.374279\n",
      "Iteration   500, loss = 0.373522\n",
      "Iteration   600, loss = 0.373031\n",
      "Iteration   700, loss = 0.372701\n",
      "Iteration   800, loss = 0.372475\n",
      "Iteration   900, loss = 0.372318\n",
      "Iteration  1000, loss = 0.372206\n",
      "Iteration  1100, loss = 0.372126\n",
      "Iteration  1200, loss = 0.372068\n",
      "Iteration  1300, loss = 0.372026\n",
      "Iteration  1400, loss = 0.371995\n",
      "Iteration  1500, loss = 0.371972\n",
      "Iteration  1600, loss = 0.371955\n",
      "Iteration  1700, loss = 0.371942\n",
      "Iteration  1800, loss = 0.371933\n",
      "Iteration  1900, loss = 0.371926\n",
      "Iteration  2000, loss = 0.371920\n",
      "Iteration  2100, loss = 0.371916\n",
      "Iteration  2200, loss = 0.371913\n",
      "Iteration  2300, loss = 0.371911\n",
      "Iteration  2400, loss = 0.371909\n",
      "Iteration  2500, loss = 0.371907\n",
      "Converged at iteration 2581\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.364363\n",
      "Iteration   200, loss = 0.357849\n",
      "Iteration   300, loss = 0.355491\n",
      "Iteration   400, loss = 0.354365\n",
      "Iteration   500, loss = 0.353749\n",
      "Iteration   600, loss = 0.353383\n",
      "Iteration   700, loss = 0.353152\n",
      "Iteration   800, loss = 0.353000\n",
      "Iteration   900, loss = 0.352897\n",
      "Iteration  1000, loss = 0.352825\n",
      "Iteration  1100, loss = 0.352774\n",
      "Iteration  1200, loss = 0.352737\n",
      "Iteration  1300, loss = 0.352710\n",
      "Iteration  1400, loss = 0.352691\n",
      "Iteration  1500, loss = 0.352676\n",
      "Iteration  1600, loss = 0.352665\n",
      "Iteration  1700, loss = 0.352656\n",
      "Iteration  1800, loss = 0.352650\n",
      "Iteration  1900, loss = 0.352645\n",
      "Iteration  2000, loss = 0.352641\n",
      "Iteration  2100, loss = 0.352638\n",
      "Iteration  2200, loss = 0.352636\n",
      "Iteration  2300, loss = 0.352634\n",
      "Iteration  2400, loss = 0.352633\n",
      "Converged at iteration 2462\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.367902\n",
      "Iteration   200, loss = 0.363557\n",
      "Iteration   300, loss = 0.362158\n",
      "Iteration   400, loss = 0.361496\n",
      "Iteration   500, loss = 0.361131\n",
      "Iteration   600, loss = 0.360912\n",
      "Iteration   700, loss = 0.360772\n",
      "Iteration   800, loss = 0.360679\n",
      "Iteration   900, loss = 0.360615\n",
      "Iteration  1000, loss = 0.360570\n",
      "Iteration  1100, loss = 0.360538\n",
      "Iteration  1200, loss = 0.360515\n",
      "Iteration  1300, loss = 0.360497\n",
      "Iteration  1400, loss = 0.360485\n",
      "Iteration  1500, loss = 0.360475\n",
      "Iteration  1600, loss = 0.360468\n",
      "Iteration  1700, loss = 0.360463\n",
      "Iteration  1800, loss = 0.360459\n",
      "Iteration  1900, loss = 0.360456\n",
      "Iteration  2000, loss = 0.360454\n",
      "Iteration  2100, loss = 0.360452\n",
      "Iteration  2200, loss = 0.360451\n",
      "Converged at iteration 2257\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.373653\n",
      "Iteration   200, loss = 0.367876\n",
      "Iteration   300, loss = 0.365844\n",
      "Iteration   400, loss = 0.364835\n",
      "Iteration   500, loss = 0.364256\n",
      "Iteration   600, loss = 0.363894\n",
      "Iteration   700, loss = 0.363655\n",
      "Iteration   800, loss = 0.363492\n",
      "Iteration   900, loss = 0.363378\n",
      "Iteration  1000, loss = 0.363295\n",
      "Iteration  1100, loss = 0.363236\n",
      "Iteration  1200, loss = 0.363192\n",
      "Iteration  1300, loss = 0.363160\n",
      "Iteration  1400, loss = 0.363135\n",
      "Iteration  1500, loss = 0.363117\n",
      "Iteration  1600, loss = 0.363104\n",
      "Iteration  1700, loss = 0.363093\n",
      "Iteration  1800, loss = 0.363085\n",
      "Iteration  1900, loss = 0.363079\n",
      "Iteration  2000, loss = 0.363075\n",
      "Iteration  2100, loss = 0.363071\n",
      "Iteration  2200, loss = 0.363068\n",
      "Iteration  2300, loss = 0.363066\n",
      "Iteration  2400, loss = 0.363065\n",
      "Iteration  2500, loss = 0.363063\n",
      "Converged at iteration 2536\n",
      " Accuracy: 22.90%\n",
      " F1 Score: 0.1763\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.387809\n",
      "Iteration   200, loss = 0.382998\n",
      "Iteration   300, loss = 0.381450\n",
      "Iteration   400, loss = 0.380730\n",
      "Iteration   500, loss = 0.380329\n",
      "Iteration   600, loss = 0.380079\n",
      "Iteration   700, loss = 0.379912\n",
      "Iteration   800, loss = 0.379794\n",
      "Iteration   900, loss = 0.379710\n",
      "Iteration  1000, loss = 0.379648\n",
      "Iteration  1100, loss = 0.379602\n",
      "Iteration  1200, loss = 0.379567\n",
      "Iteration  1300, loss = 0.379540\n",
      "Iteration  1400, loss = 0.379520\n",
      "Iteration  1500, loss = 0.379505\n",
      "Iteration  1600, loss = 0.379493\n",
      "Iteration  1700, loss = 0.379484\n",
      "Iteration  1800, loss = 0.379477\n",
      "Iteration  1900, loss = 0.379472\n",
      "Iteration  2000, loss = 0.379468\n",
      "Iteration  2100, loss = 0.379465\n",
      "Iteration  2200, loss = 0.379462\n",
      "Iteration  2300, loss = 0.379460\n",
      "Iteration  2400, loss = 0.379459\n",
      "Iteration  2500, loss = 0.379457\n",
      "Converged at iteration 2521\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.221028\n",
      "Iteration   200, loss = 0.218579\n",
      "Iteration   300, loss = 0.218082\n",
      "Iteration   400, loss = 0.217874\n",
      "Iteration   500, loss = 0.217757\n",
      "Iteration   600, loss = 0.217683\n",
      "Iteration   700, loss = 0.217633\n",
      "Iteration   800, loss = 0.217598\n",
      "Iteration   900, loss = 0.217574\n",
      "Iteration  1000, loss = 0.217557\n",
      "Iteration  1100, loss = 0.217545\n",
      "Iteration  1200, loss = 0.217536\n",
      "Iteration  1300, loss = 0.217529\n",
      "Iteration  1400, loss = 0.217525\n",
      "Iteration  1500, loss = 0.217521\n",
      "Iteration  1600, loss = 0.217519\n",
      "Iteration  1700, loss = 0.217517\n",
      "Iteration  1800, loss = 0.217515\n",
      "Converged at iteration 1873\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.216906\n",
      "Iteration   200, loss = 0.214285\n",
      "Iteration   300, loss = 0.213749\n",
      "Iteration   400, loss = 0.213540\n",
      "Iteration   500, loss = 0.213429\n",
      "Iteration   600, loss = 0.213362\n",
      "Iteration   700, loss = 0.213318\n",
      "Iteration   800, loss = 0.213289\n",
      "Iteration   900, loss = 0.213268\n",
      "Iteration  1000, loss = 0.213254\n",
      "Iteration  1100, loss = 0.213243\n",
      "Iteration  1200, loss = 0.213236\n",
      "Iteration  1300, loss = 0.213231\n",
      "Iteration  1400, loss = 0.213227\n",
      "Iteration  1500, loss = 0.213224\n",
      "Iteration  1600, loss = 0.213221\n",
      "Iteration  1700, loss = 0.213220\n",
      "Iteration  1800, loss = 0.213218\n",
      "Converged at iteration 1832\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218420\n",
      "Iteration   200, loss = 0.215899\n",
      "Iteration   300, loss = 0.215423\n",
      "Iteration   400, loss = 0.215247\n",
      "Iteration   500, loss = 0.215158\n",
      "Iteration   600, loss = 0.215105\n",
      "Iteration   700, loss = 0.215071\n",
      "Iteration   800, loss = 0.215048\n",
      "Iteration   900, loss = 0.215033\n",
      "Iteration  1000, loss = 0.215022\n",
      "Iteration  1100, loss = 0.215014\n",
      "Iteration  1200, loss = 0.215008\n",
      "Iteration  1300, loss = 0.215004\n",
      "Iteration  1400, loss = 0.215002\n",
      "Iteration  1500, loss = 0.214999\n",
      "Iteration  1600, loss = 0.214998\n",
      "Iteration  1700, loss = 0.214997\n",
      "Converged at iteration 1710\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.222193\n",
      "Iteration   200, loss = 0.219767\n",
      "Iteration   300, loss = 0.219275\n",
      "Iteration   400, loss = 0.219076\n",
      "Iteration   500, loss = 0.218966\n",
      "Iteration   600, loss = 0.218897\n",
      "Iteration   700, loss = 0.218850\n",
      "Iteration   800, loss = 0.218817\n",
      "Iteration   900, loss = 0.218794\n",
      "Iteration  1000, loss = 0.218778\n",
      "Iteration  1100, loss = 0.218766\n",
      "Iteration  1200, loss = 0.218757\n",
      "Iteration  1300, loss = 0.218750\n",
      "Iteration  1400, loss = 0.218745\n",
      "Iteration  1500, loss = 0.218742\n",
      "Iteration  1600, loss = 0.218739\n",
      "Iteration  1700, loss = 0.218737\n",
      "Iteration  1800, loss = 0.218735\n",
      "Iteration  1900, loss = 0.218734\n",
      "Converged at iteration 1910\n",
      " Accuracy: 82.55%\n",
      " F1 Score: 0.3866\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218887\n",
      "Iteration   200, loss = 0.216287\n",
      "Iteration   300, loss = 0.215803\n",
      "Iteration   400, loss = 0.215630\n",
      "Iteration   500, loss = 0.215543\n",
      "Iteration   600, loss = 0.215492\n",
      "Iteration   700, loss = 0.215458\n",
      "Iteration   800, loss = 0.215434\n",
      "Iteration   900, loss = 0.215418\n",
      "Iteration  1000, loss = 0.215406\n",
      "Iteration  1100, loss = 0.215397\n",
      "Iteration  1200, loss = 0.215391\n",
      "Iteration  1300, loss = 0.215386\n",
      "Iteration  1400, loss = 0.215382\n",
      "Iteration  1500, loss = 0.215380\n",
      "Iteration  1600, loss = 0.215378\n",
      "Iteration  1700, loss = 0.215376\n",
      "Iteration  1800, loss = 0.215375\n",
      "Converged at iteration 1810\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.382847\n",
      "Iteration   200, loss = 0.377677\n",
      "Iteration   300, loss = 0.375503\n",
      "Iteration   400, loss = 0.374279\n",
      "Iteration   500, loss = 0.373522\n",
      "Iteration   600, loss = 0.373031\n",
      "Iteration   700, loss = 0.372701\n",
      "Iteration   800, loss = 0.372475\n",
      "Iteration   900, loss = 0.372318\n",
      "Iteration  1000, loss = 0.372206\n",
      "Iteration  1100, loss = 0.372126\n",
      "Iteration  1200, loss = 0.372068\n",
      "Iteration  1300, loss = 0.372026\n",
      "Iteration  1400, loss = 0.371995\n",
      "Iteration  1500, loss = 0.371972\n",
      "Iteration  1600, loss = 0.371955\n",
      "Iteration  1700, loss = 0.371942\n",
      "Iteration  1800, loss = 0.371933\n",
      "Iteration  1900, loss = 0.371926\n",
      "Iteration  2000, loss = 0.371920\n",
      "Iteration  2100, loss = 0.371916\n",
      "Iteration  2200, loss = 0.371913\n",
      "Iteration  2300, loss = 0.371911\n",
      "Iteration  2400, loss = 0.371909\n",
      "Iteration  2500, loss = 0.371907\n",
      "Converged at iteration 2581\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.364363\n",
      "Iteration   200, loss = 0.357849\n",
      "Iteration   300, loss = 0.355491\n",
      "Iteration   400, loss = 0.354365\n",
      "Iteration   500, loss = 0.353749\n",
      "Iteration   600, loss = 0.353383\n",
      "Iteration   700, loss = 0.353152\n",
      "Iteration   800, loss = 0.353000\n",
      "Iteration   900, loss = 0.352897\n",
      "Iteration  1000, loss = 0.352825\n",
      "Iteration  1100, loss = 0.352774\n",
      "Iteration  1200, loss = 0.352737\n",
      "Iteration  1300, loss = 0.352710\n",
      "Iteration  1400, loss = 0.352691\n",
      "Iteration  1500, loss = 0.352676\n",
      "Iteration  1600, loss = 0.352665\n",
      "Iteration  1700, loss = 0.352656\n",
      "Iteration  1800, loss = 0.352650\n",
      "Iteration  1900, loss = 0.352645\n",
      "Iteration  2000, loss = 0.352641\n",
      "Iteration  2100, loss = 0.352638\n",
      "Iteration  2200, loss = 0.352636\n",
      "Iteration  2300, loss = 0.352634\n",
      "Iteration  2400, loss = 0.352633\n",
      "Converged at iteration 2462\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.367902\n",
      "Iteration   200, loss = 0.363557\n",
      "Iteration   300, loss = 0.362158\n",
      "Iteration   400, loss = 0.361496\n",
      "Iteration   500, loss = 0.361131\n",
      "Iteration   600, loss = 0.360912\n",
      "Iteration   700, loss = 0.360772\n",
      "Iteration   800, loss = 0.360679\n",
      "Iteration   900, loss = 0.360615\n",
      "Iteration  1000, loss = 0.360570\n",
      "Iteration  1100, loss = 0.360538\n",
      "Iteration  1200, loss = 0.360515\n",
      "Iteration  1300, loss = 0.360497\n",
      "Iteration  1400, loss = 0.360485\n",
      "Iteration  1500, loss = 0.360475\n",
      "Iteration  1600, loss = 0.360468\n",
      "Iteration  1700, loss = 0.360463\n",
      "Iteration  1800, loss = 0.360459\n",
      "Iteration  1900, loss = 0.360456\n",
      "Iteration  2000, loss = 0.360454\n",
      "Iteration  2100, loss = 0.360452\n",
      "Iteration  2200, loss = 0.360451\n",
      "Converged at iteration 2257\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.373653\n",
      "Iteration   200, loss = 0.367876\n",
      "Iteration   300, loss = 0.365844\n",
      "Iteration   400, loss = 0.364835\n",
      "Iteration   500, loss = 0.364256\n",
      "Iteration   600, loss = 0.363894\n",
      "Iteration   700, loss = 0.363655\n",
      "Iteration   800, loss = 0.363492\n",
      "Iteration   900, loss = 0.363378\n",
      "Iteration  1000, loss = 0.363295\n",
      "Iteration  1100, loss = 0.363236\n",
      "Iteration  1200, loss = 0.363192\n",
      "Iteration  1300, loss = 0.363160\n",
      "Iteration  1400, loss = 0.363135\n",
      "Iteration  1500, loss = 0.363117\n",
      "Iteration  1600, loss = 0.363104\n",
      "Iteration  1700, loss = 0.363093\n",
      "Iteration  1800, loss = 0.363085\n",
      "Iteration  1900, loss = 0.363079\n",
      "Iteration  2000, loss = 0.363075\n",
      "Iteration  2100, loss = 0.363071\n",
      "Iteration  2200, loss = 0.363068\n",
      "Iteration  2300, loss = 0.363066\n",
      "Iteration  2400, loss = 0.363065\n",
      "Iteration  2500, loss = 0.363063\n",
      "Converged at iteration 2536\n",
      " Accuracy: 26.80%\n",
      " F1 Score: 0.1839\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.387809\n",
      "Iteration   200, loss = 0.382998\n",
      "Iteration   300, loss = 0.381450\n",
      "Iteration   400, loss = 0.380730\n",
      "Iteration   500, loss = 0.380329\n",
      "Iteration   600, loss = 0.380079\n",
      "Iteration   700, loss = 0.379912\n",
      "Iteration   800, loss = 0.379794\n",
      "Iteration   900, loss = 0.379710\n",
      "Iteration  1000, loss = 0.379648\n",
      "Iteration  1100, loss = 0.379602\n",
      "Iteration  1200, loss = 0.379567\n",
      "Iteration  1300, loss = 0.379540\n",
      "Iteration  1400, loss = 0.379520\n",
      "Iteration  1500, loss = 0.379505\n",
      "Iteration  1600, loss = 0.379493\n",
      "Iteration  1700, loss = 0.379484\n",
      "Iteration  1800, loss = 0.379477\n",
      "Iteration  1900, loss = 0.379472\n",
      "Iteration  2000, loss = 0.379468\n",
      "Iteration  2100, loss = 0.379465\n",
      "Iteration  2200, loss = 0.379462\n",
      "Iteration  2300, loss = 0.379460\n",
      "Iteration  2400, loss = 0.379459\n",
      "Iteration  2500, loss = 0.379457\n",
      "Converged at iteration 2521\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.221028\n",
      "Iteration   200, loss = 0.218579\n",
      "Iteration   300, loss = 0.218082\n",
      "Iteration   400, loss = 0.217874\n",
      "Iteration   500, loss = 0.217757\n",
      "Iteration   600, loss = 0.217683\n",
      "Iteration   700, loss = 0.217633\n",
      "Iteration   800, loss = 0.217598\n",
      "Iteration   900, loss = 0.217574\n",
      "Iteration  1000, loss = 0.217557\n",
      "Iteration  1100, loss = 0.217545\n",
      "Iteration  1200, loss = 0.217536\n",
      "Iteration  1300, loss = 0.217529\n",
      "Iteration  1400, loss = 0.217525\n",
      "Iteration  1500, loss = 0.217521\n",
      "Iteration  1600, loss = 0.217519\n",
      "Iteration  1700, loss = 0.217517\n",
      "Iteration  1800, loss = 0.217515\n",
      "Converged at iteration 1873\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.216906\n",
      "Iteration   200, loss = 0.214285\n",
      "Iteration   300, loss = 0.213749\n",
      "Iteration   400, loss = 0.213540\n",
      "Iteration   500, loss = 0.213429\n",
      "Iteration   600, loss = 0.213362\n",
      "Iteration   700, loss = 0.213318\n",
      "Iteration   800, loss = 0.213289\n",
      "Iteration   900, loss = 0.213268\n",
      "Iteration  1000, loss = 0.213254\n",
      "Iteration  1100, loss = 0.213243\n",
      "Iteration  1200, loss = 0.213236\n",
      "Iteration  1300, loss = 0.213231\n",
      "Iteration  1400, loss = 0.213227\n",
      "Iteration  1500, loss = 0.213224\n",
      "Iteration  1600, loss = 0.213221\n",
      "Iteration  1700, loss = 0.213220\n",
      "Iteration  1800, loss = 0.213218\n",
      "Converged at iteration 1832\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218420\n",
      "Iteration   200, loss = 0.215899\n",
      "Iteration   300, loss = 0.215423\n",
      "Iteration   400, loss = 0.215247\n",
      "Iteration   500, loss = 0.215158\n",
      "Iteration   600, loss = 0.215105\n",
      "Iteration   700, loss = 0.215071\n",
      "Iteration   800, loss = 0.215048\n",
      "Iteration   900, loss = 0.215033\n",
      "Iteration  1000, loss = 0.215022\n",
      "Iteration  1100, loss = 0.215014\n",
      "Iteration  1200, loss = 0.215008\n",
      "Iteration  1300, loss = 0.215004\n",
      "Iteration  1400, loss = 0.215002\n",
      "Iteration  1500, loss = 0.214999\n",
      "Iteration  1600, loss = 0.214998\n",
      "Iteration  1700, loss = 0.214997\n",
      "Converged at iteration 1710\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.222193\n",
      "Iteration   200, loss = 0.219767\n",
      "Iteration   300, loss = 0.219275\n",
      "Iteration   400, loss = 0.219076\n",
      "Iteration   500, loss = 0.218966\n",
      "Iteration   600, loss = 0.218897\n",
      "Iteration   700, loss = 0.218850\n",
      "Iteration   800, loss = 0.218817\n",
      "Iteration   900, loss = 0.218794\n",
      "Iteration  1000, loss = 0.218778\n",
      "Iteration  1100, loss = 0.218766\n",
      "Iteration  1200, loss = 0.218757\n",
      "Iteration  1300, loss = 0.218750\n",
      "Iteration  1400, loss = 0.218745\n",
      "Iteration  1500, loss = 0.218742\n",
      "Iteration  1600, loss = 0.218739\n",
      "Iteration  1700, loss = 0.218737\n",
      "Iteration  1800, loss = 0.218735\n",
      "Iteration  1900, loss = 0.218734\n",
      "Converged at iteration 1910\n",
      " Accuracy: 85.10%\n",
      " F1 Score: 0.4064\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218887\n",
      "Iteration   200, loss = 0.216287\n",
      "Iteration   300, loss = 0.215803\n",
      "Iteration   400, loss = 0.215630\n",
      "Iteration   500, loss = 0.215543\n",
      "Iteration   600, loss = 0.215492\n",
      "Iteration   700, loss = 0.215458\n",
      "Iteration   800, loss = 0.215434\n",
      "Iteration   900, loss = 0.215418\n",
      "Iteration  1000, loss = 0.215406\n",
      "Iteration  1100, loss = 0.215397\n",
      "Iteration  1200, loss = 0.215391\n",
      "Iteration  1300, loss = 0.215386\n",
      "Iteration  1400, loss = 0.215382\n",
      "Iteration  1500, loss = 0.215380\n",
      "Iteration  1600, loss = 0.215378\n",
      "Iteration  1700, loss = 0.215376\n",
      "Iteration  1800, loss = 0.215375\n",
      "Converged at iteration 1810\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.382847\n",
      "Iteration   200, loss = 0.377677\n",
      "Iteration   300, loss = 0.375503\n",
      "Iteration   400, loss = 0.374279\n",
      "Iteration   500, loss = 0.373522\n",
      "Iteration   600, loss = 0.373031\n",
      "Iteration   700, loss = 0.372701\n",
      "Iteration   800, loss = 0.372475\n",
      "Iteration   900, loss = 0.372318\n",
      "Iteration  1000, loss = 0.372206\n",
      "Iteration  1100, loss = 0.372126\n",
      "Iteration  1200, loss = 0.372068\n",
      "Iteration  1300, loss = 0.372026\n",
      "Iteration  1400, loss = 0.371995\n",
      "Iteration  1500, loss = 0.371972\n",
      "Iteration  1600, loss = 0.371955\n",
      "Iteration  1700, loss = 0.371942\n",
      "Iteration  1800, loss = 0.371933\n",
      "Iteration  1900, loss = 0.371926\n",
      "Iteration  2000, loss = 0.371920\n",
      "Iteration  2100, loss = 0.371916\n",
      "Iteration  2200, loss = 0.371913\n",
      "Iteration  2300, loss = 0.371911\n",
      "Iteration  2400, loss = 0.371909\n",
      "Iteration  2500, loss = 0.371907\n",
      "Converged at iteration 2581\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.364363\n",
      "Iteration   200, loss = 0.357849\n",
      "Iteration   300, loss = 0.355491\n",
      "Iteration   400, loss = 0.354365\n",
      "Iteration   500, loss = 0.353749\n",
      "Iteration   600, loss = 0.353383\n",
      "Iteration   700, loss = 0.353152\n",
      "Iteration   800, loss = 0.353000\n",
      "Iteration   900, loss = 0.352897\n",
      "Iteration  1000, loss = 0.352825\n",
      "Iteration  1100, loss = 0.352774\n",
      "Iteration  1200, loss = 0.352737\n",
      "Iteration  1300, loss = 0.352710\n",
      "Iteration  1400, loss = 0.352691\n",
      "Iteration  1500, loss = 0.352676\n",
      "Iteration  1600, loss = 0.352665\n",
      "Iteration  1700, loss = 0.352656\n",
      "Iteration  1800, loss = 0.352650\n",
      "Iteration  1900, loss = 0.352645\n",
      "Iteration  2000, loss = 0.352641\n",
      "Iteration  2100, loss = 0.352638\n",
      "Iteration  2200, loss = 0.352636\n",
      "Iteration  2300, loss = 0.352634\n",
      "Iteration  2400, loss = 0.352633\n",
      "Converged at iteration 2462\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.367902\n",
      "Iteration   200, loss = 0.363557\n",
      "Iteration   300, loss = 0.362158\n",
      "Iteration   400, loss = 0.361496\n",
      "Iteration   500, loss = 0.361131\n",
      "Iteration   600, loss = 0.360912\n",
      "Iteration   700, loss = 0.360772\n",
      "Iteration   800, loss = 0.360679\n",
      "Iteration   900, loss = 0.360615\n",
      "Iteration  1000, loss = 0.360570\n",
      "Iteration  1100, loss = 0.360538\n",
      "Iteration  1200, loss = 0.360515\n",
      "Iteration  1300, loss = 0.360497\n",
      "Iteration  1400, loss = 0.360485\n",
      "Iteration  1500, loss = 0.360475\n",
      "Iteration  1600, loss = 0.360468\n",
      "Iteration  1700, loss = 0.360463\n",
      "Iteration  1800, loss = 0.360459\n",
      "Iteration  1900, loss = 0.360456\n",
      "Iteration  2000, loss = 0.360454\n",
      "Iteration  2100, loss = 0.360452\n",
      "Iteration  2200, loss = 0.360451\n",
      "Converged at iteration 2257\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.373653\n",
      "Iteration   200, loss = 0.367876\n",
      "Iteration   300, loss = 0.365844\n",
      "Iteration   400, loss = 0.364835\n",
      "Iteration   500, loss = 0.364256\n",
      "Iteration   600, loss = 0.363894\n",
      "Iteration   700, loss = 0.363655\n",
      "Iteration   800, loss = 0.363492\n",
      "Iteration   900, loss = 0.363378\n",
      "Iteration  1000, loss = 0.363295\n",
      "Iteration  1100, loss = 0.363236\n",
      "Iteration  1200, loss = 0.363192\n",
      "Iteration  1300, loss = 0.363160\n",
      "Iteration  1400, loss = 0.363135\n",
      "Iteration  1500, loss = 0.363117\n",
      "Iteration  1600, loss = 0.363104\n",
      "Iteration  1700, loss = 0.363093\n",
      "Iteration  1800, loss = 0.363085\n",
      "Iteration  1900, loss = 0.363079\n",
      "Iteration  2000, loss = 0.363075\n",
      "Iteration  2100, loss = 0.363071\n",
      "Iteration  2200, loss = 0.363068\n",
      "Iteration  2300, loss = 0.363066\n",
      "Iteration  2400, loss = 0.363065\n",
      "Iteration  2500, loss = 0.363063\n",
      "Converged at iteration 2536\n",
      " Accuracy: 30.75%\n",
      " F1 Score: 0.1924\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.387809\n",
      "Iteration   200, loss = 0.382998\n",
      "Iteration   300, loss = 0.381450\n",
      "Iteration   400, loss = 0.380730\n",
      "Iteration   500, loss = 0.380329\n",
      "Iteration   600, loss = 0.380079\n",
      "Iteration   700, loss = 0.379912\n",
      "Iteration   800, loss = 0.379794\n",
      "Iteration   900, loss = 0.379710\n",
      "Iteration  1000, loss = 0.379648\n",
      "Iteration  1100, loss = 0.379602\n",
      "Iteration  1200, loss = 0.379567\n",
      "Iteration  1300, loss = 0.379540\n",
      "Iteration  1400, loss = 0.379520\n",
      "Iteration  1500, loss = 0.379505\n",
      "Iteration  1600, loss = 0.379493\n",
      "Iteration  1700, loss = 0.379484\n",
      "Iteration  1800, loss = 0.379477\n",
      "Iteration  1900, loss = 0.379472\n",
      "Iteration  2000, loss = 0.379468\n",
      "Iteration  2100, loss = 0.379465\n",
      "Iteration  2200, loss = 0.379462\n",
      "Iteration  2300, loss = 0.379460\n",
      "Iteration  2400, loss = 0.379459\n",
      "Iteration  2500, loss = 0.379457\n",
      "Converged at iteration 2521\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.221028\n",
      "Iteration   200, loss = 0.218579\n",
      "Iteration   300, loss = 0.218082\n",
      "Iteration   400, loss = 0.217874\n",
      "Iteration   500, loss = 0.217757\n",
      "Iteration   600, loss = 0.217683\n",
      "Iteration   700, loss = 0.217633\n",
      "Iteration   800, loss = 0.217598\n",
      "Iteration   900, loss = 0.217574\n",
      "Iteration  1000, loss = 0.217557\n",
      "Iteration  1100, loss = 0.217545\n",
      "Iteration  1200, loss = 0.217536\n",
      "Iteration  1300, loss = 0.217529\n",
      "Iteration  1400, loss = 0.217525\n",
      "Iteration  1500, loss = 0.217521\n",
      "Iteration  1600, loss = 0.217519\n",
      "Iteration  1700, loss = 0.217517\n",
      "Iteration  1800, loss = 0.217515\n",
      "Converged at iteration 1873\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.216906\n",
      "Iteration   200, loss = 0.214285\n",
      "Iteration   300, loss = 0.213749\n",
      "Iteration   400, loss = 0.213540\n",
      "Iteration   500, loss = 0.213429\n",
      "Iteration   600, loss = 0.213362\n",
      "Iteration   700, loss = 0.213318\n",
      "Iteration   800, loss = 0.213289\n",
      "Iteration   900, loss = 0.213268\n",
      "Iteration  1000, loss = 0.213254\n",
      "Iteration  1100, loss = 0.213243\n",
      "Iteration  1200, loss = 0.213236\n",
      "Iteration  1300, loss = 0.213231\n",
      "Iteration  1400, loss = 0.213227\n",
      "Iteration  1500, loss = 0.213224\n",
      "Iteration  1600, loss = 0.213221\n",
      "Iteration  1700, loss = 0.213220\n",
      "Iteration  1800, loss = 0.213218\n",
      "Converged at iteration 1832\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218420\n",
      "Iteration   200, loss = 0.215899\n",
      "Iteration   300, loss = 0.215423\n",
      "Iteration   400, loss = 0.215247\n",
      "Iteration   500, loss = 0.215158\n",
      "Iteration   600, loss = 0.215105\n",
      "Iteration   700, loss = 0.215071\n",
      "Iteration   800, loss = 0.215048\n",
      "Iteration   900, loss = 0.215033\n",
      "Iteration  1000, loss = 0.215022\n",
      "Iteration  1100, loss = 0.215014\n",
      "Iteration  1200, loss = 0.215008\n",
      "Iteration  1300, loss = 0.215004\n",
      "Iteration  1400, loss = 0.215002\n",
      "Iteration  1500, loss = 0.214999\n",
      "Iteration  1600, loss = 0.214998\n",
      "Iteration  1700, loss = 0.214997\n",
      "Converged at iteration 1710\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.222193\n",
      "Iteration   200, loss = 0.219767\n",
      "Iteration   300, loss = 0.219275\n",
      "Iteration   400, loss = 0.219076\n",
      "Iteration   500, loss = 0.218966\n",
      "Iteration   600, loss = 0.218897\n",
      "Iteration   700, loss = 0.218850\n",
      "Iteration   800, loss = 0.218817\n",
      "Iteration   900, loss = 0.218794\n",
      "Iteration  1000, loss = 0.218778\n",
      "Iteration  1100, loss = 0.218766\n",
      "Iteration  1200, loss = 0.218757\n",
      "Iteration  1300, loss = 0.218750\n",
      "Iteration  1400, loss = 0.218745\n",
      "Iteration  1500, loss = 0.218742\n",
      "Iteration  1600, loss = 0.218739\n",
      "Iteration  1700, loss = 0.218737\n",
      "Iteration  1800, loss = 0.218735\n",
      "Iteration  1900, loss = 0.218734\n",
      "Converged at iteration 1910\n",
      " Accuracy: 87.25%\n",
      " F1 Score: 0.4028\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218887\n",
      "Iteration   200, loss = 0.216287\n",
      "Iteration   300, loss = 0.215803\n",
      "Iteration   400, loss = 0.215630\n",
      "Iteration   500, loss = 0.215543\n",
      "Iteration   600, loss = 0.215492\n",
      "Iteration   700, loss = 0.215458\n",
      "Iteration   800, loss = 0.215434\n",
      "Iteration   900, loss = 0.215418\n",
      "Iteration  1000, loss = 0.215406\n",
      "Iteration  1100, loss = 0.215397\n",
      "Iteration  1200, loss = 0.215391\n",
      "Iteration  1300, loss = 0.215386\n",
      "Iteration  1400, loss = 0.215382\n",
      "Iteration  1500, loss = 0.215380\n",
      "Iteration  1600, loss = 0.215378\n",
      "Iteration  1700, loss = 0.215376\n",
      "Iteration  1800, loss = 0.215375\n",
      "Converged at iteration 1810\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.382847\n",
      "Iteration   200, loss = 0.377677\n",
      "Iteration   300, loss = 0.375503\n",
      "Iteration   400, loss = 0.374279\n",
      "Iteration   500, loss = 0.373522\n",
      "Iteration   600, loss = 0.373031\n",
      "Iteration   700, loss = 0.372701\n",
      "Iteration   800, loss = 0.372475\n",
      "Iteration   900, loss = 0.372318\n",
      "Iteration  1000, loss = 0.372206\n",
      "Iteration  1100, loss = 0.372126\n",
      "Iteration  1200, loss = 0.372068\n",
      "Iteration  1300, loss = 0.372026\n",
      "Iteration  1400, loss = 0.371995\n",
      "Iteration  1500, loss = 0.371972\n",
      "Iteration  1600, loss = 0.371955\n",
      "Iteration  1700, loss = 0.371942\n",
      "Iteration  1800, loss = 0.371933\n",
      "Iteration  1900, loss = 0.371926\n",
      "Iteration  2000, loss = 0.371920\n",
      "Iteration  2100, loss = 0.371916\n",
      "Iteration  2200, loss = 0.371913\n",
      "Iteration  2300, loss = 0.371911\n",
      "Iteration  2400, loss = 0.371909\n",
      "Iteration  2500, loss = 0.371907\n",
      "Converged at iteration 2581\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.364363\n",
      "Iteration   200, loss = 0.357849\n",
      "Iteration   300, loss = 0.355491\n",
      "Iteration   400, loss = 0.354365\n",
      "Iteration   500, loss = 0.353749\n",
      "Iteration   600, loss = 0.353383\n",
      "Iteration   700, loss = 0.353152\n",
      "Iteration   800, loss = 0.353000\n",
      "Iteration   900, loss = 0.352897\n",
      "Iteration  1000, loss = 0.352825\n",
      "Iteration  1100, loss = 0.352774\n",
      "Iteration  1200, loss = 0.352737\n",
      "Iteration  1300, loss = 0.352710\n",
      "Iteration  1400, loss = 0.352691\n",
      "Iteration  1500, loss = 0.352676\n",
      "Iteration  1600, loss = 0.352665\n",
      "Iteration  1700, loss = 0.352656\n",
      "Iteration  1800, loss = 0.352650\n",
      "Iteration  1900, loss = 0.352645\n",
      "Iteration  2000, loss = 0.352641\n",
      "Iteration  2100, loss = 0.352638\n",
      "Iteration  2200, loss = 0.352636\n",
      "Iteration  2300, loss = 0.352634\n",
      "Iteration  2400, loss = 0.352633\n",
      "Converged at iteration 2462\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.367902\n",
      "Iteration   200, loss = 0.363557\n",
      "Iteration   300, loss = 0.362158\n",
      "Iteration   400, loss = 0.361496\n",
      "Iteration   500, loss = 0.361131\n",
      "Iteration   600, loss = 0.360912\n",
      "Iteration   700, loss = 0.360772\n",
      "Iteration   800, loss = 0.360679\n",
      "Iteration   900, loss = 0.360615\n",
      "Iteration  1000, loss = 0.360570\n",
      "Iteration  1100, loss = 0.360538\n",
      "Iteration  1200, loss = 0.360515\n",
      "Iteration  1300, loss = 0.360497\n",
      "Iteration  1400, loss = 0.360485\n",
      "Iteration  1500, loss = 0.360475\n",
      "Iteration  1600, loss = 0.360468\n",
      "Iteration  1700, loss = 0.360463\n",
      "Iteration  1800, loss = 0.360459\n",
      "Iteration  1900, loss = 0.360456\n",
      "Iteration  2000, loss = 0.360454\n",
      "Iteration  2100, loss = 0.360452\n",
      "Iteration  2200, loss = 0.360451\n",
      "Converged at iteration 2257\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.373653\n",
      "Iteration   200, loss = 0.367876\n",
      "Iteration   300, loss = 0.365844\n",
      "Iteration   400, loss = 0.364835\n",
      "Iteration   500, loss = 0.364256\n",
      "Iteration   600, loss = 0.363894\n",
      "Iteration   700, loss = 0.363655\n",
      "Iteration   800, loss = 0.363492\n",
      "Iteration   900, loss = 0.363378\n",
      "Iteration  1000, loss = 0.363295\n",
      "Iteration  1100, loss = 0.363236\n",
      "Iteration  1200, loss = 0.363192\n",
      "Iteration  1300, loss = 0.363160\n",
      "Iteration  1400, loss = 0.363135\n",
      "Iteration  1500, loss = 0.363117\n",
      "Iteration  1600, loss = 0.363104\n",
      "Iteration  1700, loss = 0.363093\n",
      "Iteration  1800, loss = 0.363085\n",
      "Iteration  1900, loss = 0.363079\n",
      "Iteration  2000, loss = 0.363075\n",
      "Iteration  2100, loss = 0.363071\n",
      "Iteration  2200, loss = 0.363068\n",
      "Iteration  2300, loss = 0.363066\n",
      "Iteration  2400, loss = 0.363065\n",
      "Iteration  2500, loss = 0.363063\n",
      "Converged at iteration 2536\n",
      " Accuracy: 34.80%\n",
      " F1 Score: 0.2020\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.387809\n",
      "Iteration   200, loss = 0.382998\n",
      "Iteration   300, loss = 0.381450\n",
      "Iteration   400, loss = 0.380730\n",
      "Iteration   500, loss = 0.380329\n",
      "Iteration   600, loss = 0.380079\n",
      "Iteration   700, loss = 0.379912\n",
      "Iteration   800, loss = 0.379794\n",
      "Iteration   900, loss = 0.379710\n",
      "Iteration  1000, loss = 0.379648\n",
      "Iteration  1100, loss = 0.379602\n",
      "Iteration  1200, loss = 0.379567\n",
      "Iteration  1300, loss = 0.379540\n",
      "Iteration  1400, loss = 0.379520\n",
      "Iteration  1500, loss = 0.379505\n",
      "Iteration  1600, loss = 0.379493\n",
      "Iteration  1700, loss = 0.379484\n",
      "Iteration  1800, loss = 0.379477\n",
      "Iteration  1900, loss = 0.379472\n",
      "Iteration  2000, loss = 0.379468\n",
      "Iteration  2100, loss = 0.379465\n",
      "Iteration  2200, loss = 0.379462\n",
      "Iteration  2300, loss = 0.379460\n",
      "Iteration  2400, loss = 0.379459\n",
      "Iteration  2500, loss = 0.379457\n",
      "Converged at iteration 2521\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.221028\n",
      "Iteration   200, loss = 0.218579\n",
      "Iteration   300, loss = 0.218082\n",
      "Iteration   400, loss = 0.217874\n",
      "Iteration   500, loss = 0.217757\n",
      "Iteration   600, loss = 0.217683\n",
      "Iteration   700, loss = 0.217633\n",
      "Iteration   800, loss = 0.217598\n",
      "Iteration   900, loss = 0.217574\n",
      "Iteration  1000, loss = 0.217557\n",
      "Iteration  1100, loss = 0.217545\n",
      "Iteration  1200, loss = 0.217536\n",
      "Iteration  1300, loss = 0.217529\n",
      "Iteration  1400, loss = 0.217525\n",
      "Iteration  1500, loss = 0.217521\n",
      "Iteration  1600, loss = 0.217519\n",
      "Iteration  1700, loss = 0.217517\n",
      "Iteration  1800, loss = 0.217515\n",
      "Converged at iteration 1873\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.216906\n",
      "Iteration   200, loss = 0.214285\n",
      "Iteration   300, loss = 0.213749\n",
      "Iteration   400, loss = 0.213540\n",
      "Iteration   500, loss = 0.213429\n",
      "Iteration   600, loss = 0.213362\n",
      "Iteration   700, loss = 0.213318\n",
      "Iteration   800, loss = 0.213289\n",
      "Iteration   900, loss = 0.213268\n",
      "Iteration  1000, loss = 0.213254\n",
      "Iteration  1100, loss = 0.213243\n",
      "Iteration  1200, loss = 0.213236\n",
      "Iteration  1300, loss = 0.213231\n",
      "Iteration  1400, loss = 0.213227\n",
      "Iteration  1500, loss = 0.213224\n",
      "Iteration  1600, loss = 0.213221\n",
      "Iteration  1700, loss = 0.213220\n",
      "Iteration  1800, loss = 0.213218\n",
      "Converged at iteration 1832\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218420\n",
      "Iteration   200, loss = 0.215899\n",
      "Iteration   300, loss = 0.215423\n",
      "Iteration   400, loss = 0.215247\n",
      "Iteration   500, loss = 0.215158\n",
      "Iteration   600, loss = 0.215105\n",
      "Iteration   700, loss = 0.215071\n",
      "Iteration   800, loss = 0.215048\n",
      "Iteration   900, loss = 0.215033\n",
      "Iteration  1000, loss = 0.215022\n",
      "Iteration  1100, loss = 0.215014\n",
      "Iteration  1200, loss = 0.215008\n",
      "Iteration  1300, loss = 0.215004\n",
      "Iteration  1400, loss = 0.215002\n",
      "Iteration  1500, loss = 0.214999\n",
      "Iteration  1600, loss = 0.214998\n",
      "Iteration  1700, loss = 0.214997\n",
      "Converged at iteration 1710\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.222193\n",
      "Iteration   200, loss = 0.219767\n",
      "Iteration   300, loss = 0.219275\n",
      "Iteration   400, loss = 0.219076\n",
      "Iteration   500, loss = 0.218966\n",
      "Iteration   600, loss = 0.218897\n",
      "Iteration   700, loss = 0.218850\n",
      "Iteration   800, loss = 0.218817\n",
      "Iteration   900, loss = 0.218794\n",
      "Iteration  1000, loss = 0.218778\n",
      "Iteration  1100, loss = 0.218766\n",
      "Iteration  1200, loss = 0.218757\n",
      "Iteration  1300, loss = 0.218750\n",
      "Iteration  1400, loss = 0.218745\n",
      "Iteration  1500, loss = 0.218742\n",
      "Iteration  1600, loss = 0.218739\n",
      "Iteration  1700, loss = 0.218737\n",
      "Iteration  1800, loss = 0.218735\n",
      "Iteration  1900, loss = 0.218734\n",
      "Converged at iteration 1910\n",
      " Accuracy: 88.35%\n",
      " F1 Score: 0.3753\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.218887\n",
      "Iteration   200, loss = 0.216287\n",
      "Iteration   300, loss = 0.215803\n",
      "Iteration   400, loss = 0.215630\n",
      "Iteration   500, loss = 0.215543\n",
      "Iteration   600, loss = 0.215492\n",
      "Iteration   700, loss = 0.215458\n",
      "Iteration   800, loss = 0.215434\n",
      "Iteration   900, loss = 0.215418\n",
      "Iteration  1000, loss = 0.215406\n",
      "Iteration  1100, loss = 0.215397\n",
      "Iteration  1200, loss = 0.215391\n",
      "Iteration  1300, loss = 0.215386\n",
      "Iteration  1400, loss = 0.215382\n",
      "Iteration  1500, loss = 0.215380\n",
      "Iteration  1600, loss = 0.215378\n",
      "Iteration  1700, loss = 0.215376\n",
      "Iteration  1800, loss = 0.215375\n",
      "Converged at iteration 1810\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.382847\n",
      "Iteration   200, loss = 0.377677\n",
      "Iteration   300, loss = 0.375503\n",
      "Iteration   400, loss = 0.374279\n",
      "Iteration   500, loss = 0.373522\n",
      "Iteration   600, loss = 0.373031\n",
      "Iteration   700, loss = 0.372701\n",
      "Iteration   800, loss = 0.372475\n",
      "Iteration   900, loss = 0.372318\n",
      "Iteration  1000, loss = 0.372206\n",
      "Iteration  1100, loss = 0.372126\n",
      "Iteration  1200, loss = 0.372068\n",
      "Iteration  1300, loss = 0.372026\n",
      "Iteration  1400, loss = 0.371995\n",
      "Iteration  1500, loss = 0.371972\n",
      "Iteration  1600, loss = 0.371955\n",
      "Iteration  1700, loss = 0.371942\n",
      "Iteration  1800, loss = 0.371933\n",
      "Iteration  1900, loss = 0.371926\n",
      "Iteration  2000, loss = 0.371920\n",
      "Iteration  2100, loss = 0.371916\n",
      "Iteration  2200, loss = 0.371913\n",
      "Iteration  2300, loss = 0.371911\n",
      "Iteration  2400, loss = 0.371909\n",
      "Iteration  2500, loss = 0.371907\n",
      "Converged at iteration 2581\n",
      " Accuracy: 91.40%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 2 out of 5\n",
      "Cleaning of batch 2 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.364363\n",
      "Iteration   200, loss = 0.357849\n",
      "Iteration   300, loss = 0.355491\n",
      "Iteration   400, loss = 0.354365\n",
      "Iteration   500, loss = 0.353749\n",
      "Iteration   600, loss = 0.353383\n",
      "Iteration   700, loss = 0.353152\n",
      "Iteration   800, loss = 0.353000\n",
      "Iteration   900, loss = 0.352897\n",
      "Iteration  1000, loss = 0.352825\n",
      "Iteration  1100, loss = 0.352774\n",
      "Iteration  1200, loss = 0.352737\n",
      "Iteration  1300, loss = 0.352710\n",
      "Iteration  1400, loss = 0.352691\n",
      "Iteration  1500, loss = 0.352676\n",
      "Iteration  1600, loss = 0.352665\n",
      "Iteration  1700, loss = 0.352656\n",
      "Iteration  1800, loss = 0.352650\n",
      "Iteration  1900, loss = 0.352645\n",
      "Iteration  2000, loss = 0.352641\n",
      "Iteration  2100, loss = 0.352638\n",
      "Iteration  2200, loss = 0.352636\n",
      "Iteration  2300, loss = 0.352634\n",
      "Iteration  2400, loss = 0.352633\n",
      "Converged at iteration 2462\n",
      " Accuracy: 90.65%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 3 out of 5\n",
      "Cleaning of batch 3 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.367902\n",
      "Iteration   200, loss = 0.363557\n",
      "Iteration   300, loss = 0.362158\n",
      "Iteration   400, loss = 0.361496\n",
      "Iteration   500, loss = 0.361131\n",
      "Iteration   600, loss = 0.360912\n",
      "Iteration   700, loss = 0.360772\n",
      "Iteration   800, loss = 0.360679\n",
      "Iteration   900, loss = 0.360615\n",
      "Iteration  1000, loss = 0.360570\n",
      "Iteration  1100, loss = 0.360538\n",
      "Iteration  1200, loss = 0.360515\n",
      "Iteration  1300, loss = 0.360497\n",
      "Iteration  1400, loss = 0.360485\n",
      "Iteration  1500, loss = 0.360475\n",
      "Iteration  1600, loss = 0.360468\n",
      "Iteration  1700, loss = 0.360463\n",
      "Iteration  1800, loss = 0.360459\n",
      "Iteration  1900, loss = 0.360456\n",
      "Iteration  2000, loss = 0.360454\n",
      "Iteration  2100, loss = 0.360452\n",
      "Iteration  2200, loss = 0.360451\n",
      "Converged at iteration 2257\n",
      " Accuracy: 91.15%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 4 out of 5\n",
      "Cleaning of batch 4 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.373653\n",
      "Iteration   200, loss = 0.367876\n",
      "Iteration   300, loss = 0.365844\n",
      "Iteration   400, loss = 0.364835\n",
      "Iteration   500, loss = 0.364256\n",
      "Iteration   600, loss = 0.363894\n",
      "Iteration   700, loss = 0.363655\n",
      "Iteration   800, loss = 0.363492\n",
      "Iteration   900, loss = 0.363378\n",
      "Iteration  1000, loss = 0.363295\n",
      "Iteration  1100, loss = 0.363236\n",
      "Iteration  1200, loss = 0.363192\n",
      "Iteration  1300, loss = 0.363160\n",
      "Iteration  1400, loss = 0.363135\n",
      "Iteration  1500, loss = 0.363117\n",
      "Iteration  1600, loss = 0.363104\n",
      "Iteration  1700, loss = 0.363093\n",
      "Iteration  1800, loss = 0.363085\n",
      "Iteration  1900, loss = 0.363079\n",
      "Iteration  2000, loss = 0.363075\n",
      "Iteration  2100, loss = 0.363071\n",
      "Iteration  2200, loss = 0.363068\n",
      "Iteration  2300, loss = 0.363066\n",
      "Iteration  2400, loss = 0.363065\n",
      "Iteration  2500, loss = 0.363063\n",
      "Converged at iteration 2536\n",
      " Accuracy: 38.20%\n",
      " F1 Score: 0.2097\n",
      "Start cleaning batch 5 out of 5\n",
      "Cleaning of batch 5 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.387809\n",
      "Iteration   200, loss = 0.382998\n",
      "Iteration   300, loss = 0.381450\n",
      "Iteration   400, loss = 0.380730\n",
      "Iteration   500, loss = 0.380329\n",
      "Iteration   600, loss = 0.380079\n",
      "Iteration   700, loss = 0.379912\n",
      "Iteration   800, loss = 0.379794\n",
      "Iteration   900, loss = 0.379710\n",
      "Iteration  1000, loss = 0.379648\n",
      "Iteration  1100, loss = 0.379602\n",
      "Iteration  1200, loss = 0.379567\n",
      "Iteration  1300, loss = 0.379540\n",
      "Iteration  1400, loss = 0.379520\n",
      "Iteration  1500, loss = 0.379505\n",
      "Iteration  1600, loss = 0.379493\n",
      "Iteration  1700, loss = 0.379484\n",
      "Iteration  1800, loss = 0.379477\n",
      "Iteration  1900, loss = 0.379472\n",
      "Iteration  2000, loss = 0.379468\n",
      "Iteration  2100, loss = 0.379465\n",
      "Iteration  2200, loss = 0.379462\n",
      "Iteration  2300, loss = 0.379460\n",
      "Iteration  2400, loss = 0.379459\n",
      "Iteration  2500, loss = 0.379457\n",
      "Converged at iteration 2521\n",
      " Accuracy: 91.70%\n",
      " F1 Score: 0.0000\n",
      "Start cleaning batch 1 out of 5\n",
      "Cleaning of batch 1 done. Stating the model training.\n",
      "Iteration     0, loss = 0.693147\n",
      "Iteration   100, loss = 0.221028\n",
      "Iteration   200, loss = 0.218579\n",
      "Iteration   300, loss = 0.218082\n",
      "Iteration   400, loss = 0.217874\n",
      "Iteration   500, loss = 0.217757\n",
      "Iteration   600, loss = 0.217683\n",
      "Iteration   700, loss = 0.217633\n",
      "Iteration   800, loss = 0.217598\n",
      "Iteration   900, loss = 0.217574\n",
      "Iteration  1000, loss = 0.217557\n",
      "Iteration  1100, loss = 0.217545\n",
      "Iteration  1200, loss = 0.217536\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m acc_scores_bal = np.array([])\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m np.arange(\u001b[32m0\u001b[39m, \u001b[32m1.0\u001b[39m, \u001b[32m0.05\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     acc_no_bal, f1_no_bal = \u001b[43mkfold_logistic_ridge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_balanced_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     acc_bal, f1_bal = kfold_logistic_ridge(X_train_data, Y_train_data, balanced_data, k=\u001b[32m5\u001b[39m, gamma=\u001b[32m0.5\u001b[39m, lambda_=\u001b[32m1e-3\u001b[39m, alpha = alpha)\n\u001b[32m      9\u001b[39m     f1_scores_no_bal = np.append(f1_scores_no_bal, f1_no_bal)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janfo\\OneDrive\\Desktop\\universitat\\EPFL\\Machine Learning\\ML_proj1\\project1\\Implemented_functions.py:140\u001b[39m, in \u001b[36mkfold_logistic_ridge\u001b[39m\u001b[34m(X, y, process_data, k, gamma, lambda_, alpha, threshold, random_state)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCleaning of batch\u001b[39m\u001b[33m\"\u001b[39m, current_batch, \u001b[33m\"\u001b[39m\u001b[33mdone. Stating the model training.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# training the model and computing error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m loss, w = \u001b[43mlogistic_regression_penalized_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m acc, f1 = evaluate_logistic_model(y_val, X_val, w, threshold=alpha)\n\u001b[32m    142\u001b[39m accuracies.append(acc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janfo\\OneDrive\\Desktop\\universitat\\EPFL\\Machine Learning\\ML_proj1\\project1\\Implemented_functions.py:74\u001b[39m, in \u001b[36mlogistic_regression_penalized_gradient_descent\u001b[39m\u001b[34m(y, x, max_iter, gamma, lambda_, threshold)\u001b[39m\n\u001b[32m     71\u001b[39m w = np.zeros(tx.shape[\u001b[32m1\u001b[39m])\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     loss = \u001b[43mlogistic_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m + lambda_ * w.T\u001b[38;5;129m@w\u001b[39m\n\u001b[32m     75\u001b[39m     grad = compute_gradient_logistic(y, tx, w) + \u001b[32m2\u001b[39m* lambda_ * w  \n\u001b[32m     76\u001b[39m     w = w - gamma * grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janfo\\OneDrive\\Desktop\\universitat\\EPFL\\Machine Learning\\ML_proj1\\project1\\Implemented_functions.py:54\u001b[39m, in \u001b[36mlogistic_loss\u001b[39m\u001b[34m(y, tx, w)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Stable logistic loss computation.\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m eps = \u001b[32m1e-15\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m pred = \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m pred = np.clip(pred, eps, \u001b[32m1\u001b[39m - eps)\n\u001b[32m     56\u001b[39m loss = -np.mean(y * np.log(pred) + (\u001b[32m1\u001b[39m - y) * np.log(\u001b[32m1\u001b[39m - pred))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janfo\\OneDrive\\Desktop\\universitat\\EPFL\\Machine Learning\\ML_proj1\\project1\\Implemented_functions.py:7\u001b[39m, in \u001b[36msigmoid\u001b[39m\u001b[34m(z)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhelpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_accuracy, f1_score\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Logistic Regression Functions\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msigmoid\u001b[39m(z):\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCompute the sigmoid function in a numerically stable way.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m     z = np.clip(z, -\u001b[32m500\u001b[39m, \u001b[32m500\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "f1_scores_no_bal = np.array([])\n",
    "f1_scores_bal = np.array([])\n",
    "acc_scores_no_bal = np.array([])\n",
    "acc_scores_bal = np.array([])\n",
    "\n",
    "for alpha in np.arange(0, 1.0, 0.05):\n",
    "    acc_no_bal, f1_no_bal = kfold_logistic_ridge(X_train_data, Y_train_data, no_balanced_data, k=5, gamma=0.5, lambda_=1e-3, alpha = alpha)\n",
    "    acc_bal, f1_bal = kfold_logistic_ridge(X_train_data, Y_train_data, balanced_data, k=5, gamma=0.5, lambda_=1e-3, alpha = alpha)\n",
    "    f1_scores_no_bal = np.append(f1_scores_no_bal, f1_no_bal)\n",
    "    f1_scores_bal = np.append(f1_scores_bal, f1_bal)    \n",
    "    acc_scores_no_bal = np.append(acc_scores_no_bal, acc_no_bal)\n",
    "    acc_scores_bal = np.append(acc_scores_bal, acc_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f8a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = np.arange(0, 1.0, 0.05)\n",
    "plt.plot(dx, f1_scores_no_bal, label='F1 Score No Balancing')\n",
    "plt.plot(dx, f1_scores_bal, label='F1 Score With Balancing')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Alpha for Balanced and Unbalanced Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dx, acc_scores_no_bal, label='Accuracy No Balancing')\n",
    "plt.plot(dx, acc_scores_bal, label='Accuracy With Balancing')   \n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Alpha for Balanced and Unbalanced Data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
